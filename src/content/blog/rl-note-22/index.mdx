---
title: "RL笔记（22）：初入多智能体强化学习 (MARL)"
publishDate: 2025-12-31
updatedDate: 2025-12-31
description: "MARL 的两个极端：详解联合动作学习 (JAL) 与独立学习 (Independent RL)。深度分析“维度灾难”与“环境非平稳性”这对核心矛盾。"
heroImage: {src : "https://pic.hana0721.top/rl-note-4.1ziqble6oe.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

在之前的 19 篇笔记中，我们研究的都是 **单智能体强化学习 (Single-Agent RL)**。
但在现实世界中，任务往往涉及多个个体。**多智能体强化学习 (MARL)** 将问题扩展到了随机博弈（Stochastic Games）的领域。

在进入复杂的 SOTA 算法之前，我们必须先理解解决 MARL 问题的两种**最朴素、最极端**的思路：
1.  **完全中心化 (JAL)**：把所有人看作一个人。
2.  **完全去中心化 (IPPO)**：把队友看作空气（或环境噪声）。

这两种思路分别对应了 MARL 的两大核心难题：**维度灾难** 与 **非平稳性**。

---

##  理论模型：随机博弈

定义为一个元组 $(N, \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$：
*   $N$：智能体数量。
*   $\mathcal{S}$：全局状态空间。
*   $\mathcal{A} = \mathcal{A}_1 \times \dots \times \mathcal{A}_N$：**联合动作空间 (Joint Action Space)**。
    *   状态转移取决于所有人的动作组合 $\mathbf{a} = (a_1, \dots, a_N)$。
    *   即 $s' \sim \mathcal{P}(\cdot | s, \mathbf{a})$。

---

##  极端一：联合动作学习 (JAL)

**Joint Action Learning (JAL)** 代表了 **完全中心化 (Fully Centralized)** 的思路。

### 核心思想
既然环境受所有人的动作 $\mathbf{a}$ 影响，那我们就构建一个拥有上帝视角的 **“超级智能体” (Super Agent)**，它接收全局状态 $s$，直接输出联合动作 $\mathbf{a}$ 来控制所有单位。

### 方法
直接套用标准的 PPO 算法：
*   输入：$s$
*   输出：$\mathbf{a} = (a_1, a_2, \dots, a_N)$
*   策略：$\Pi_\theta(\mathbf{a} | s)$

### 理论分析
*   **优势：环境平稳**。
    对于这个超级智能体来说，外界环境是静止的（Stationary），因为没有“其他人”在干扰它。因此，马尔可夫性质成立，RL 的收敛性理论依然有效。
*   **致命缺陷：维度灾难 (Curse of Dimensionality)**。
    假设每个智能体有 $|A|$ 个动作，共有 $N$ 个智能体。联合动作空间的大小为 $|A|^N$。
    *   $|A|=5, N=10 \Rightarrow 5^{10} \approx 9,765,625$。
    *   输出层需要预测近一千万个概率值，这在计算上是不可行的。

---

##  极端二：独立学习 (Independent RL / IPPO)

**Independent PPO (IPPO)** 代表了 **完全去中心化 (Fully Decentralized)** 的思路。

### 核心思想
**“把队友当空气”**。
每个智能体 $i$ 都是一个独立的个体，它只关心自己的观测 $o_i$ 和奖励 $r_i$。它把所有“其他智能体”都视为**环境的一部分**。

### 方法
同时运行 $N$ 个独立的 PPO 算法（参数可以共享，也可以不共享）：
*   对于智能体 $i$：
    *   输入：局部观测 $o_i$
    *   输出：独立动作 $a_i$
    *   策略：$\pi_{\theta_i}(a_i | o_i)$

### 理论分析
*   **优势：线性扩展**。
    计算复杂度随人数 $N$ 线性增长。无论有多少人，每个网络只输出 $|A|$ 个概率。这完美解决了维度灾难。
*   **致命缺陷：非平稳性 (Non-Stationarity)**。
    从智能体 $i$ 的视角看，状态转移概率变成了：
    $$ P(s'|s, a_i) = \sum_{\mathbf{a}_{-i}} P(s'|s, a_i, \mathbf{a}_{-i}) \pi_{-i}(\mathbf{a}_{-i}|s) $$
    注意公式里的 $\pi_{-i}$（队友的策略）。在训练过程中，**队友也在学习，$\pi_{-i}$ 一直在变**。
    这意味着：**对于智能体 $i$ 来说，昨天有用的策略，今天可能就没用了，因为环境（队友）变了。**
    这破坏了马尔可夫假设，理论上算法无法收敛。

---

##  总结与对比

| 维度 | 联合动作学习 (JAL) | 独立学习 (IPPO) |
| :--- | :--- | :--- |
| **控制方式** | **完全中心化** (Super Agent) | **完全去中心化** (Independent Agents) |
| **动作空间** | 联合动作 $\mathbf{a}$ | 独立动作 $a_i$ |
| **空间复杂度** | **指数级爆炸** $|A|^N$ | **线性增长** $N \times |A|$ |
| **环境性质** | **平稳 (Stationary)** | **非平稳 (Non-Stationary)** |
| **理论保证** | 有收敛保证 | 无收敛保证 |
| **实际表现** | 规模稍大即无法运行 | 尽管没理论保证，但在实践中往往是 Strong Baseline |

### 这一章的启示
我们陷入了两难境地：
1.  选 JAL，理论稳但算不动。
2.  选 IPPO，算得快但理论虚。

这就引出了 MARL 研究的圣杯——**CTDE (集中式训练，分布式执行)**。
我们需要一种折中的方法：**在训练时利用 JAL 的上帝视角来缓解非平稳性，在执行时利用 IPPO 的独立结构来避免维度灾难。**

这正是后续 **MAPPO** 和 **QMIX** 等算法的核心动机。