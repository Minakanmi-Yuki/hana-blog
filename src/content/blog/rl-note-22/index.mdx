---
title: "RL笔记（22）：初入多智能体强化学习 (MARL)"
publishDate: 2025-12-31
updatedDate: 2025-12-31
description: "从孤独的探索者到复杂的博弈场。详解多智能体强化学习 (MARL) 的核心挑战——非平稳性，以及将 PPO 应用于 MARL 的两种基本范式：IPPO 与 Centralized PPO。"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

在之前的笔记中，我们研究的都是 **单智能体强化学习 (Single-Agent RL, SARL)**。
在这个设定下，环境（Environment）是静止的、被动的。智能体面对的是一个固定的物理规则世界。

**多智能体强化学习 (Multi-Agent RL, MARL)** 将问题扩展到了多个智能体共存的环境。
*   **合作 (Cooperative)**：所有智能体共享一个奖励，为了共同目标努力（如《星际争霸》中的部队微操）。
*   **竞争 (Competitive)**：零和博弈，一方收益等于另一方损失（如围棋、拳皇）。
*   **混合 (Mixed-Motives)**：既有竞争又有合作（如足球比赛，队内合作，队间竞争）。

MARL 最核心的难点在于：**环境不再是静止的**。

---

##  问题形式化：随机博弈 (Stochastic Games)

在 SARL 中，我们用 **MDP** 建模。在 MARL 中，我们将模型扩展为 **随机博弈 (Stochastic Games)**，也称为马尔可夫博弈。

定义为一个元组 $(N, \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{O}, \gamma)$：
*   $N$：智能体的数量，索引为 $i \in \{1, \dots, N\}$。
*   $\mathcal{S}$：全局状态空间（包含了所有智能体和环境的信息）。
*   $\mathcal{A} = \mathcal{A}_1 \times \dots \times \mathcal{A}_N$：**联合动作空间 (Joint Action Space)**。所有智能体动作的组合 $\mathbf{a} = (a_1, \dots, a_N)$ 决定了状态转移。
*   $\mathcal{P}(s'|s, \mathbf{a})$：状态转移概率，依赖于**联合动作**。
*   $\mathcal{R}_i(s, \mathbf{a})$：智能体 $i$ 的奖励函数。
*   $\mathcal{O}_i$：智能体 $i$ 的**局部观测 (Local Observation)**。通常 MARL 是部分可观测的 (POMDP)，智能体只能看到自己周围的信息 $o_i$。

---

##  核心挑战：非平稳性 (Non-Stationarity)

这是 MARL 区别于 SARL 的本质特征。

### 移动的目标 (Moving Target)
在 SARL 中，状态转移 $P(s'|s,a)$ 是固定的。
在 MARL 中，对于智能体 $i$ 来说，环境的“状态转移”实际上隐含了其他智能体的策略 $\pi_{-i}$：
$$ P(s'|s, a_i) = \sum_{\mathbf{a}_{-i}} P(s'|s, a_i, \mathbf{a}_{-i}) \pi_{-i}(\mathbf{a}_{-i}|s) $$

*   如果在训练过程中，**其他智能体也在学习（$\pi_{-i}$ 在变）**，那么从智能体 $i$ 的视角来看，**环境的动力学规则一直在变**。
*   这就破坏了马尔可夫假设（状态转移不再仅依赖于状态，还依赖于时间/轮次），导致原本收敛的算法（如 Q-Learning）可能失效。

---

##  范式一：独立 PPO (IPPO)

这是最直接、最简单的思路：**完全去中心化 (Fully Decentralized)**。

### 核心思想
**“把队友当空气（或者环境的一部分）”**。
我们不修改 PPO 算法的任何结构，直接让 $N$ 个智能体各自运行一个独立的 PPO 实例。

### 算法定义
对于智能体 $i$，它拥有独立的 Actor $\pi_{\theta_i}(a_i|o_i)$ 和 Critic $V_{\phi_i}(o_i)$。
*   **Actor 输入**：仅局部观测 $o_i$。
*   **Critic 输入**：仅局部观测 $o_i$。

### IPPO 的优劣
*   **优点**：
    *   **极易实现**：直接复用单智能体 PPO 代码即可。
    *   **可扩展性强**：智能体数量增加时，计算复杂度线性增长。
*   **缺点**：
    *   **理论缺陷**：严重受**非平稳性**影响。因为 Critic 仅凭局部观测 $o_i$ 无法准确评估状态价值（它看不到队友在做什么），导致价值估计震荡。
*   **现状**：尽管理论上有问题，但 IPPO 在实践中（尤其是在 Cooperative 任务中）表现出人意料地强劲，常被作为 Strong Baseline。

---

##  范式二：中心化 PPO (Centralized PPO)

为了解决非平稳性问题，我们引入了 **集中式训练，分布式执行 (CTDE)** 的架构。
其中最基础的形态就是 **Centralized PPO**。

### 核心思想
**“上帝视角的评论家，凡人视角的演员”。**

在训练时（Simulator 中），我们是可以获取所有信息的（全局状态 $s$ 和其他人的动作）。为什么不利用这些信息来帮助训练呢？

### 架构定义
1.  **Actor (策略网络)**：保持 **去中心化**。
    *   输入：局部观测 $o_i$。
    *   输出：动作 $a_i$。
    *   原因：在执行阶段（Testing），智能体必须能够独立行动。
    *   公式：$\pi_{\theta_i}(a_i | o_i)$

2.  **Critic (价值网络)**：变为 **中心化**。
    *   输入：**全局状态 $s$**（或者所有智能体观测的拼接 $\{o_1, \dots, o_N\}$）。
    *   输出：全局或局部状态的价值。
    *   原因：Critic 只在训练时计算优势函数 $A(s,a)$ 用，执行时不需要 Critic。
    *   公式：$V_{\phi_i}(s)$

### 为什么它能解决非平稳性？
$$ \delta_{i, t} = r_{i, t} + \gamma V_{\phi_i}(s_{t+1}) - V_{\phi_i}(s_t) $$

当 Critic 输入的是全局状态 $s$ 时，它包含了所有智能体的信息。
即使其他智能体的策略变了，全局状态 $s$ 到 $s'$ 的转移依然是符合环境物理规律的（马尔可夫的）。
因此，**中心化 Critic 面对的是一个平稳的预测问题**，能给出更准确的价值估计，从而算出更准的梯度来指导 Actor 更新。

---

## 5. 总结与对比

我们有两种将 PPO 扩展到 MARL 的基本方式：

| 维度 | **IPPO** (独立学习) | **Centralized PPO** (中心化学习) |
| :--- | :--- | :--- |
| **范式** | DTDE (分布式训练分布式执行) | **CTDE** (集中式训练分布式执行) |
| **Actor 输入** | 局部观测 $o_i$ | 局部观测 $o_i$ |
| **Critic 输入** | **局部观测 $o_i$** | **全局状态 $s$** |
| **非平稳性** | 严重受影响 (视队友为噪声) | **缓解** (Critic 能看到全局局势) |
| **信息需求** | 仅需局部信息 | 训练需全局信息，执行仅需局部 |
| **表现** | 实践中强劲，但方差较大 | 理论更优，适合强协作场景 |

在接下来的笔记中，我们将深入探讨基于价值的多智能体算法（如 QMIX），看看如何在 Value-Based 方法中实现 CTDE。