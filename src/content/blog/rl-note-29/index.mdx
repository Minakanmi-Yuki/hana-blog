---
title: "RL笔记（29）：推理模型的崛起 (GRPO & PRM)"
publishDate: 2026-01-07
updatedDate: 2026-01-07
description: "大模型训练的新范式：详解 DeepSeek 提出的 GRPO 如何彻底省去 Critic 网络，以及 PRM 如何通过过程监督让模型学会正确推理。"
heroImage: {src : "https://pic.hana0721.top/rl-note-5.32ifmhjene.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

在上一篇笔记中，我们介绍了经典的 **PPO + RLHF** 流程。虽然 PPO 非常有效，但在训练超大规模语言模型时，它面临两个挑战：
1.  **资源消耗巨大**：PPO 需要维护 Actor、Ref、Reward、Critic 四个模型，显存开销极高。
2.  **结果导向的局限**：传统的奖励模型只对最终答案评分（ORM），而不关心推理过程。如果模型凑巧猜对了答案，也会得到高分，这会导致“走捷径”和幻觉。

本章将介绍 **GRPO** ——一种更轻量、更高效的策略优化算法，以及 **PRM** ——一种对思维过程进行细粒度监督的奖励机制。

---

##  GRPO (Group Relative Policy Optimization)

> **论文**：[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in LLMs](https://arxiv.org/abs/2402.03300)
> **应用**：DeepSeek-V3 / DeepSeek-R1

GRPO 的核心创新在于：**彻底丢弃了 Critic 模型，利用组内相对排名来估计优势函数（Advantage）。**

### 核心思想：组内相对评价
在 PPO 中，我们需要 Critic 网络来预测状态价值 $V(s)$，从而计算优势 $A = Q - V$。
而在 GRPO 中，对于每一个提示词（Prompt）$q$：
1.  让模型生成**一组**不同的回复 $\{o_1, o_2, o_3, ..., o_G\}$（组大小为 $G$）。
2.  利用奖励模型（或规则奖励）计算出这一组回复的分数 $\{r_1, r_2, r_3, ..., r_G\}$。
3.  通过这组分数的**相对强弱**来直接得出每个回复的优势。

### 优势函数计算
第 $i$ 个回复的优势函数 $A_i$ 计算公式为：
$$
A_i = \frac{r_i - \text{mean}(r_1, r_2, ..., r_G)}{\text{std}(r_1, r_2, ..., r_G)}
$$

> **💡 直觉理解**：
> 这就像是在班级里考试。我们不需要一个绝对的“满分标准”（Critic），只需要看你在班级里的排名。如果你比班级平均分高，我们就增加你这种行为的概率；反之则降低。

### GRPO 损失函数
$$
L_{GRPO}(\theta) = \frac{1}{G} \sum_{i=1}^G \left[ \min\left( \frac{\pi_\theta(o_i|q)}{\pi_{\text{old}}(o_i|q)} A_i, \text{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\text{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right) A_i \right) - \beta D_{KL}(\pi_\theta || \pi_{\text{ref}}) \right]
$$

*   **优势**：由于省去了 Critic 网络，在大模型训练中可以节省约 **50%** 的梯度计算相关显存，从而允许更长的上下文或更大的 Batch Size。

---

##  PRM (Process Reward Models)

传统的奖励模型被称为 **ORM (Outcome Reward Models)**：只看结果。
**PRM (Process Reward Models)** 则是对推理链条中的**每一个步骤**进行打分。

### 核心动机
在复杂的数学推导或编程任务中：
*   **中间错一步，步步错**：即使最终答案对，中间逻辑也可能有毒。
*   **奖励稀疏**：只有到最后才给分，模型很难学会长链条的逻辑。

### 训练与运作机制
1.  **步骤拆解**：将模型生成的思维链（CoT）利用换行符或特殊 token 拆分为 $S_1, S_2, ..., S_n$。
2.  **细粒度标注**：通过人类专家或更强模型（如 GPT-4）对每一个 $S_i$ 标注“正确”、“错误”或“中性”。
3.  **模型预测**：PRM 模型学习预测每一步的正确概率。

### 强化学习中的应用
在 RL 过程中，奖励函数不再是标量，而是一个序列：
$$ \mathcal{R} = \{r(S_1), r(S_2), ..., r(S_n)\} $$
这允许 PPO 或 GRPO 进行**更密集的奖励信号反馈**，显著提升模型处理复杂推理问题的逻辑严密性。

---

##  案例分析：DeepSeek-R1 的强化学习范式

DeepSeek-R1 展示了 GRPO 结合“规则奖励”的惊人效果：

### 规则导向的 PRM
在推理任务中，我们有时不需要神经网络做奖励模型，而是使用**硬规则**：
1.  **准确性奖励 (Accuracy)**：答案必须对（例如数学题，提取最后的结果比对）。
2.  **格式奖励 (Format)**：思维链必须放在 `<think>` 和 `</think>` 标签之间。

### 涌现能力
DeepSeek 发现，通过这种简单的 GRPO + 规则奖励，模型在训练过程中会出现**自我反思（Self-reflection）**。
*   当模型发现之前的推导有问题时，它会自发地写下“Wait, that's not right...”（等一下，这不对……），然后重新推导。
*   这种能力并不是通过 SFT（监督微调）刻意教出来的，而是通过 RL 最大化奖励的过程中**自发进化**出来的。

---

##  总结与对比：PPO vs. GRPO

| 维度 | PPO (标准版) | GRPO (组相对版) |
| :--- | :--- | :--- |
| **显存消耗** | 高 (需要 Critic 网络) | **低 (丢弃 Critic 网络)** |
| **优势估计** | 依赖学出来的 $V(s)$ | **依赖组内样本的统计分布** |
| **训练稳定性** | 容易受 Critic 网络波动影响 | 更稳定，因为对比是相对的 |
| **主要应用** | 早期 ChatGPT, Llama 3 对齐 | **DeepSeek-R1/V3 推理训练** |

### 启示
强化学习在大模型时代的进化方向非常明确：
1.  **算力优化**：通过像 GRPO 这样的算法减少训练开销。
2.  **逻辑监督**：通过 PRM 或 规则奖励 强化模型的推理过程，而不仅仅是最终答案。