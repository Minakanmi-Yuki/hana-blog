---
title: "RL笔记（21）：目标导向的强化学习 (Goal-Conditioned RL)"
publishDate: 2025-12-30
updatedDate: 2025-12-30
description: "从解决单一任务到解决一类任务：详解目标导向 RL 的数学形式化。涵盖通用价值函数近似 (UVFA) 理论，以及解决稀疏奖励难题的核心技术——事后经验回放 (HER)。"
heroImage: {src : "https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

在标准的强化学习设定中，我们通常训练一个智能体去完成**一个特定的任务**（例如：打赢一局游戏，或者让机器人走到特定的坐标 $(x,y)$）。如果任务目标变了（例如：走到新的坐标 $(x', y')$），我们通常需要重新训练网络。

**目标导向的强化学习 (Goal-Conditioned RL, GCRL)** 旨在打破这一限制。我们希望训练一个智能体，它不仅能根据状态 $s$ 做出决策，还能根据**输入的目标 $g$** 动态调整策略。

即策略函数变为 $\pi(a|s, g)$，价值函数变为 $Q(s, a, g)$。这使得智能体具备了**多任务泛化**的能力。

---

##  问题形式化 (Problem Formulation)

我们将标准的马尔可夫决策过程 (MDP) 扩展为 **目标增强的 MDP (Augmented MDP)**。

### 扩展元组
新的元组定义为 $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{G}, r, \gamma)$。
*   $\mathcal{G}$：目标空间（通常是状态空间 $\mathcal{S}$ 的子集）。
*   $r(s, a, g)$：目标导向的奖励函数。

### 奖励函数的设计
在 GCRL 中，最常见的奖励函数有两种形式：

1.  **稀疏奖励 (Sparse Reward)**：
    只有当智能体真正达成目标时才给奖励（通常用于机械臂抓取等精确任务）。
    $$
    r(s, a, g) = \mathbb{I}(\phi(s) = g) = \begin{cases} 0 & \text{if } ||\phi(s) - g|| < \epsilon \\ -1 & \text{otherwise} \end{cases}
    $$
    *(注：这里用 0/-1 奖励结构比 1/0 更常见，因为这样价值函数代表了“到达目标的期望步数”的负值)*

2.  **稠密奖励 (Dense Reward)**：
    基于当前状态与目标的距离。
    $$
    r(s, a, g) = -||\phi(s) - g||_2
    $$

### 目标：UVFA
我们的优化目标是找到一个最优策略 $\pi^*$，最大化所有可能的起始状态和目标的期望回报：
$$
J(\pi) = \mathbb{E}_{g \sim p(g), s_0 \sim p(s_0), \tau \sim \pi} \left[ \sum_{t=0}^\infty \gamma^t r(s_t, a_t, g) \right]
$$

这种能够同时泛化状态 $s$ 和目标 $g$ 的价值函数近似器，被称为 **通用价值函数近似 (Universal Value Function Approximators, UVFA)**。
$$ V(s, g; \theta) \approx \mathbb{E} [R | s, g] $$
神经网络结构通常是将状态特征 $\phi(s)$ 和目标特征 $\psi(g)$ 拼接后输入。

---

##  稀疏奖励难题

尽管 UVFA 提供了理论框架，但在实际训练中，如果使用**稀疏奖励**，学习会极其困难。

**问题**：在一个高维环境中（例如机械臂控制），随机探索碰到目标 $g$ 的概率几乎为 0。
这意味着：
*   经验回放池 $\mathcal{D}$ 中几乎所有的样本，其奖励 $r$ 都是 -1（或 0）。
*   智能体收不到任何正反馈，梯度无法指引优化的方向。

这被称为 **稀疏奖励问题 (Sparse Reward Problem)**。

---

##  事后经验回放 (Hindsight Experience Replay, HER)

**HER** 是 GCRL 领域最核心的算法创新。它的灵感来源于人类的“精神胜利法”。

> **💡 直觉**：
> 假设你的目标是射中靶心，但你射偏了，射到了旁边的树上。
> 虽然作为“射靶心”的任务你失败了，但如果我们事后回顾（Hindsight），假设原本的目标就是“射中那棵树”，那你通过刚才的动作序列，完美地完成了任务！

### 理论推导
HER 的核心在于**重标记 (Relabeling)** 经验回放池中的目标。

假设智能体在目标 $g$ 的指导下，产生了一条轨迹：
$$ \tau = \{s_0, a_0, r_0, s_1, \dots, s_T\} $$
其中，最后的状态 $s_T$ 并没有到达预设目标 $g$，所以所有的 $r_t = -1$。

**HER 的操作**：
我们构造一个新的虚假目标 $g' = s_T$（即把最后实际达到的状态当作目标）。
对于轨迹中的每一个转换 $(s_t, a_t, s_{t+1})$，我们根据新目标 $g'$ 重新计算奖励：
$$
r'_t = r(s_t, a_t, g') = \begin{cases} 0 & \text{if } s_{t+1} = g' \\ -1 & \text{otherwise} \end{cases}
$$
由于 $g'$ 就是 $s_T$，那么在轨迹结束时，智能体**必然**获得了正奖励。

我们将原始数据 $(s_t, a_t, r_t, s_{t+1}, g)$ 和修改后的数据 $(s_t, a_t, r'_t, s_{t+1}, g')$ 都存入 Replay Buffer。

### 为什么数学上是成立的？(Off-Policy 的重要性)
HER 只能用于 **离线策略 (Off-Policy)** 算法（如 DQN, DDPG, SAC）。

**原因**：
我们原本产生的轨迹 $\tau$ 是由策略 $\pi(\cdot | s, g)$ 生成的。
但在训练时，我们将其强行解释为是由策略 $\pi(\cdot | s, g')$ 生成的。
这本质上是一种极端的 Off-Policy 学习：**行为策略的目标与更新时的目标完全不同。**

只要算法支持 Off-Policy（即不仅可以利用自己产生的旧数据，还可以利用“别人”产生的数据），HER 就是数学上合法的。因为对于 $(s_t, a_t, s_{t+1})$ 这个物理转换来说，它只遵循环境动力学 $P(s'|s,a)$，与目标 $g$ 无关。环境的物理规律是不随心中的目标而改变的。

---

##  HER 的变种与理论解释

### 不同的重标记策略 (Replay Strategy)
在将数据存入 Buffer 时，我们可以选择不同的重标记方式：
1.  **Future**: 从当前时间步 $t$ 之后的轨迹中随机选取一个状态作为新目标 $g'$。（这是标准做法，效果最好）。
2.  **Final**: 仅使用轨迹的最后一个状态 $s_T$ 作为 $g'$。
3.  **Episode**: 从整个轨迹中随机选取一个状态。
4.  **Random**: 随机生成一个全新的目标（效果通常不好）。

### 隐式课程学习 (Implicit Curriculum Learning)
从理论上讲，HER 实现了一种自动的课程学习。
*   **初期**：智能体很笨，只能碰到离起始点很近的状态。HER 把这些近的状态设为目标，智能体学会了如何“走一小步”。
*   **中期**：学会走一小步后，智能体能探索到稍远一点的状态。HER 再次把稍远的状态设为目标。
*   **后期**：随着能力扩展，智能体最终能学会到达真正的远距离目标 $g$。

这种奖励信号从容易到困难的自动传播，是 HER 解决稀疏奖励问题的本质原因。

---

##  总结

目标导向 RL 将强化学习从“解决一个问题”提升到了“解决一类问题”。

*   **UVFA** 提供了网络结构的理论基础，让 $V(s,g)$ 成为可能。
*   **HER** 解决了最为棘手的稀疏奖励问题，通过**事后重标记**，将每一次失败的尝试都转化为另一次成功的训练数据。

至此，我们已经掌握了让智能体学会“指哪打哪”的关键技术。