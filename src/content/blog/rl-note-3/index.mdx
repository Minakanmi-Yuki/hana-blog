---
title: "RL笔记（3）：马尔可夫决策过程"
publishDate: 2025-12-23
updatedDate: 2025-12-23
description: "马尔可夫决策过程"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 马尔可夫过程（Markov Process）

### 随机过程（Stochastic Process）
随机过程是研究随时间演变的随机现象。
在随机过程中，随机现象在$$t$$时刻的取值为$S_t$，并且$S_t$通常会取决于之前的状态 $(S_1,...,S_{t-1})$。
在已知 $(S_1,...,S_{t-1})$的情况下，下一个时刻 $t$的状态为 $S_t$的概率可以表示为$P(S_t | S_1,...,S_{t-1})$。

### 马尔可夫性质（Markov Property）
当且仅当某个时刻的状态只取决于上个时刻的状态时，这个随机过程满足马尔可夫性质，即$P(S_t | S_{t-1}) = P(S_t | S_1, ..., S_{t-1})$。

### 马尔可夫过程（Markov Process）
马尔可夫过程就是满足了马尔可夫性质的随机过程，也叫马尔可夫链（Markov Chain）。
通常使用一个二元组 $<\mathcal{S}, \mathcal{P}>$ 来描述马尔可夫过程，其中 $\mathcal{S}$是有限数量的状态集合， $\mathcal{P}$是状态转移矩阵（State Transition Matrix）。
假设 $|\mathcal{S}|=n$，那么有：

$$
\mathcal{P}=\begin{bmatrix} P(s_1|s_1) & \cdots & P(s_n|s_1) 
\cr \vdots & \ddots & \vdots \cr P(s_n|s_1) & \cdots & P(s_n|s_n) \end{bmatrix}
$$

且对于任意 $i$行，满足 $\sum_{j=1}^{n} P(s_j|s_i)=1$。
特别的，当一个状态 $s_t$ 满足 $P(s_t|s_t)=1$，那么称 $s_t$为终止状态（Terminal State）。
给定马尔可夫过程，从某个状态 $S_1$ 出发，根据状态转移矩阵 $\mathcal{P}$ 得出一段状态序列 $S_1 S_2...S_T$（Episode），这个过程也叫做采样（Sampling）。

## 马尔可夫奖励过程（Markov Reward Process）
马尔可夫奖励过程基于马尔可夫过程加入了奖励函数 $\mathcal{R}$（Reward Function）与折扣因子 $$\gamma$$（Discount Factor），通常由四元组 $<\mathcal{S},\mathcal{P},\mathcal{R},\gamma>$ 构成：
- $\mathcal{S}$ 是有限状态集；
- $\mathcal{P}$ 是状态转移矩阵；
- $\mathcal{R}$ 是奖励函数，$\mathcal{R}(s)$ 表示达到状态 $s$ 可以获得的奖励；
- $\gamma$ 是折扣因子，取值为 $[0,1)$。 $\gamma$ 的越小，对于未来的利益折扣越大；

### 回报（Return）
在一个马尔可夫奖励过程中，在某个时刻 $t$，其回报 $G_t$ 的定义如下：
$$
G_t=R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+...=\sum_{k=0}^\infin \gamma^k R_{t+k}
$$
其中， $R_t$ 表示 $t$ 时刻获得的奖励。

### 价值函数（Value Function）
某个状态 $s$的期望回报 $\mathbb{E}[G_t|S_t=s]$，被称为这个状态的价值（Value）。那么所有状态的价值组成了价值函数（Value Function），展开如下：
$$
\begin{align}
V(s)&=\mathbb{E}[G_t|S_t=s] \notag \\
&=\mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+...|S_t=s] \notag \\
&=\mathbb{E}[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+...)|S_t=s] \notag \\
&=\mathbb{E}[R_t+\gamma G_{t+1}|S_t=s] \notag \\
&= \mathbb{E}[R_t+\gamma V(S_{t+1})|S_t=s] \notag \\
\end{align}
$$
经过几步简单的推导，可以得出贝尔曼方程（Bellman Equation）：
$$
\begin{align}
V(s)&= \mathbb{E}[R_t+\gamma V(S_{t+1})|S_t=s] \notag \\
&= \mathbb{E}[R_t|S_t=s]+\gamma\mathbb{E}[V(S_{t+1})|S_t=s] \notag \\
&= r(s)+\gamma\sum_{s^\prime \in \mathcal{S}} P(s^\prime|s) V(s^\prime) \notag
\end{align}
$$
可以写成矩阵形式：
$$
\begin{align}
\mathcal{V}&=\mathcal{R}+\gamma\mathcal{P}\mathcal{V} \notag \\
(I-\gamma\mathcal{P})\mathcal{V}&=\mathcal{R} \notag \\
\mathcal{V}&=(I-\gamma\mathcal{P})^{-1}\mathcal{R} \notag
\end{align}
$$

## 马尔可夫决策过程（Markov Decision Process）
在马尔可夫奖励过程的基础上，加入外界的刺激，即智能体（Agent）的动作（Action），就有了马尔可夫决策过程（Markov Decision Process）。
马尔可夫决策由五元组 $<\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma>$构成，分别是：
-  $\mathcal{S}$ 是有限状态集；
-  $\mathcal{A}$ 是动作的集合；
-  $\gamma$ 是折扣因子；
-  $\mathcal{R}(s,a)$ 是奖励函数；
-  $\mathcal{P}(s^\prime|s,a)$ 是状态转移函数；

### 策略（Policy）
策略的定义：智能体根据当前的状态 $S_t$，并从动作集合 $\mathcal{A}$中选取一个 $A_t$ 的函数，一般记为 $\pi$。
$\pi(a|s)=P(A_t=a|S_t=s)$ 表示状态 $s$ 下，选择动作 $a$ 的概率。

#### 确定性策略（Deterministic Policy）
如果一个策略 $\pi$ ，对于每一个状态 $s\in\mathcal{S}$ ，有且仅有唯一动作 $a\in\mathcal{A}$ ，使得 $\pi(a|s)=1$ ，那么这个策略就是一个确定性策略。

#### 随机性策略（Stochastic Policy）
随机性策略和确定性策略相反，它输出的各个动作的概率分布（Probability  Distribution），每次采取动作时会从分布中进行采样。