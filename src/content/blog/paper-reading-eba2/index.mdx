---
title: 'Paper Reading: Embodied AI 2'
publishDate: 2025-12-26
updatedDate: 2026-01-06
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

import { ManualTOC } from '@/components/advanced'

<ManualTOC
  title=''
  categories={[
    {
      title: 'Embodied AI Paper Reading',
      items: [
        {
          title: 'Batch 1',
          href: '/blog/paper-reading-eba1',
          order: '1'
        },
        {
          title: 'Batch 2',
          href: '/blog/paper-reading-eba2',
          order: '2'
        },
        {
          title: 'Batch 3',
          href: '/blog/paper-reading-eba3',
          order: '3'
        },
        {
          title: 'Batch 4',
          href: '/blog/paper-reading-eba4',
          order: '4'
        }
      ]
    }
  ]}
/>

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## LLARVA

<ArxivRating id='2406.11815' tldr='OpenVLA-like 的 VLA，使用 2D轨迹 预测去引导动作生成。' rank={2}>

![](https://pic.hana0721.top/LLARVA.8z70f5s0j6.webp)
LLARVA 是一种 OpenVLA-like 的 VLA，使用 Instruction Tuning 进行微调，并通过输出 2D Visual Trace 去引导 Action 生成。

![](https://pic.hana0721.top/LLARVA-prompt.7axnhz1uu9.webp)
LLARVA 的输入指令被规范化为 Prompt，详细见图。
LLM 的主干为 LLaMA2 7B，并使用 LoRA 进行微调。
先预测 2D Visual Trace 再生成 Action Chunk，本质上是基于 CoT 的思想。

</ArxivRating>

## ATM

<ArxivRating id='2401.00025' tldr='使用 Label-free 的视频去训练以当前观察 Obs 和指令 Instruction 为 Condition 的 Points-Tracker，并使用 Points-Track 去引导 Action 生成。' rank={4}>

![](https://pic.hana0721.top/ATM.83aizqrop9.webp)
ATM 的亮点在于仅可以使用 Action-free 的数据去训练机器人操作策略。
ATM 是一种 Pipeline 方法，分为任意点轨迹建模与轨迹引导策略学习两个部分。
在任意点轨迹建模部分，ATM 利用了大量 Label-free 的视频训练了一个 Points-Tracker，输入是当前观察 Obs、任务 Instruction 以及初始 Points，输出是未来的 Points-Trajectory。
在轨迹引导策略学习部分，策略网络以上个步骤预测的 Points-Trajectory 以及当前 Obs 为输入，输出机器人的 Action。

任意点轨迹建模：
在数据处理部分，ATM 使用了离线的 Points-Tracker 模型去得出的 Label。
同时，过滤掉了大部分静止的 Points，只保留了 32 个高频活动的 Points。
对于当前 Obs，使用 ViT 将图像切分为 Patches，并随机 Mask 掉 50% 的 Patches，得出 Image Token。
对于任务 Instruction，使用 BERT 进行信息提取 Language Token。
对于初始 Points，在文中是有32个，编码为 Track Token。
以 Obs 以及 Instruction 为 Base 的 Points-Tracker 被定义为一个 Transformer，输入为 Image Token、Language Token 以及 Track Token，输出为未来的 Points-Trajectory。
按照题主的理解，输出的 Points-Trajectory 其实就是任务导向的 Vision-Language Fusion。

![](https://pic.hana0721.top/ATM-policy.9kgo1hvysf.webp)
轨迹引导策略学习：
首先，使用了 BERT-like 的 Transformer 对 Image Token 以及 Track Token 进行 Fusion，得出特征 \[CLS\]。
然后，策略网络设定为 MLP，以 \[CLS\] 以及 Track Token 为输入，输出机器人的 Action。
在策略学习的过程中，并没有引入 Instruction 的信息，因为在轨迹建模阶段已经引入了。
同时，对 Track Token 进行了两次 Fusion，这样会有更好的效果，详细见原文。

</ArxivRating>

## Track2Act

<ArxivRating id='2405.01527' tldr='使用 Points-Trajectory 去生成动作，并使用残差策略修正动作。' rank={2}>

Track2Act 本质上和 ATM 是一样的，都是以 Points-Trajectory 去引导动作的生成。
Track2Act 主要分为三个阶段：Points-Trajectory 预测、Open-loop 动作求解以及 Residual Policy 闭环修正。

![](https://pic.hana0721.top/Track2Act-pt.13m9i5xvjf.webp)
在 Points-Trajectory 预测阶段，Track2Act 使用了 DiT 生成模型。
DiT 的 Condition 被设置为初始图像 $I_0$ 以及目标图像 $\mathcal{G}$ 的表征，给定随机采样的查询点集 $P_0$，生成 Points-Trajectory。

在 Open-loop 动作求解阶段，Track2Act 使用了一个确定性的算法进行刚性动作的求解，不涉及神经网络训练。
这个算法的输入是 Points-Trajectory，输出一系列的刚性变换 $[\textbf{T}_t]_{t=1}^H$。
具体求解算法见原文，这里不详细展开。
最后，基于刚性变换，可以求解出机器人的 Open-loop 动作序列 $[\bar{a}_t]_{t=1}^H$。

![](https://pic.hana0721.top/Track2Act-rp.4n877z0ttz.webp)
在 Residual Policy 闭环修正阶段，Track2Act 使用一个 Residual Policy 去修正 Open-loop 动作。其实就是使用一个 Transformer-Encoder 去预测误差，最后加在一起。

最后简单瑞萍一下，其实博主觉得后面两个阶段完成可以使用神经网络进行表征，有点多余，虽然残差修正动作这个思想是好的。

</ArxivRating>

## ECoT

<ArxivRating id='2407.08693' tldr='具有 Embodied 性质的 CoT。' rank={3}>

![](https://pic.hana0721.top/ECoT.73ufntxfff.webp)
ECoT 这篇工作指出 LLM 那边的 CoT 是无法直接迁移到 Embodied 任务环境中的。
若只是简单的语义推理，那么就会出现大量不符合实际物理环境的幻觉。
因此，ECoT 在 CoT 的基础上添加了智能体对环境的感知，从而使得 CoT 具有 Embodied 性质。
ECoT 的具体设置见上图，和 CoT 类似，只是加入了 Embodied 的感知。
ECoT 使用的模型基座是 OpenVLA，使用 ECoT 后效果得到了很好的改善。

![](https://pic.hana0721.top/ECoT-data.1vz50u4k0o.webp)
ECoT 还提出了一套使用机器人数据产生 CoT 训练数据的 Pipeline。
具体就不多说了，图里面说的很清楚。

这种推理过程是显式的，具有一定的解释性，缺点就是这样会降低推理的速度，文中也给了一些解决方案。

</ArxivRating>

## VoxPoser

<ArxivRating id='2307.05973' tldr='使用 LLM 生成 Python 代码去调用 VLM 的 API 计算 3D Value Map，并作为价值引导进行 MPC。' rank={2}>

VoxPoser 是一种 Code for Robotic 的方法。
简单的来说，VoxPoser 使用 LLM 去生成 Python 代码，这里的代码会调用 VLM 的 API 生成 3D Value Map，然后进行 MPC。

![](https://pic.hana0721.top/VoxPoser.1vz50wovrd.webp)
首先，VoxPoser 会告诉 LLM 任务的文本描述以及一些 VLM 具有的 API函数，让它去产生能够基于 RGB-D 图像计算 3D Value Map 的 Python 代码。
然后，使用 Python 执行器去执行代码，过程中会调用 VLM 的 API，其实就是 OWL-ViT 获取边框还有 SAM 获取 Mask，然后重建 3D Points Cloud，计算得出 3D Value Map。
最后，有了 3D Value Map 作为价值引导，就可以使用 MPC 的方法合成轨迹，进行规划。
在文中，提供了在线训练 MPC 以及 启发式 MPC 两种方式，前者需要环境交互，后者则无需训练。

</ArxivRating>

## PIVOT

<ArxivRating id='2402.07872' tldr='' rank={0}>

</ArxivRating>

## Code As Policies

<ArxivRating id='2209.07753' tldr='' rank={0}>

</ArxivRating>

## RoboPoint

<ArxivRating id='2406.10721' tldr='' rank={0}>

</ArxivRating>

## GR-1

<ArxivRating id='2312.13139' tldr='' rank={0}>

</ArxivRating>

## HPT

<ArxivRating id='2409.20537' tldr='' rank={0}>

</ArxivRating>

## RoboDual

<ArxivRating id='2410.08001' tldr='' rank={0}>

</ArxivRating>

## GR-2

<ArxivRating id='2410.06158' tldr='' rank={0}>

</ArxivRating>

## Humanoid Manipulation

<ArxivRating id='2410.10803' tldr='' rank={0}>

</ArxivRating>

## Surfer

<ArxivRating id='2306.11335' tldr='' rank={0}>

</ArxivRating>

## SceneVerse

<ArxivRating id='2401.09340' tldr='' rank={0}>

</ArxivRating>

## Robot See Robot Do

<ArxivRating id='2409.18121' tldr='' rank={0}>

</ArxivRating>

## ReKep

<ArxivRating id='2409.01652' tldr='' rank={0}>

</ArxivRating>

## OmniManip

<ArxivRating id='2501.03841' tldr='' rank={0}>

</ArxivRating>

## SOFAR

<ArxivRating id='2502.13143' tldr='' rank={0}>

</ArxivRating>

## PIVOT-R

<ArxivRating id='2410.10394' tldr='' rank={0}>

</ArxivRating>

## ManipGen

<ArxivRating id='2410.22332' tldr='' rank={0}>

</ArxivRating>

## DemoGen

<ArxivRating id='2502.16932' tldr='' rank={0}>

</ArxivRating>

## ArticuBot

<ArxivRating id='2503.03045' tldr='' rank={0}>

</ArxivRating>

## LAPA

<ArxivRating id='2410.11758' tldr='' rank={0}>

</ArxivRating>

## GR00T

<ArxivRating id='2503.14734' tldr='' rank={0}>

</ArxivRating>
