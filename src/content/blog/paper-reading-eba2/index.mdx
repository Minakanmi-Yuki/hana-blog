---
title: 'Paper Reading: Embodied AI 2'
publishDate: 2025-12-26
updatedDate: 2026-01-06
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

import { ManualTOC } from '@/components/advanced'

<ManualTOC
  title=''
  categories={[
    {
      title: 'Embodied AI Paper Reading',
      items: [
        {
          title: 'Batch 1',
          href: '/blog/paper-reading-eba1',
          order: '1'
        },
        {
          title: 'Batch 2',
          href: '/blog/paper-reading-eba2',
          order: '2'
        },
        {
          title: 'Batch 3',
          href: '/blog/paper-reading-eba3',
          order: '3'
        },
        {
          title: 'Batch 4',
          href: '/blog/paper-reading-eba4',
          order: '4'
        }
      ]
    }
  ]}
/>

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## LLARVA

<ArxivRating id='2406.11815' tldr='OpenVLA-like 的 VLA，使用 2D轨迹 预测去引导动作生成。' rank={2}>

![](https://pic.hana0721.top/LLARVA.8z70f5s0j6.webp)
LLARVA 是一种 OpenVLA-like 的 VLA，使用 Instruction Tuning 进行微调，并通过输出 2D Visual Trace 去引导 Action 生成。

![](https://pic.hana0721.top/LLARVA-prompt.7axnhz1uu9.webp)
LLARVA 的输入指令被规范化为 Prompt，详细见图。
LLM 的主干为 LLaMA2 7B，并使用 LoRA 进行微调。
先预测 2D Visual Trace 再生成 Action Chunk，本质上是基于 CoT 的思想。

</ArxivRating>

## ATM

<ArxivRating id='2401.00025' tldr='使用 Label-free 的视频去训练以当前观察 Obs 和指令 Instruction 为 Condition 的 Points-Tracker，并使用 Points-Track 去引导 Action 生成。' rank={4}>

![](https://pic.hana0721.top/ATM.83aizqrop9.webp)
ATM 的亮点在于仅可以使用 Action-free 的数据去训练机器人操作策略。
ATM 是一种 Pipeline 方法，分为任意点轨迹建模与轨迹引导策略学习两个部分。
在任意点轨迹建模部分，ATM 利用了大量 Label-free 的视频训练了一个 Points-Tracker，输入是当前观察 Obs、任务 Instruction 以及初始 Points，输出是未来的 Points-Trajectory。
在轨迹引导策略学习部分，策略网络以上个步骤预测的 Points-Trajectory 以及当前 Obs 为输入，输出机器人的 Action。

任意点轨迹建模：
在数据处理部分，ATM 使用了离线的 Points-Tracker 模型去得出的 Label。
同时，过滤掉了大部分静止的 Points，只保留了 32 个高频活动的 Points。
对于当前 Obs，使用 ViT 将图像切分为 Patches，并随机 Mask 掉 50% 的 Patches，得出 Image Token。
对于任务 Instruction，使用 BERT 进行信息提取 Language Token。
对于初始 Points，在文中是有32个，编码为 Track Token。
以 Obs 以及 Instruction 为 Base 的 Points-Tracker 被定义为一个 Transformer，输入为 Image Token、Language Token 以及 Track Token，输出为未来的 Points-Trajectory。
按照题主的理解，输出的 Points-Trajectory 其实就是任务导向的 Vision-Language Fusion。

![](https://pic.hana0721.top/ATM-policy.9kgo1hvysf.webp)
轨迹引导策略学习：
首先，使用了 BERT-like 的 Transformer 对 Image Token 以及 Track Token 进行 Fusion，得出特征 \[CLS\]。
然后，策略网络设定为 MLP，以 \[CLS\] 以及 Track Token 为输入，输出机器人的 Action。
在策略学习的过程中，并没有引入 Instruction 的信息，因为在轨迹建模阶段已经引入了。
同时，对 Track Token 进行了两次 Fusion，这样会有更好的效果，详细见原文。

</ArxivRating>

## Track2Act

<ArxivRating id='2405.01527' tldr='使用 Points-Trajectory 去生成动作，并使用残差策略修正动作。' rank={2}>

Track2Act 本质上和 ATM 是一样的，都是以 Points-Trajectory 去引导动作的生成。
Track2Act 主要分为三个阶段：Points-Trajectory 预测、Open-loop 动作求解以及 Residual Policy 闭环修正。

![](https://pic.hana0721.top/Track2Act-pt.13m9i5xvjf.webp)
在 Points-Trajectory 预测阶段，Track2Act 使用了 DiT 生成模型。
DiT 的 Condition 被设置为初始图像 $I_0$ 以及目标图像 $\mathcal{G}$ 的表征，给定随机采样的查询点集 $P_0$，生成 Points-Trajectory。

在 Open-loop 动作求解阶段，Track2Act 使用了一个确定性的算法进行刚性动作的求解，不涉及神经网络训练。
这个算法的输入是 Points-Trajectory，输出一系列的刚性变换 $[\textbf{T}_t]_{t=1}^H$。
具体求解算法见原文，这里不详细展开。
最后，基于刚性变换，可以求解出机器人的 Open-loop 动作序列 $[\bar{a}_t]_{t=1}^H$。

![](https://pic.hana0721.top/Track2Act-rp.4n877z0ttz.webp)
在 Residual Policy 闭环修正阶段，Track2Act 使用一个 Residual Policy 去修正 Open-loop 动作。其实就是使用一个 Transformer-Encoder 去预测误差，最后加在一起。

最后简单瑞萍一下，其实博主觉得后面两个阶段完成可以使用神经网络进行表征，有点多余，虽然残差修正动作这个思想是好的。

</ArxivRating>

## ECoT

<ArxivRating id='2407.08693' tldr='具有 Embodied 性质的 CoT。' rank={3}>

![](https://pic.hana0721.top/ECoT.73ufntxfff.webp)
ECoT 这篇工作指出 LLM 那边的 CoT 是无法直接迁移到 Embodied 任务环境中的。
若只是简单的语义推理，那么就会出现大量不符合实际物理环境的幻觉。
因此，ECoT 在 CoT 的基础上添加了智能体对环境的感知，从而使得 CoT 具有 Embodied 性质。
ECoT 的具体设置见上图，和 CoT 类似，只是加入了 Embodied 的感知。
ECoT 使用的模型基座是 OpenVLA，使用 ECoT 后效果得到了很好的改善。

![](https://pic.hana0721.top/ECoT-data.1vz50u4k0o.webp)
ECoT 还提出了一套使用机器人数据产生 CoT 训练数据的 Pipeline。
具体就不多说了，图里面说的很清楚。

这种推理过程是显式的，具有一定的解释性，缺点就是这样会降低推理的速度，文中也给了一些解决方案。

</ArxivRating>

## VoxPoser

<ArxivRating id='2307.05973' tldr='使用 LLM 生成 Python 代码去调用 VLM 的 API 计算 3D Value Map，并作为价值引导进行 MPC。' rank={2}>

VoxPoser 是一种 Code for Robotic 的方法。
简单的来说，VoxPoser 使用 LLM 去生成 Python 代码，这里的代码会调用 VLM 的 API 生成 3D Value Map，然后进行 MPC。

![](https://pic.hana0721.top/VoxPoser.1vz50wovrd.webp)
首先，VoxPoser 会告诉 LLM 任务的文本描述以及一些 VLM 具有的 API函数，让它去产生能够基于 RGB-D 图像计算 3D Value Map 的 Python 代码。
然后，使用 Python 执行器去执行代码，过程中会调用 VLM 的 API，其实就是 OWL-ViT 获取边框还有 SAM 获取 Mask，然后重建 3D Points Cloud，计算得出 3D Value Map。
最后，有了 3D Value Map 作为价值引导，就可以使用 MPC 的方法合成轨迹，进行规划。
在文中，提供了在线训练 MPC 以及 启发式 MPC 两种方式，前者需要环境交互，后者则无需训练。

</ArxivRating>

## PIVOT

<ArxivRating id='2402.07872' tldr='将低级 Action 投影到图像中，并采用 VQA 的任务形式去让 VLM 选择 Action。' rank={3}>

![](https://pic.hana0721.top/PIVOT-example.2rvmgdu2g2.webp)
PIVOT 的思想很简单，就是把机器人的低级 Action 投影到图像中，并采用 VQA 的形式让 VLM 去选择 Action 集合。

![](https://pic.hana0721.top/PIVOT.szfq1osvp.webp)
PIVOT 维护一个 Action 的分布，初始化为均匀分布，有点像 MPC 中的交叉熵方法。
PIVOT 使用的是 Zero-Shot 的迭代算法，不断去优化 Action 的分布，直到收敛。
首先，PIVOT 从分布中采样一系列的 Action，然后将 Action 投影到 2D 图像上，详细见图，每个 Action 都对应了序号。
然后，给定任务描述文本以及 2D 图像，编写合适的 Prompt 让 VLM 去选择最优 Action 集合。
接着，根据选取的 Action 集合去维护 Action 分布，类似于 MPC 中的交叉熵方法。
经过不断迭代，可以得出较好的 Action 候选集合。

PIVOT 的思路很有启发性，但缺陷也是明显的。比如，3D 歧义理解、细粒度控制以及 VLM 的幻觉问题。

</ArxivRating>

## Code As Policies

<ArxivRating id='2209.07753' tldr='让 LLM 输出 Python 代码，并进行完成机器人任务。' rank={3}>

CaP 核心思想不是训练策略网络，而是通过 Prompt 让 LLM 去调用机器人 API，从而完成任务。

![](https://pic.hana0721.top/CaP.77e1n0y43g.webp)
首先，CaP 进行了 LMP 形式化，其实就是让策略利用 Python 语言的一些特性。
然后，为了处理复杂任务代码过长或者逻辑复杂的问题，CaP 使用了层次化代码生成的方法，其实就是自下而上分而治之的思想。
最后，CaP 进行了感知与动作的对齐。
LLM 会基于 Prompt 得出一些预定义的感知 API 以及 控制 API。
LLM 可以通过感知 API 去获取视觉信息的变量。
LLM 会计算控制 API 的参数，并执行。

CaP 的工作虽然简单，但它将高层规划和底层执行联系在了一起。

</ArxivRating>

## RoboPoint

<ArxivRating id='2406.10721' tldr='使用 VLM 获取 Point Grounding，用于引导下游任务。' rank={2}>

RoboPoint 通过构建一套程序化合成数据生成流水线，将通用视觉语言模型（VLM）微调为能根据自然语言指令预测精确 2D 空间操作点（Spatial Affordance）的模型，从而实现了无需真实世界数据训练的机器人零样本（Zero-shot）操控与导航。

![](https://pic.hana0721.top/RoboPoint.lw7w2yur8.webp)
在数据生成阶段，RoboPoint 利用程序化生成技术构建多样化的 3D 仿真场景，通过利用模拟器中的精确几何信息自动计算物体间的空间关系，并创造性地采用“移除目标物体再采样”的策略来自动标注自由空间（Free Space），从而低成本地生成了海量包含“图像-指令-像素坐标点”的高质量合成训练数据。

在指令微调阶段，RoboPoint 套用 LLaVA 的架构，冻住视觉层只练语言模型，把“找坐标”直接转化成“文本生成”任务，让模型直接输出坐标数字
同时，为了防止了 VLM 的遗忘，加入了 VQA 数据进行 Co-Training。

在真实执行阶段，先使用 RoboPoint 计算出 RGB 图像上的 2D 坐标，然后结合深度图转换为 3D 坐标，最后直接套用一个预设好的抓取姿态，把目标传给传统的运动规划算法去解算路径，机械臂照做就行了。

RoboPoint 其实也是一种高层决策的规划，也是一种 System 2 的方法。

</ArxivRating>

## GR-1

<ArxivRating id='2312.13139' tldr='在视频数据上进行预训练，然后在机器人数据上微调，从而建立 World Model。' rank={0}>

GR-1 证明了视频生成是策略学习的高效代理任务，它通过大规模视频生成式预训练迫使 GPT 模型内化环境动态与物理先验，从而在下游机器人任务中实现了卓越的小样本学习与零样本泛化能力。

![](https://pic.hana0721.top/GR-1-all.1sfj4q0vsm.webp)
GR-1 采用 GPT-style Causal Transformer 架构处理多模态交织序列，先通过大规模视频生成预训练学习环境动态，再经由动作与未来帧联合预测任务进行微调，实现端到端的自回归策略学习。

![](https://pic.hana0721.top/GR-1-input.2vf8flx9ka.webp)
在预训练阶段，GR-1 利用大规模人类第一视角视频数据集（Ego4D）执行语言条件下的视频预测任务，旨在无需动作标注即可习得通用物理动态。
模型架构上冻结了 CLIP 文本编码器与 MAE-ViT 视觉编码器（辅以 Perceiver Resampler 压缩特征），将语言指令、历史图像帧与可学习的 \[OBS\] Token 交织输入至 GPT 主干网络。
训练过程中，模型屏蔽 \[ACT\] Token，仅通过因果掩码注意力机制利用 \[OBS\] Token 驱动视觉解码器（Vision Decoder）重建未来帧像素，从而迫使模型内化视觉演变规律与视语对齐关系，为下游机器人操作任务提供具备强泛化能力的 World Model 初始化权重。

在微调阶段，GR-1 加载预训练权重并扩展输入序列，将机器人本体状态（Proprioceptive State）与语言、图像交织输入，引入 \[ACT\] Token 用于回归 7-DoF 机械臂动作与夹爪状态。
训练采用多任务联合优化策略，在执行行为克隆（Behavior Cloning）学习策略分布的同时，保留未来帧预测（Video Prediction）作为辅助任务，通过联合最小化动作回归损失与图像重建损失，迫使策略在决策时显式利用预训练阶段习得的物理世界动态（World Model），从而实现感知到动作的高效迁移与对齐。

GR-1 的 Limitation 也是明显的。
因为要处理高维图像，所以导致推理延迟较高。
同时预训练的人类视频与机械臂控制之间存在形态鸿沟（Embodiment Gap），目前仅依靠微调隐式对齐而缺乏显式的动作重定向机制。

</ArxivRating>

## HPT

<ArxivRating id='2409.20537' tldr='对齐到 Cross-Embodied 的预训练 Transformer。' rank={3}>

HPT 使用一种模块化 Token 对齐机制将异构机器人的 Proprioception 与 Vision 映射到共享潜空间到通用策略架构，并在大规模 Cross-Embodied 数据上验证了机器人策略学习的 Scaling Laws 以及迁移能力。

HPT 采用了一种高度模块化的 Stem-Trunk-Head 设计范式来解决 Cross-Embodied 难题。

![](https://pic.hana0721.top/HPT-Stem.8z70jpnbae.webp)
在 Stem 模块中，利用 Cross-Attention 将异构的 Proprioception 与 Vision 对齐到统一的 Embodied 潜在表征。首先，机器人的 Vision 信息被编码成 Cross-Attention 中的 K 与 V，并与一组可学习的 Q 进行聚合得出 Vision Token。同样，机器人的 Proprioception 也经过相同的操作，得出 Proprio. Token。不同的 Embodied 对应了不同 Stem，是分别进行学习的。

![](https://pic.hana0721.top/HPT.2yyuezdjdc.webp)
在 Trunk 模块中，所有 Embodied 任务共享一个 Transformer，负责将 Stem 对齐后的多模态 Token 进行深度融合和推理。最终，经过池化后得出最终 Latent。

在 Head 模块中，负责将 Trunk 得出 Latent 进行解码，这里的解码器可以是 MLP、DP 以及 ACT，得出机器人的 Action。请注意，异构的机器人分别对应了不同的 Head。

在训练数据上，HPT 使用 OXE、Simulation 以及 Human Videos 等。
HPT 使用了最暴力的 BC 方法，证明了其 Scaling Law 的存在。

</ArxivRating>

## RoboDual

<ArxivRating id='2410.08001' tldr='一种 Generalist 与 Specialist 相结合的 Double System 机器人操作系统' rank={2}>

![](https://pic.hana0721.top/RoboDual-Overview.86u6gy6bzq.webp)
RoboDual 的整体架构由负责 High-level 决策的 Generalist 与负责 Low-level 决策的 Speicalist 组成。
其中，Generalist 是类似于 OpenVLA 的 VLA 模型，虽然具有一定的泛化能力，但推理效率比较低。
而 Specialist 是一种类似于 ACT 或是 DiT 的决策模型，在特定领域上有比较好的效果，且推理效率高，但难以适应新的任务环境。
RoboDual 做的事情就是将两者结合在一起，Generalist 负责 High-level 的任务规划，而 Specialist 进行 Low-level 的实时决策。

![](https://pic.hana0721.top/RoboDual.et1hb05w6.webp)
RoboDual 的具体实现很简单，就是将 Generalist 归纳出的 Task Latent 与 Action Latnet 作为 Specialist 的 Condition，然后让 Specialist 输出 Action Chunk。
在文章中，Generalist 是 OpenVLA 架构，Specialist 是 DiT 架构。
由于 Generalist 和 Specialist 的推理效率不同，RoboDual 在推理过程中使用双线程的方式，即 Generalist 和 Specialist 分别使用独立的线程。
在实际的使用中，Specialist 拿到的 Generalist 的提示总是有延迟的。因此，RoboDual 在实际训练中采用了延迟感知训练的 Trick。

实际上，RoboDual 在对 OpenVLA 进行了一定修改，让其输出 Action Chunk。因此，Specialist 是有一段比较粗略的动作轨迹作为指导的，它只是进一步地细化这段动作轨迹，相当于 Specialist 对 Generalist 的输出结果进行插值。

</ArxivRating>

## GR-2

<ArxivRating id='2410.06158' tldr='' rank={2}>

![](https://pic.hana0721.top/GR-2.7i0wy0nlxr.webp)
GR-2 的思路和 GR-1 的一样，只是扩展了更多的优质数据集。
具体做法分为两个阶段。
首先，在大规模的 Web 级别的 Video 数据集预训练一个 World Model。
然后，在机器人数据上进行微调，得出一个 End-to-end 的 VLA 模型。

</ArxivRating>

## Humanoid Manipulation

<ArxivRating id='2410.10803' tldr='' rank={0}>

</ArxivRating>

## Surfer

<ArxivRating id='2306.11335' tldr='' rank={0}>

</ArxivRating>

## SceneVerse

<ArxivRating id='2401.09340' tldr='' rank={0}>

</ArxivRating>

## Robot See Robot Do

<ArxivRating id='2409.18121' tldr='' rank={0}>

</ArxivRating>

## ReKep

<ArxivRating id='2409.01652' tldr='' rank={0}>

</ArxivRating>

## OmniManip

<ArxivRating id='2501.03841' tldr='' rank={0}>

</ArxivRating>

## SOFAR

<ArxivRating id='2502.13143' tldr='' rank={0}>

</ArxivRating>

## PIVOT-R

<ArxivRating id='2410.10394' tldr='' rank={0}>

</ArxivRating>

## ManipGen

<ArxivRating id='2410.22332' tldr='' rank={0}>

</ArxivRating>

## DemoGen

<ArxivRating id='2502.16932' tldr='' rank={0}>

</ArxivRating>

## ArticuBot

<ArxivRating id='2503.03045' tldr='' rank={0}>

</ArxivRating>

## LAPA

<ArxivRating id='2410.11758' tldr='' rank={0}>

</ArxivRating>

## GR00T

<ArxivRating id='2503.14734' tldr='' rank={0}>

</ArxivRating>
