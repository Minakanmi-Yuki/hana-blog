---
title: 'Paper Reading: Embodied AI 2'
publishDate: 2025-12-26
updatedDate: 2026-01-06
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

import { ManualTOC } from '@/components/advanced'

<ManualTOC
  title=''
  categories={[
    {
      title: 'Embodied AI Paper Reading',
      items: [
        {
          title: 'Batch 1',
          href: '/blog/paper-reading-eba1',
          order: '1'
        },
        {
          title: 'Batch 2',
          href: '/blog/paper-reading-eba2',
          order: '2'
        },
        {
          title: 'Batch 3',
          href: '/blog/paper-reading-eba3',
          order: '3'
        },
        {
          title: 'Batch 4',
          href: '/blog/paper-reading-eba4',
          order: '4'
        }
      ]
    }
  ]}
/>

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## LLARVA

<ArxivRating id='2406.11815' tldr='OpenVLA-like 的 VLA，使用 2D轨迹 预测去引导动作生成。' rank={2}>

![](https://pic.hana0721.top/LLARVA.8z70f5s0j6.webp)
LLARVA 是一种 OpenVLA-like 的 VLA，使用 Instruction Tuning 进行微调，并通过输出 2D Visual Trace 去引导 Action 生成。

![](https://pic.hana0721.top/LLARVA-prompt.7axnhz1uu9.webp)
LLARVA 的输入指令被规范化为 Prompt，详细见图。
LLM 的主干为 LLaMA2 7B，并使用 LoRA 进行微调。
先预测 2D Visual Trace 再生成 Action Chunk，本质上是基于 CoT 的思想。

</ArxivRating>

## ATM

<ArxivRating id='2401.00025' tldr='使用 Label-free 的视频去训练以当前观察 Obs 和指令 Instruction 为 Condition 的 Points-Tracker，并使用 Points-Track 去引导 Action 生成。' rank={4}>

![](https://pic.hana0721.top/ATM.83aizqrop9.webp)
ATM 的亮点在于仅可以使用 Action-free 的数据去训练机器人操作策略。
ATM 是一种 Pipeline 方法，分为任意点轨迹建模与轨迹引导策略学习两个部分。
在任意点轨迹建模部分，ATM 利用了大量 Label-free 的视频训练了一个 Points-Tracker，输入是当前观察 Obs、任务 Instruction 以及初始 Points，输出是未来的 Points-Trajectory。
在轨迹引导策略学习部分，策略网络以上个步骤预测的 Points-Trajectory 以及当前 Obs 为输入，输出机器人的 Action。

任意点轨迹建模：
在数据处理部分，ATM 使用了离线的 Points-Tracker 模型去得出的 Label。
同时，过滤掉了大部分静止的 Points，只保留了 32 个高频活动的 Points。
对于当前 Obs，使用 ViT 将图像切分为 Patches，并随机 Mask 掉 50% 的 Patches，得出 Image Token。
对于任务 Instruction，使用 BERT 进行信息提取 Language Token。
对于初始 Points，在文中是有32个，编码为 Track Token。
以 Obs 以及 Instruction 为 Base 的 Points-Tracker 被定义为一个 Transformer，输入为 Image Token、Language Token 以及 Track Token，输出为未来的 Points-Trajectory。
按照题主的理解，输出的 Points-Trajectory 其实就是任务导向的 Vision-Language Fusion。

![](https://pic.hana0721.top/ATM-policy.9kgo1hvysf.webp)
轨迹引导策略学习：
首先，使用了 BERT-like 的 Transformer 对 Image Token 以及 Track Token 进行 Fusion，得出特征 \[CLS\]。
然后，策略网络设定为 MLP，以 \[CLS\] 以及 Track Token 为输入，输出机器人的 Action。
在策略学习的过程中，并没有引入 Instruction 的信息，因为在轨迹建模阶段已经引入了。
同时，对 Track Token 进行了两次 Fusion，这样会有更好的效果，详细见原文。

</ArxivRating>

## Track2Act

<ArxivRating id='2405.01527' tldr='使用 Points-Trajectory 去生成动作，并使用残差策略修正动作。' rank={2}>

Track2Act 本质上和 ATM 是一样的，都是以 Points-Trajectory 去引导动作的生成。
Track2Act 主要分为三个阶段：Points-Trajectory 预测、Open-loop 动作求解以及 Residual Policy 闭环修正。

![](https://pic.hana0721.top/Track2Act-pt.13m9i5xvjf.webp)
在 Points-Trajectory 预测阶段，Track2Act 使用了 DiT 生成模型。
DiT 的 Condition 被设置为初始图像 $I_0$ 以及目标图像 $\mathcal{G}$ 的表征，给定随机采样的查询点集 $P_0$，生成 Points-Trajectory。

在 Open-loop 动作求解阶段，Track2Act 使用了一个确定性的算法进行刚性动作的求解，不涉及神经网络训练。
这个算法的输入是 Points-Trajectory，输出一系列的刚性变换 $[\textbf{T}_t]_{t=1}^H$。
具体求解算法见原文，这里不详细展开。
最后，基于刚性变换，可以求解出机器人的 Open-loop 动作序列 $[\bar{a}_t]_{t=1}^H$。

![](https://pic.hana0721.top/Track2Act-rp.4n877z0ttz.webp)
在 Residual Policy 闭环修正阶段，Track2Act 使用一个 Residual Policy 去修正 Open-loop 动作。其实就是使用一个 Transformer-Encoder 去预测误差，最后加在一起。

最后简单瑞萍一下，其实博主觉得后面两个阶段完成可以使用神经网络进行表征，有点多余，虽然残差修正动作这个思想是好的。

</ArxivRating>

## ECoT

<ArxivRating id='2407.08693' tldr='具有 Embodied 性质的 CoT。' rank={3}>

![](https://pic.hana0721.top/ECoT.73ufntxfff.webp)
ECoT 这篇工作指出 LLM 那边的 CoT 是无法直接迁移到 Embodied 任务环境中的。
若只是简单的语义推理，那么就会出现大量不符合实际物理环境的幻觉。
因此，ECoT 在 CoT 的基础上添加了智能体对环境的感知，从而使得 CoT 具有 Embodied 性质。
ECoT 的具体设置见上图，和 CoT 类似，只是加入了 Embodied 的感知。
ECoT 使用的模型基座是 OpenVLA，使用 ECoT 后效果得到了很好的改善。

![](https://pic.hana0721.top/ECoT-data.1vz50u4k0o.webp)
ECoT 还提出了一套使用机器人数据产生 CoT 训练数据的 Pipeline。
具体就不多说了，图里面说的很清楚。

这种推理过程是显式的，具有一定的解释性，缺点就是这样会降低推理的速度，文中也给了一些解决方案。

</ArxivRating>

## VoxPoser

<ArxivRating id='2307.05973' tldr='使用 LLM 生成 Python 代码去调用 VLM 的 API 计算 3D Value Map，并作为价值引导进行 MPC。' rank={2}>

VoxPoser 是一种 Code for Robotic 的方法。
简单的来说，VoxPoser 使用 LLM 去生成 Python 代码，这里的代码会调用 VLM 的 API 生成 3D Value Map，然后进行 MPC。

![](https://pic.hana0721.top/VoxPoser.1vz50wovrd.webp)
首先，VoxPoser 会告诉 LLM 任务的文本描述以及一些 VLM 具有的 API函数，让它去产生能够基于 RGB-D 图像计算 3D Value Map 的 Python 代码。
然后，使用 Python 执行器去执行代码，过程中会调用 VLM 的 API，其实就是 OWL-ViT 获取边框还有 SAM 获取 Mask，然后重建 3D Points Cloud，计算得出 3D Value Map。
最后，有了 3D Value Map 作为价值引导，就可以使用 MPC 的方法合成轨迹，进行规划。
在文中，提供了在线训练 MPC 以及 启发式 MPC 两种方式，前者需要环境交互，后者则无需训练。

</ArxivRating>

## PIVOT

<ArxivRating id='2402.07872' tldr='将低级 Action 投影到图像中，并采用 VQA 的任务形式去让 VLM 选择 Action。' rank={3}>

![](https://pic.hana0721.top/PIVOT-example.2rvmgdu2g2.webp)
PIVOT 的思想很简单，就是把机器人的低级 Action 投影到图像中，并采用 VQA 的形式让 VLM 去选择 Action 集合。

![](https://pic.hana0721.top/PIVOT.szfq1osvp.webp)
PIVOT 维护一个 Action 的分布，初始化为均匀分布，有点像 MPC 中的交叉熵方法。
PIVOT 使用的是 Zero-Shot 的迭代算法，不断去优化 Action 的分布，直到收敛。
首先，PIVOT 从分布中采样一系列的 Action，然后将 Action 投影到 2D 图像上，详细见图，每个 Action 都对应了序号。
然后，给定任务描述文本以及 2D 图像，编写合适的 Prompt 让 VLM 去选择最优 Action 集合。
接着，根据选取的 Action 集合去维护 Action 分布，类似于 MPC 中的交叉熵方法。
经过不断迭代，可以得出较好的 Action 候选集合。

PIVOT 的思路很有启发性，但缺陷也是明显的。比如，3D 歧义理解、细粒度控制以及 VLM 的幻觉问题。

</ArxivRating>

## Code As Policies

<ArxivRating id='2209.07753' tldr='让 LLM 输出 Python 代码，并进行完成机器人任务。' rank={3}>

CaP 核心思想不是训练策略网络，而是通过 Prompt 让 LLM 去调用机器人 API，从而完成任务。

![](https://pic.hana0721.top/CaP.77e1n0y43g.webp)
首先，CaP 进行了 LMP 形式化，其实就是让策略利用 Python 语言的一些特性。
然后，为了处理复杂任务代码过长或者逻辑复杂的问题，CaP 使用了层次化代码生成的方法，其实就是自下而上分而治之的思想。
最后，CaP 进行了感知与动作的对齐。
LLM 会基于 Prompt 得出一些预定义的感知 API 以及 控制 API。
LLM 可以通过感知 API 去获取视觉信息的变量。
LLM 会计算控制 API 的参数，并执行。

CaP 的工作虽然简单，但它将高层规划和底层执行联系在了一起。

</ArxivRating>

## RoboPoint

<ArxivRating id='2406.10721' tldr='使用 VLM 获取 Point Grounding，用于引导下游任务。' rank={2}>

RoboPoint 通过构建一套程序化合成数据生成流水线，将通用视觉语言模型（VLM）微调为能根据自然语言指令预测精确 2D 空间操作点（Spatial Affordance）的模型，从而实现了无需真实世界数据训练的机器人零样本（Zero-shot）操控与导航。

![](https://pic.hana0721.top/RoboPoint.lw7w2yur8.webp)
在数据生成阶段，RoboPoint 利用程序化生成技术构建多样化的 3D 仿真场景，通过利用模拟器中的精确几何信息自动计算物体间的空间关系，并创造性地采用“移除目标物体再采样”的策略来自动标注自由空间（Free Space），从而低成本地生成了海量包含“图像-指令-像素坐标点”的高质量合成训练数据。

在指令微调阶段，RoboPoint 套用 LLaVA 的架构，冻住视觉层只练语言模型，把“找坐标”直接转化成“文本生成”任务，让模型直接输出坐标数字
同时，为了防止了 VLM 的遗忘，加入了 VQA 数据进行 Co-Training。

在真实执行阶段，先使用 RoboPoint 计算出 RGB 图像上的 2D 坐标，然后结合深度图转换为 3D 坐标，最后直接套用一个预设好的抓取姿态，把目标传给传统的运动规划算法去解算路径，机械臂照做就行了。

RoboPoint 其实也是一种高层决策的规划，也是一种 System 2 的方法。

</ArxivRating>

## GR-1

<ArxivRating id='2312.13139' tldr='在视频数据上进行预训练，然后在机器人数据上微调，从而建立 World Model。' rank={0}>

GR-1 证明了视频生成是策略学习的高效代理任务，它通过大规模视频生成式预训练迫使 GPT 模型内化环境动态与物理先验，从而在下游机器人任务中实现了卓越的小样本学习与零样本泛化能力。

![](https://pic.hana0721.top/GR-1-all.1sfj4q0vsm.webp)
GR-1 采用 GPT-style Causal Transformer 架构处理多模态交织序列，先通过大规模视频生成预训练学习环境动态，再经由动作与未来帧联合预测任务进行微调，实现端到端的自回归策略学习。

![](https://pic.hana0721.top/GR-1-input.2vf8flx9ka.webp)
在预训练阶段，GR-1 利用大规模人类第一视角视频数据集（Ego4D）执行语言条件下的视频预测任务，旨在无需动作标注即可习得通用物理动态。
模型架构上冻结了 CLIP 文本编码器与 MAE-ViT 视觉编码器（辅以 Perceiver Resampler 压缩特征），将语言指令、历史图像帧与可学习的 \[OBS\] Token 交织输入至 GPT 主干网络。
训练过程中，模型屏蔽 \[ACT\] Token，仅通过因果掩码注意力机制利用 \[OBS\] Token 驱动视觉解码器（Vision Decoder）重建未来帧像素，从而迫使模型内化视觉演变规律与视语对齐关系，为下游机器人操作任务提供具备强泛化能力的 World Model 初始化权重。

在微调阶段，GR-1 加载预训练权重并扩展输入序列，将机器人本体状态（Proprioceptive State）与语言、图像交织输入，引入 \[ACT\] Token 用于回归 7-DoF 机械臂动作与夹爪状态。
训练采用多任务联合优化策略，在执行行为克隆（Behavior Cloning）学习策略分布的同时，保留未来帧预测（Video Prediction）作为辅助任务，通过联合最小化动作回归损失与图像重建损失，迫使策略在决策时显式利用预训练阶段习得的物理世界动态（World Model），从而实现感知到动作的高效迁移与对齐。

GR-1 的 Limitation 也是明显的。
因为要处理高维图像，所以导致推理延迟较高。
同时预训练的人类视频与机械臂控制之间存在形态鸿沟（Embodiment Gap），目前仅依靠微调隐式对齐而缺乏显式的动作重定向机制。

</ArxivRating>

## HPT

<ArxivRating id='2409.20537' tldr='对齐到 Cross-Embodied 的预训练 Transformer。' rank={3}>

HPT 使用一种模块化 Token 对齐机制将异构机器人的 Proprioception 与 Vision 映射到共享潜空间到通用策略架构，并在大规模 Cross-Embodied 数据上验证了机器人策略学习的 Scaling Laws 以及迁移能力。

HPT 采用了一种高度模块化的 Stem-Trunk-Head 设计范式来解决 Cross-Embodied 难题。

![](https://pic.hana0721.top/HPT-Stem.8z70jpnbae.webp)
在 Stem 模块中，利用 Cross-Attention 将异构的 Proprioception 与 Vision 对齐到统一的 Embodied 潜在表征。首先，机器人的 Vision 信息被编码成 Cross-Attention 中的 K 与 V，并与一组可学习的 Q 进行聚合得出 Vision Token。同样，机器人的 Proprioception 也经过相同的操作，得出 Proprio. Token。不同的 Embodied 对应了不同 Stem，是分别进行学习的。

![](https://pic.hana0721.top/HPT.2yyuezdjdc.webp)
在 Trunk 模块中，所有 Embodied 任务共享一个 Transformer，负责将 Stem 对齐后的多模态 Token 进行深度融合和推理。最终，经过池化后得出最终 Latent。

在 Head 模块中，负责将 Trunk 得出 Latent 进行解码，这里的解码器可以是 MLP、DP 以及 ACT，得出机器人的 Action。请注意，异构的机器人分别对应了不同的 Head。

在训练数据上，HPT 使用 OXE、Simulation 以及 Human Videos 等。
HPT 使用了最暴力的 BC 方法，证明了其 Scaling Law 的存在。

</ArxivRating>

## RoboDual

<ArxivRating id='2410.08001' tldr='一种 Generalist 与 Specialist 相结合的 Double System 机器人操作系统' rank={2}>

![](https://pic.hana0721.top/RoboDual-Overview.86u6gy6bzq.webp)
RoboDual 的整体架构由负责 High-level 决策的 Generalist 与负责 Low-level 决策的 Speicalist 组成。
其中，Generalist 是类似于 OpenVLA 的 VLA 模型，虽然具有一定的泛化能力，但推理效率比较低。
而 Specialist 是一种类似于 ACT 或是 DiT 的决策模型，在特定领域上有比较好的效果，且推理效率高，但难以适应新的任务环境。
RoboDual 做的事情就是将两者结合在一起，Generalist 负责 High-level 的任务规划，而 Specialist 进行 Low-level 的实时决策。

![](https://pic.hana0721.top/RoboDual.et1hb05w6.webp)
RoboDual 的具体实现很简单，就是将 Generalist 归纳出的 Task Latent 与 Action Latnet 作为 Specialist 的 Condition，然后让 Specialist 输出 Action Chunk。
在文章中，Generalist 是 OpenVLA 架构，Specialist 是 DiT 架构。
由于 Generalist 和 Specialist 的推理效率不同，RoboDual 在推理过程中使用双线程的方式，即 Generalist 和 Specialist 分别使用独立的线程。
在实际的使用中，Specialist 拿到的 Generalist 的提示总是有延迟的。因此，RoboDual 在实际训练中采用了延迟感知训练的 Trick。

实际上，RoboDual 在对 OpenVLA 进行了一定修改，让其输出 Action Chunk。因此，Specialist 是有一段比较粗略的动作轨迹作为指导的，它只是进一步地细化这段动作轨迹，相当于 Specialist 对 Generalist 的输出结果进行插值。

</ArxivRating>

## GR-2

<ArxivRating id='2410.06158' tldr='GR-1 之后的改进工作。' rank={2}>

![](https://pic.hana0721.top/GR-2.7i0wy0nlxr.webp)
GR-2 的思路和 GR-1 的一样，只是扩展了更多的优质数据集。
具体做法分为两个阶段。
首先，在大规模的 Web 级别的 Video 数据集预训练一个 World Model。
然后，在机器人数据上进行微调，得出一个 End-to-end 的 VLA 模型。

</ArxivRating>

## Humanoid Manipulation

<ArxivRating id='2410.10803' tldr='适用于通用机器人的 DP3 模型。' rank={2}>

{/* 这篇工作对 DP3 进行改进，并将其应用到通用机器人操作任务。 */}
{/* 该研究使用 Fourier GR1 作为实验平台，并固定住双腿，以确保稳定性。 */}
{/* 在数据采集方面，使用了遥操作收集。 */}
{/* 观测主要包含了 Camera 的 3D 点云以及机器人本体感知。 */}
{/* 在策略学习中，因为机器人的 Camera 并不是固定的，所以该研究提出了 iDP3，也就是以 Camera 作为参考系去调整 3D 点云的坐标，以适应任务环境。 */}
{/* 同时，使用卷积去代替 MLP 处理视觉信息会更加的平滑。 */}
{/* 最后，延长预测时域使得策略更加的平稳一致。 */}

![](https://pic.hana0721.top/iDP3.45i746rn30.webp)
这项工作对 DP3 算法进行了针对性改进，目的是让其能更好地服务于通用机器人的操作任务。
研究使用了 Fourier GR1 作为实验载体，为了保证操作过程中的绝对稳定，机器人的双腿被固定住，数据则通过遥操作的方式进行收集。
系统的输入端主要包含相机的 3D 点云以及机器人的本体感知数据。
在策略学习阶段，作者注意到机器人的相机并非固定不动，因此提出了名为 iDP3 的新方法，即以相机自身作为参考系去标准化 3D 点云的坐标，以此适应动态的任务环境。
此外，为了让视觉信息的处理更加平滑，算法使用卷积层代替了传统的 MLP。
最后配合延长的预测时域，成功使得机器人生成的策略动作更加平稳和连贯。

</ArxivRating>

## Surfer

<ArxivRating id='2306.11335' tldr='基于 World Model 的 VLA 模型。' rank={2}>

![](https://pic.hana0721.top/Surfer.175x1vj6ob.webp)
Surfer 提出了一个基于世界模型（World Model）的架构，旨在通过接收历史图像帧（History Frames）、语言指令（Instruction）以及机器人的本体感知信息（Proprioception）来预测下一步的动作（Action）及执行后的未来帧。在多模态特征编码方面，Surfer 采用了高效的处理策略：对于视觉输入，利用冻结的 CLIP 编码器将图像帧转化为 Tokens，并引入 TokenLearner 进行压缩以提升推理效率；对于文本输入，使用冻结的 Text Encoder 提取 Text Latent；对于本体感知信息，则通过 MLP 编码得到 Proprio Latent。

Surfer 的世界模型主体由两个基于多层 Transformer 的模块构成，分别负责动作预测与场景预测。在动作预测模块中，模型以历史帧的 Visual Tokens 作为 Query，将拼接后的 Text Latent 和 Proprio Latent 作为 Key 和 Value，通过 Cross-Attention 机制生成当前时间步的 Action。随后，帧预测模块复用历史帧 Tokens 作为 Query，并将刚刚预测出的 Action 作为 Key 和 Value，同样利用 Cross-Attention 推导出执行动作后的未来帧 Tokens，并采用 L2 Loss 进行监督训练。

总结来看，Surfer 的核心亮点在于引入了未来帧预测作为动作决策的辅助监督信号。这种设计符合直觉：如果模型生成的动作能够准确预测出环境的未来状态，说明该动作决策本身是符合物理逻辑的。然而，这种方法的局限性也显而易见：一旦帧预测模块不够准确，错误的预测反而会引入噪声，形成负面监督。此外，该工作尚未验证模型在 Scaling up（扩大规模）后的表现，其在大规模数据下的潜力仍有待探索。


{/* Surfer 构建了一个 World Model，以历史 Frame、语言 Instruction 以及机器人感知 Proprio 为输入，输出下一步的 Action 以及 Frame。 */}
{/* 在 Vision 处理方面，Surfer 使用用冻结的 CLIP 将 Frame 处理成 Tokens，并使用 TokenLearner 对这些 Tokens，以提高推理效率。 */}
{/* 在 Text 处理方面，Surfer 使用冻结的 Text Encoder 处理成 Text Latent。 */}
{/* 在 Proprio 处理方面，Surfer 使用 MLP 去学习得出 Proprio Latent。 */}
{/*  */}
{/* Surfer 的 World Model 分为两个部分，分别负责预测当前时间步的 Action 以及执行后的 Frame。 */}
{/* 这两个部分都是由多层的 Transformer 模块组成。 */}
{/*  在 Action 预测模块，使用经过 CLIP 处理的历史 Frame Tokens 作为 Query，[Text Latent, Proprio Latent] 作为 Key 和 Value，经过 Cross-Attention 得出 Action。 */} 
{/*  在 Frame 预测模块，使用经过 CLIP 处理的历史 Frame Tokens 作为 Query，预测的 Action 作为 Key 和 Value，经过 Cross-Attention 得出执行 Action 后的 Frame Tokens，并使用 L2 Loss 来进行训练。 */} 
{/*  */}
{/*  小结一下，Surfer 的亮点在于使用未来的 Frame 作为训练 Action 决策的监督信号，这相当于就是说如果执行这个 Action 能够预测出未来的 Frame，那么我的预测就是合理的，相当的符合直觉。缺点也是明显的，如果 Frame 预测得不准，那么这种监督反而还是一种负面的，且 Surfer 并没有验证 Scaling up 后的效果。 */}


</ArxivRating>

## SceneVerse

<ArxivRating id='2401.09340' tldr='一个大型的 3D-VL 数据集' rank={3}>

SceneVerse 是一个大型的 3D-VL 数据集，总共包含 2.5M 个场景-语言对，由 ScanNet、ARKitScenes、HM3D、3RScan、MultiScan、Structured3D、ProcTHOR 等多个 3D 数据集集合而成。

![](https://pic.hana0721.top/SceneVerse-data.41ylez0j3l.webp)
为了确保不同数据源之间的一致性，每个 Scan 处理成了 $\mathbb{R}^{N \times 8}$ 的维度，分别为 3D 坐标、RGB 颜色、实例 ID 和语义标签定义。
首先，SceneVerse 将 3D 场景建模为 Graph，Object 为 Node，Object 之间的空间关系被建模为 Edge。
接着，SceneVerse 使用一种基于 Prompt 的 Pipeline 方法去生成不同纬度的 Caption，详细见图。

![](https://pic.hana0721.top/SceneVerse-GPS.70avihp8so.webp)
最后，SceneVerse 训练了一个叫 GPS 的 Transformer，用于对齐 3D 场景和文本信息，类似于 3D 领域中的 CLIP。
SceneVerse 的工作量还是很足的，值得肯定。

</ArxivRating>

## Robot See Robot Do

<ArxivRating id='2409.18121' tldr='生成 Articulated Object 的 3D 模型，并基于 One-Shot 的示范中完成 Manipulation' rank={2}>

Robot See Robot Do 是一种 Pipeline 方法，用于完成 Articulated Object Manipulation 任务，只需单目人类示例即可完成学习。

![](https://pic.hana0721.top/RSRD-overview.5xb68ueasx.webp)
RSRD 分为 See 和 Do 两个部分。在 See 的过程中，首先对静态 Object 进行特征学习。具体流程如下，基于环绕待操作 Object 的视频，先使用 3D Gaussian Splatting 转换为 3D 点云，再使用 GARField 对点进行聚类，最后使用 DINOv2 学习点的特征表示。

![](https://pic.hana0721.top/RSRD-4D.4qrv08p81o.webp)
最后，从人类演示视频中，反解出这每个部件在每一帧的 3D 位姿。具体流程如下，首先对每时间步可优化的部件姿态参数渲染出 DINO 特征和深度，并与输入帧中提取的 DINO 特征和单目深度进行比较。然后，使用 ARAP 损失惩罚高斯分布偏离其初始配置过远的情况，相对于邻近点。这些损失共同反向传播至部件姿态，并通过梯度下降进行优化，以恢复 3D 部件运动。

在 Do 的过程中，则将物体的运动轨迹映射到机器人的操作中去。

Robot See Robot Do 的方法虽然无需训练，但无法进行泛化，只能做一些简单的任务。


</ArxivRating>

## ReKep

<ArxivRating id='2409.01652' tldr='' rank={0}>

</ArxivRating>

## OmniManip

<ArxivRating id='2501.03841' tldr='' rank={0}>

</ArxivRating>

## SOFAR

<ArxivRating id='2502.13143' tldr='' rank={0}>

</ArxivRating>

## PIVOT-R

<ArxivRating id='2410.10394' tldr='' rank={0}>

</ArxivRating>

## ManipGen

<ArxivRating id='2410.22332' tldr='' rank={0}>

</ArxivRating>

## DemoGen

<ArxivRating id='2502.16932' tldr='' rank={0}>

</ArxivRating>

## ArticuBot

<ArxivRating id='2503.03045' tldr='' rank={0}>

</ArxivRating>

## LAPA

<ArxivRating id='2410.11758' tldr='' rank={0}>

</ArxivRating>

## GR00T

<ArxivRating id='2503.14734' tldr='' rank={0}>

</ArxivRating>
