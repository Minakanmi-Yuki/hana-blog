---
title: 'Paper Reading: Embodied AI 2'
publishDate: 2025-12-26
updatedDate: 2026-01-06
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

import { ManualTOC } from '@/components/advanced'

<ManualTOC
  title=''
  categories={[
    {
      title: 'Embodied AI Paper Reading',
      items: [
        {
          title: 'Batch 1',
          href: '/blog/paper-reading-eba1',
          order: '1'
        },
        {
          title: 'Batch 2',
          href: '/blog/paper-reading-eba2',
          order: '2'
        },
        {
          title: 'Batch 3',
          href: '/blog/paper-reading-eba3',
          order: '3'
        },
        {
          title: 'Batch 4',
          href: '/blog/paper-reading-eba4',
          order: '4'
        }
      ]
    }
  ]}
/>

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## LLARVA

<ArxivRating id='2406.11815' tldr='OpenVLA-like 的 VLA，使用 2D轨迹 预测去引导动作生成。' rank={2}>

![](https://pic.hana0721.top/LLARVA.8z70f5s0j6.webp)
LLARVA 是一种 OpenVLA-like 的 VLA，使用 Instruction Tuning 进行微调，并通过输出 2D Visual Trace 去引导 Action 生成。

![](https://pic.hana0721.top/LLARVA-prompt.7axnhz1uu9.webp)
LLARVA 的输入指令被规范化为 Prompt，详细见图。
LLM 的主干为 LLaMA2 7B，并使用 LoRA 进行微调。
先预测 2D Visual Trace 再生成 Action Chunk，本质上是基于 CoT 的思想。

</ArxivRating>

## ATM

<ArxivRating id='2401.00025' tldr='使用 Label-free 的视频去训练以当前观察 Obs 和指令 Instruction 为 Condition 的 Points-Tracker，并使用 Points-Track 去引导 Action 生成。' rank={4}>

![](https://pic.hana0721.top/ATM.83aizqrop9.webp)
ATM 的亮点在于仅可以使用 Action-free 的数据去训练机器人操作策略。
ATM 是一种 Pipeline 方法，分为任意点轨迹建模与轨迹引导策略学习两个部分。
在任意点轨迹建模部分，ATM 利用了大量 Label-free 的视频训练了一个 Points-Tracker，输入是当前观察 Obs、任务 Instruction 以及初始 Points，输出是未来的 Points-Trajectory。
在轨迹引导策略学习部分，策略网络以上个步骤预测的 Points-Trajectory 以及当前 Obs 为输入，输出机器人的 Action。

任意点轨迹建模：
在数据处理部分，ATM 使用了离线的 Points-Tracker 模型去得出的 Label。
同时，过滤掉了大部分静止的 Points，只保留了 32 个高频活动的 Points。
对于当前 Obs，使用 ViT 将图像切分为 Patches，并随机 Mask 掉 50% 的 Patches，得出 Image Token。
对于任务 Instruction，使用 BERT 进行信息提取 Language Token。
对于初始 Points，在文中是有32个，编码为 Track Token。
以 Obs 以及 Instruction 为 Base 的 Points-Tracker 被定义为一个 Transformer，输入为 Image Token、Language Token 以及 Track Token，输出为未来的 Points-Trajectory。
按照题主的理解，输出的 Points-Trajectory 其实就是任务导向的 Vision-Language Fusion。

![](https://pic.hana0721.top/ATM-policy.9kgo1hvysf.webp)
轨迹引导策略学习：
首先，使用了 BERT-like 的 Transformer 对 Image Token 以及 Track Token 进行 Fusion，得出特征 \[CLS\]。
然后，策略网络设定为 MLP，以 \[CLS\] 以及 Track Token 为输入，输出机器人的 Action。
在策略学习的过程中，并没有引入 Instruction 的信息，因为在轨迹建模阶段已经引入了。
同时，对 Track Token 进行了两次 Fusion，这样会有更好的效果，详细见原文。

</ArxivRating>

## Track2Act

<ArxivRating id='2405.01527' tldr='使用 Points-Trajectory 去生成动作，并使用残差策略修正动作。' rank={2}>

Track2Act 本质上和 ATM 是一样的，都是以 Points-Trajectory 去引导动作的生成。
Track2Act 主要分为三个阶段：Points-Trajectory 预测、Open-loop 动作求解以及 Residual Policy 闭环修正。

![](https://pic.hana0721.top/Track2Act-pt.13m9i5xvjf.webp)
在 Points-Trajectory 预测阶段，Track2Act 使用了 DiT 生成模型。
DiT 的 Condition 被设置为初始图像 $I_0$ 以及目标图像 $\mathcal{G}$ 的表征，给定随机采样的查询点集 $P_0$，生成 Points-Trajectory。

在 Open-loop 动作求解阶段，Track2Act 使用了一个确定性的算法进行刚性动作的求解，不涉及神经网络训练。
这个算法的输入是 Points-Trajectory，输出一系列的刚性变换 $[\textbf{T}_t]_{t=1}^H$。
具体求解算法见原文，这里不详细展开。
最后，基于刚性变换，可以求解出机器人的 Open-loop 动作序列 $[\bar{a}_t]_{t=1}^H$。

![](https://pic.hana0721.top/Track2Act-rp.4n877z0ttz.webp)
在 Residual Policy 闭环修正阶段，Track2Act 使用一个 Residual Policy 去修正 Open-loop 动作。其实就是使用一个 Transformer-Encoder 去预测误差，最后加在一起。

最后简单瑞萍一下，其实博主觉得后面两个阶段完成可以使用神经网络进行表征，有点多余，虽然残差修正动作这个思想是好的。

</ArxivRating>

## ECoT

<ArxivRating id='2407.08693' tldr='具有 Embodied 性质的 CoT。' rank={3}>

![](https://pic.hana0721.top/ECoT.73ufntxfff.webp)
ECoT 这篇工作指出 LLM 那边的 CoT 是无法直接迁移到 Embodied 任务环境中的。
若只是简单的语义推理，那么就会出现大量不符合实际物理环境的幻觉。
因此，ECoT 在 CoT 的基础上添加了智能体对环境的感知，从而使得 CoT 具有 Embodied 性质。
ECoT 的具体设置见上图，和 CoT 类似，只是加入了 Embodied 的感知。
ECoT 使用的模型基座是 OpenVLA，使用 ECoT 后效果得到了很好的改善。

![](https://pic.hana0721.top/ECoT-data.1vz50u4k0o.webp)
ECoT 还提出了一套使用机器人数据产生 CoT 训练数据的 Pipeline。
具体就不多说了，图里面说的很清楚。

这种推理过程是显式的，具有一定的解释性，缺点就是这样会降低推理的速度，文中也给了一些解决方案。

</ArxivRating>

## VoxPoser

<ArxivRating id='2307.05973' tldr='使用 LLM 生成 Python 代码去调用 VLM 的 API 计算 3D Value Map，并作为价值引导进行 MPC。' rank={2}>

VoxPoser 是一种 Code for Robotic 的方法。
简单的来说，VoxPoser 使用 LLM 去生成 Python 代码，这里的代码会调用 VLM 的 API 生成 3D Value Map，然后进行 MPC。

![](https://pic.hana0721.top/VoxPoser.1vz50wovrd.webp)
首先，VoxPoser 会告诉 LLM 任务的文本描述以及一些 VLM 具有的 API函数，让它去产生能够基于 RGB-D 图像计算 3D Value Map 的 Python 代码。
然后，使用 Python 执行器去执行代码，过程中会调用 VLM 的 API，其实就是 OWL-ViT 获取边框还有 SAM 获取 Mask，然后重建 3D Points Cloud，计算得出 3D Value Map。
最后，有了 3D Value Map 作为价值引导，就可以使用 MPC 的方法合成轨迹，进行规划。
在文中，提供了在线训练 MPC 以及 启发式 MPC 两种方式，前者需要环境交互，后者则无需训练。

</ArxivRating>

## PIVOT

<ArxivRating id='2402.07872' tldr='将低级 Action 投影到图像中，并采用 VQA 的任务形式去让 VLM 选择 Action。' rank={3}>

![](https://pic.hana0721.top/PIVOT-example.2rvmgdu2g2.webp)
PIVOT 的思想很简单，就是把机器人的低级 Action 投影到图像中，并采用 VQA 的形式让 VLM 去选择 Action 集合。

![](https://pic.hana0721.top/PIVOT.szfq1osvp.webp)
PIVOT 维护一个 Action 的分布，初始化为均匀分布，有点像 MPC 中的交叉熵方法。
PIVOT 使用的是 Zero-Shot 的迭代算法，不断去优化 Action 的分布，直到收敛。
首先，PIVOT 从分布中采样一系列的 Action，然后将 Action 投影到 2D 图像上，详细见图，每个 Action 都对应了序号。
然后，给定任务描述文本以及 2D 图像，编写合适的 Prompt 让 VLM 去选择最优 Action 集合。
接着，根据选取的 Action 集合去维护 Action 分布，类似于 MPC 中的交叉熵方法。
经过不断迭代，可以得出较好的 Action 候选集合。

PIVOT 的思路很有启发性，但缺陷也是明显的。比如，3D 歧义理解、细粒度控制以及 VLM 的幻觉问题。

</ArxivRating>

## Code As Policies

<ArxivRating id='2209.07753' tldr='让 LLM 输出 Python 代码，并进行完成机器人任务。' rank={3}>

CaP 核心思想不是训练策略网络，而是通过 Prompt 让 LLM 去调用机器人 API，从而完成任务。

![](https://pic.hana0721.top/CaP.77e1n0y43g.webp)
首先，CaP 进行了 LMP 形式化，其实就是让策略利用 Python 语言的一些特性。
然后，为了处理复杂任务代码过长或者逻辑复杂的问题，CaP 使用了层次化代码生成的方法，其实就是自下而上分而治之的思想。
最后，CaP 进行了感知与动作的对齐。
LLM 会基于 Prompt 得出一些预定义的感知 API 以及 控制 API。
LLM 可以通过感知 API 去获取视觉信息的变量。
LLM 会计算控制 API 的参数，并执行。

CaP 的工作虽然简单，但它将高层规划和底层执行联系在了一起。

</ArxivRating>

## RoboPoint

<ArxivRating id='2406.10721' tldr='使用 VLM 获取 Point Grounding，用于引导下游任务。' rank={2}>

RoboPoint 通过构建一套程序化合成数据生成流水线，将通用视觉语言模型（VLM）微调为能根据自然语言指令预测精确 2D 空间操作点（Spatial Affordance）的模型，从而实现了无需真实世界数据训练的机器人零样本（Zero-shot）操控与导航。

![](https://pic.hana0721.top/RoboPoint.lw7w2yur8.webp)
在数据生成阶段，RoboPoint 利用程序化生成技术构建多样化的 3D 仿真场景，通过利用模拟器中的精确几何信息自动计算物体间的空间关系，并创造性地采用“移除目标物体再采样”的策略来自动标注自由空间（Free Space），从而低成本地生成了海量包含“图像-指令-像素坐标点”的高质量合成训练数据。

在指令微调阶段，RoboPoint 套用 LLaVA 的架构，冻住视觉层只练语言模型，把“找坐标”直接转化成“文本生成”任务，让模型直接输出坐标数字
同时，为了防止了 VLM 的遗忘，加入了 VQA 数据进行 Co-Training。

在真实执行阶段，先使用 RoboPoint 计算出 RGB 图像上的 2D 坐标，然后结合深度图转换为 3D 坐标，最后直接套用一个预设好的抓取姿态，把目标传给传统的运动规划算法去解算路径，机械臂照做就行了。

RoboPoint 其实也是一种高层决策的规划，也是一种 System 2 的方法。

</ArxivRating>

## GR-1

<ArxivRating id='2312.13139' tldr='' rank={0}>

</ArxivRating>

## HPT

<ArxivRating id='2409.20537' tldr='' rank={0}>

</ArxivRating>

## RoboDual

<ArxivRating id='2410.08001' tldr='' rank={0}>

</ArxivRating>

## GR-2

<ArxivRating id='2410.06158' tldr='' rank={0}>

</ArxivRating>

## Humanoid Manipulation

<ArxivRating id='2410.10803' tldr='' rank={0}>

</ArxivRating>

## Surfer

<ArxivRating id='2306.11335' tldr='' rank={0}>

</ArxivRating>

## SceneVerse

<ArxivRating id='2401.09340' tldr='' rank={0}>

</ArxivRating>

## Robot See Robot Do

<ArxivRating id='2409.18121' tldr='' rank={0}>

</ArxivRating>

## ReKep

<ArxivRating id='2409.01652' tldr='' rank={0}>

</ArxivRating>

## OmniManip

<ArxivRating id='2501.03841' tldr='' rank={0}>

</ArxivRating>

## SOFAR

<ArxivRating id='2502.13143' tldr='' rank={0}>

</ArxivRating>

## PIVOT-R

<ArxivRating id='2410.10394' tldr='' rank={0}>

</ArxivRating>

## ManipGen

<ArxivRating id='2410.22332' tldr='' rank={0}>

</ArxivRating>

## DemoGen

<ArxivRating id='2502.16932' tldr='' rank={0}>

</ArxivRating>

## ArticuBot

<ArxivRating id='2503.03045' tldr='' rank={0}>

</ArxivRating>

## LAPA

<ArxivRating id='2410.11758' tldr='' rank={0}>

</ArxivRating>

## GR00T

<ArxivRating id='2503.14734' tldr='' rank={0}>

</ArxivRating>
