---
title: "RL笔记（15）：SAC"
publishDate: 2025-12-24
updatedDate: 2025-12-24
description: "SAC"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）
**Soft Actor-Critic (SAC)** 是一种基于最大熵强化学习的 **Off-Policy（离线策略）** 算法，于 2018 年提出。
SAC 的前身是 Soft Q-learning (SQL)。相比于 SQL 需要复杂的采样过程，SAC 引入了 Actor-Critic 架构，使其训练更加稳定且高效。SAC 在各类 Benchmark 及真实机器人任务中表现出色，以其**极强的抗干扰能力**和**对超参数的鲁棒性**著称，是现代深度强化学习的基石算法之一。

---

## 1. 最大熵强化学习 (Maximum Entropy RL)

### 核心概念
**熵 (Entropy)** 是衡量随机变量“不确定性”的指标。对于随机变量 $X$，其概率密度为 $p$，熵 $H$ 定义为：

$$
\begin{align}
H(X)&=\mathbb{E}_{x\sim p}[-\log p(x)]\notag \\
&=-\int_{x}p(x)\log p(x) \mathrm{d}x \notag 
\end{align}
$$

在强化学习中，我们用 $H(\pi(\cdot|s))$ 表示策略 $\pi$ 在状态 $s$ 下选择动作的随机程度。

### 目标函数
SAC 不仅仅追求奖励最大化，还希望策略尽可能随机（探索能力更强）。
目标函数加入熵正则项：

$$
\begin{align}
\pi_{\textbf{MaxEnt}}^*=\arg\max_{\pi}\mathbb{E}_{\pi}\left[\sum_{t=0}^\infty r(s_t,a_t)+\alpha H(\pi(\cdot|s_t))\right]\notag
\end{align}
$$

*   **$\alpha$ (Temperature)**：温度系数，决定了熵的重要性。
    *   $\alpha$ 越大：智能体越“佛系”，喜欢尝试各种动作（探索）。
    *   $\alpha$ 越小：智能体越“功利”，只选当前认为最好的动作（利用）。

---

## 2. 基于能量的模型 (Energy-Based Model)

为了将熵引入策略分布，SAC 沿用了 SQL 的理论基础——基于能量的模型 (EBM)。
物理学告诉我们，能量越低的状态越稳定（概率越高）。

$$
\begin{align}
\pi(a|s)=\frac{\exp(-\mathcal{E}(s,a))}{Z(s)}\notag
\end{align}
$$

其中 $Z(s)$ 是配分函数（归一化项）。在 SAC 的理论推导中，我们依然假设最优策略服从这种玻尔兹曼分布。

---

## 3. Soft 策略迭代 (Soft Policy Iteration)

SAC 的理论核心证明了：如果我们交替进行 **Soft 策略评估** 和 **Soft 策略提升**，最终能收敛到最优策略。

### 3.1 定义：Soft 贝尔曼方程
引入熵之后，价值函数的定义发生了变化：

**Soft 状态价值函数 $V$：**
$$
\begin{align}
V_{\textbf{soft}}^\pi(s)&=\mathbb{E}_{a\sim\pi(\cdot|s)}[Q^{\pi}_{\textbf{soft}}(s,a)-\alpha\log\pi(a|s)]\notag \\
&=\mathbb{E}_{a\sim \pi(a|s)}[Q^{\pi}_{\textbf{soft}}(s,a)]+\alpha H(\pi(\cdot|s)) \notag
\end{align}
$$
> **💡 笔记**：$V$ 值等于 $Q$ 值的期望加上“熵红利”。

**Soft 动作价值函数 $Q$：**
$$
\begin{align}
Q^{\pi}_{\textbf{soft}}(s,a)=r(s,a)+\gamma \mathbb{E}_{s^\prime\sim p(\cdot|s,a)}[V_{\textbf{soft}}^\pi(s^\prime)]\notag \\
\end{align}
$$

### 3.2 第一步：Soft 策略评估 (Policy Evaluation)
我们要证明反复应用 Bellman 算子能收敛到真实的 Q 值。

定义 **Soft 贝尔曼算子 $\mathcal{T}^\pi$**：
$$
\begin{align}
\mathcal{T}^\pi Q^{k}_{\textbf{soft}}(s,a) \triangleq r(s,a)+\gamma \mathbb{E}_{s^\prime\sim p(\cdot|s,a)}[\mathbb{E}_{a^\prime\sim\pi(\cdot|s^\prime)}[Q^{k}_{\textbf{soft}}(s^\prime,a^\prime)-\alpha\log\pi(a^\prime|s^\prime)]]\notag
\end{align}
$$

**收敛性证明（压缩映射）：**
为了简化证明，我们将奖励函数重定义为包含熵的形式 $r_{\pi}(s,a) \triangleq r(s,a) + \alpha \mathbb{E}_{s'}[H(\pi(\cdot|s'))]$。
这样算子可以写成标准形式：
$$
\mathcal{T}^\pi Q(s,a) = r_\pi(s,a) + \gamma \mathbb{E}_{s', a'}[Q(s', a')]
$$

计算两次迭代的差值：
$$
\begin{align}
\big|\mathcal{T}^\pi Q^N_{\textbf{soft}}(s,a)-\mathcal{T}^\pi Q^M_{\textbf{soft}}(s,a)\big|
&=\gamma\big|\mathbb{E}_{s^\prime, a^\prime}\left[Q^N_{\textbf{soft}}(s^\prime,a^\prime)-Q^M_{\textbf{soft}}(s^\prime,a^\prime)\right]\big|\notag \\
&\le \gamma \max_{s^\prime,a^\prime} \big|Q^N_{\textbf{soft}}(s^\prime,a^\prime)-Q^M_{\textbf{soft}}(s^\prime,a^\prime)\big| \notag
\end{align}
$$

取最大范数：
$$
\begin{align}
||\mathcal{T}^\pi Q^N - \mathcal{T}^\pi Q^M||_\infty \le \gamma ||Q^N - Q^M||_\infty
\end{align}
$$

因为 $\gamma \in (0,1)$，所以这是一个**压缩映射 (Contraction Mapping)**。这意味着无论初始值如何，不断迭代 $Q$ 值最终一定会收敛到 $Q^\pi_{soft}$。

### 3.3 第二步：Soft 策略提升 (Policy Improvement)
有了 Q 值后，我们如何更新策略让它变好？

**更新规则**：
我们将新策略 $\pi_{new}$ 设定为最小化与“指数化 Q 值”的 KL 散度：
$$
\begin{align}
\pi_{\text{new}}(\cdot|s)=\arg \min_{\pi^\prime\in\Pi}D_{\textbf{KL}}\left(\pi^\prime(\cdot|s)\bigg|\bigg|\frac{\exp(\frac{1}{\alpha}Q^{\pi_{\text{old}}}_{\textbf{soft}}(s,\cdot))}{Z^{\pi_{\text{old}}}(s)}\right)\notag
\end{align}
$$

**证明策略确实提升了 ($Q_{new} \ge Q_{old}$)**：
定义目标函数 $J(\pi) = D_{KL}(\dots)$。
根据 KL 散度的性质，且 $\pi_{new}$ 是最小化该目标的策略，必有：
$$
J(\pi_{new}) \le J(\pi_{old})
$$

展开 KL 散度并化简（$Z(s)$ 也是常数被消去），可得：
$$
\begin{align}
\mathbb{E}_{a\sim\pi_\text{new}}\left[Q_{\textbf{soft}}^{\pi_\text{old}}(s,a)-\alpha\log\pi_\text{new}(a|s)\right] \ge \mathbb{E}_{a\sim\pi_\text{old}}\left[Q_{\textbf{soft}}^{\pi_\text{old}}(s,a)-\alpha\log\pi_\text{old}(a|s)\right] = V_{\textbf{soft}}^{\pi_\text{old}}(s) \notag
\end{align}
$$

这意味着，**在当前状态 $s$，按照新策略 $\pi_{new}$ 行动获得的“Soft 价值”比旧策略更高**。
利用贝尔曼方程反复迭代展开：
$$
\begin{align}
Q_{\textbf{soft}}^{\pi_{\text{old}}}(s,a) &= r + \gamma \mathbb{E}[V_{\pi_{old}}] \notag \\
&\le r + \gamma \mathbb{E}[ \text{新策略产生的价值} ] \notag \\
&\le \dots \le Q_{\textbf{soft}}^{\pi_{\text{new}}}(s,a) \notag
\end{align}
$$
证明完毕。

### 3.4 Soft 策略迭代定理
结合上述两步：
1.  评估当前策略得到 $Q^\pi$。
2.  利用 $Q^\pi$ 优化得到更好的 $\pi'$。
重复此过程，策略序列 $\pi_i$ 会单调提升，并最终收敛到最优策略 $\pi^*$。

---

## 4. 算法实现 (SAC Algorithm)

理论上我们需要不断收敛 Q 值，但在实际 Deep RL 中，我们使用神经网络来近似。
SAC 包含三个网络：
1.  **Actor 网络 $\pi_\theta$**：输出动作分布。
2.  **Critic 网络 $Q_\omega$**：输出 Soft Q 值（使用双网络）。
3.  (可选) Value 网络 $V_\psi$：在早期版本使用，现代版本通常省略，直接用 Q 表示 V。

### 4.1 Critic 网络更新 (Q Loss)
为了缓解 Q 值过高估计（Overestimation）的问题，SAC 借鉴了 **Double Q-learning** 的思想，使用两个 Q 网络 $Q_{\omega_1}, Q_{\omega_2}$，计算目标值时**取最小值**。

损失函数（最小化贝尔曼误差）：
$$
\begin{align}
L_{Q}(\omega) &= \mathbb{E}_{(s,a,r,s')\sim \mathcal{D}}\left[\frac{1}{2}\left(Q_\omega(s,a) - y_{target}\right)^2\right] \notag
\end{align}
$$

目标值 $y_{target}$ 使用**目标网络 (Target Network)** 计算：
$$
\begin{align}
y_{target} = r + \gamma \left( \min_{j=1,2} Q_{\omega_j^-}(s', a') - \alpha \log \pi_\theta(a'|s') \right), \quad a' \sim \pi_\theta(\cdot|s') \notag
\end{align}
$$
> **💡 注意**：这里的 $a'$ 是从当前策略**采样**出来的，不是取最大值。这也是 Off-policy 的体现。

### 4.2 Actor 网络更新 (Policy Loss)
Actor 的目标是最大化价值 $V$，即最大化 $Q$ 且最大化 熵。
根据策略提升定理，这等价于最小化 KL 散度。

$$
\begin{align}
L_\pi(\theta) &= \mathbb{E}_{s \sim \mathcal{D}}\left[ D_{KL} \right] \notag \\
&\approx \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\theta} \left[ \alpha \log \pi_\theta(a|s) - Q_\omega(s,a) \right] \notag
\end{align}
$$
为了稳健，这里也取两个 Q 网络的最小值：
$$
L_\pi(\theta) = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\theta} \left[ \alpha \log \pi_\theta(a|s) - \min_{j=1,2}Q_{\omega_j}(s,a) \right]
$$

### 4.3 重参数化技巧 (Reparameterization Trick)
**困难**：$L_\pi$ 中动作 $a$ 是从 $\pi_\theta$ 采样出来的。采样操作本身不可导，无法把 Q 网络的梯度传回给策略网络 $\theta$。

**解决方案**：假设策略是高斯分布 $\mathcal{N}(\mu, \sigma)$。我们将随机性从网络中剥离出来。
先采样一个标准正态噪声 $\epsilon \sim \mathcal{N}(0, I)$，然后变换得到动作：
$$
\begin{align}
a_t &= f_\theta(\epsilon_t; s_t) \notag \\
&= \mu_\theta(s_t) + \sigma_\theta(s_t) \cdot \epsilon_t \notag
\end{align}
$$
这样，$a_t$ 对于 $\theta$ 就是可导的函数了（因为 $\mu$ 和 $\sigma$ 是网络输出），梯度可以顺利反向传播。

带入重参数化后的 Actor 损失：
$$
\begin{align}
L_\pi(\theta)=\mathbb{E}_{s_t\sim \mathcal{D}, \epsilon_t \sim \mathcal{N}}\left[\alpha\log \pi_\theta(f_\theta(\epsilon_t;s_t)|s_t)-\min_{j=1,2}Q^{\omega_j}(s_t,f_\theta(\epsilon_t;s_t))\right] \notag
\end{align}
$$

---

## 5. 进阶：自动调整熵系数 (Automating Entropy Adjustment)

在 SAC 早期版本中，$\alpha$ 是固定超参数。
但不同状态需要的探索程度不同：
*   **不确定状态**：应该多探索，$\alpha$ 大一点。
*   **确定状态**（如即将掉进悬崖）：应该求稳，$\alpha$ 小一点。

SAC v2 提出将其转化为一个**带约束的优化问题**：最大化奖励，同时限制**平均熵大于目标值 $H_0$**。

**数学推导**：
构造拉格朗日函数：
$$
\begin{align}
L(\alpha) = \mathbb{E}_{(s,a)\sim \pi} [-\alpha \log \pi(a|s) - \alpha H_0] \notag
\end{align}
$$
注意：这是一个关于对偶变量 $\alpha$ 的优化目标。我们通常使用梯度下降来更新 $\log \alpha$（保证 $\alpha > 0$）。

**直觉解释**：
*   当实际熵 $-\log \pi < H_0$（探索不够）时，$\alpha$ 会增大，增加探索惩罚的权重。
*   当实际熵 $-\log \pi > H_0$（探索太多）时，$\alpha$ 会减小，让智能体更专注于获得奖励。

通过这个机制，SAC 能够动态调整探索与利用的平衡，这也是它极其稳定的原因之一。