---
title: "RL笔记（26）：异构智能体信任区域优化 (HAPPO & HATRPO)"
publishDate: 2026-01-04
updatedDate: 2026-01-04
description: "从经验主义回归理论严谨性：详解 HAPPO 如何解决 MARL 中的单调提升难题。涵盖多智能体优势分解引理、序列更新机制以及与 MAPPO 的本质区别。"
heroImage: {src : "https://pic.hana0721.top/rl-note-4.1ziqble6oe.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

在之前的笔记中，我们介绍了 **MAPPO**，它简单地让每个智能体运行 PPO，并共用一个中心化 Critic。虽然在工程上很成功，但它在理论上存在一个巨大的漏洞：

**多智能体更新冲突**。
在单智能体中，PPO/TRPO 保证了每一步更新，策略性能 $J(\pi)$ 都是**单调不减**的。
但在多智能体中，如果所有智能体**同时**更新策略，这就好比两个人抬桌子，A 想往左，B 想往右，虽然两人各自觉得自己的方向是对的，但合起来的结果可能导致桌子翻倒（联合策略性能下降）。

**HAPPO (Heterogeneous-Agent PPO)** 和 **HATRPO** 的核心贡献在于：提出了**多智能体优势分解引理**，并据此设计了**序列更新**方案，在理论上证明了联合策略的单调提升，特别适用于**异构（Heterogeneous）** 智能体场景。

---

##  理论基石：多智能体优势分解引理

我们希望优化联合策略 $\boldsymbol{\pi} = (\pi^1, \pi^2, ..., \pi^n)$ 以最大化联合回报 $J(\boldsymbol{\pi})$。
类似于 TRPO，我们关注新旧策略的回报差 $J(\boldsymbol{\pi}_{\text{new}}) - J(\boldsymbol{\pi}_{\text{old}})$。

### 优势分解引理 (Multi-Agent Advantage Decomposition Lemma)
HAPPO 证明了，联合策略的优势函数（Joint Advantage）可以精确分解为每个智能体局部优势函数的**累加**，前提是这些优势是**按顺序**计算的。

$$
A_{\boldsymbol{\pi}}^{\text{joint}}(s, \mathbf{a}) = \sum_{i=1}^n A_{\pi}^{i}(s, a^i, a^1, a^2, ..., a^{i-1})
$$

这里的 $A_{\pi}^{i}$ 是第 $i$ 个智能体的优势函数，但有一个关键细节：
它不仅依赖于当前状态 $s$ 和动作 $a^i$，还依赖于**排在他前面的智能体 $(a^1, a^2, ..., a^{i-1})$ 的动作**。

这意味着：如果我们按照某种顺序 $1, 2, ..., n$ 依次更新智能体，那么整体性能的提升等于每个人贡献的提升之和。

---

##  核心机制：序列更新 (Sequential Update)

为了利用上述引理，HAPPO 抛弃了 MAPPO 的“同步更新”模式，采用了 **序列更新** 模式。

### 流程
在每一次训练迭代中：
1.  **随机排列**：随机生成一个智能体的更新顺序（例如：Agent 3 -> Agent 1 -> Agent 2）。
2.  **依次更新**：
    *   **Agent 3**：基于旧策略 $\boldsymbol{\pi}_{\text{old}}$ 计算自己的优势，更新策略，变为 $\pi^3_{\text{new}}$。
    *   **Agent 1**：看到 Agent 3 已经更新了（环境变了），基于 $(\pi^3_{\text{new}}, \pi^1_{\text{old}}, \pi^2_{\text{old}})$ 计算自己的优势，更新为 $\pi^1_{\text{new}}$。
    *   **Agent 2**：看到 3 和 1 都变了，基于 $(\pi^3_{\text{new}}, \pi^1_{\text{new}}, \pi^2_{\text{old}})$ 更新自己。
3.  **循环结束**：所有智能体更新完毕，进入下一轮采样。

### 为什么这样就稳了？
通过这种方式，对于每一个智能体 $i$ 来说，它在更新时，排在它前面的队友已经是“新策略”了，排在后面的还是“旧策略”。它只需要在当前这个**固定**的上下文中找到最优提升方向，就能保证整个团队的总分是增加的。

> **💡 直觉**：
> 就像多人过独木桥。大家一起跑容易挤掉下去（同步更新）。HAPPO 的做法是：第一个人先走稳，第二个人看准第一个人的位置再走，依次类推。

---

##  HATRPO 与 HAPPO

基于这个理论框架，衍生出了两个算法。它们都共享同一个**中心化 Critic** $V(s)$。

### HATRPO (Trust Region)
这是 TRPO 的多智能体版本。
*   **目标**：最大化局部优势期望。
*   **约束**：严格限制每个智能体更新前后的 KL 散度：
    $$ \mathbb{E}[D_{KL}(\pi^i_{\text{old}} || \pi^i_{\text{new}})] \le \delta $$
*   **求解**：同样需要使用共轭梯度法（CG），计算量较大，但理论性质最强。

### HAPPO (Proximal Policy)
这是 PPO 的多智能体版本，更实用。
*   **目标**：使用 Clip 技巧来替代 KL 约束。
    定义比率 $r_t^i(\theta) = \frac{\pi^i(a^i|s)}{\pi^i_{\text{old}}(a^i|s)}$，损失函数为：
    $$
    L^{HAPPO}(\theta^i) = \mathbb{E} \left[ \min \left( r_t^i(\theta) \hat{M}^i, \text{clip}\left(r_t^i(\theta), 1-\epsilon, 1+\epsilon\right) \hat{M}^i \right) \right]
    $$
*   **关键点 $\hat{M}^i$**：这是**修正后的优势函数**。
    在计算 Agent $i$ 的优势时，必须扣除排在它前面的队友 $1, ..., i-1$ 已经拿走的优势，确保不重复计算功劳。

---

##  深度对比：HAPPO vs. MAPPO

这两个算法非常相似，都是 PPO + CTDE，很容易混淆。

| 维度 | MAPPO (Multi-Agent PPO) | HAPPO (Heterogeneous-Agent PPO) |
| :--- | :--- | :--- |
| **更新方式** | **同步更新 (Simultaneous)** | **序列更新 (Sequential)** |
| **理论保证** | 无单调提升保证 (可能震荡) | **有单调提升保证** |
| **参数共享** | 强烈依赖 (Parameter Sharing) | **不需要** (天生支持异构) |
| **优势计算** | $A(s, a^i)$ (假设队友不变) | $A(s, a^i, a^1, ..., a^{i-1})$ (考虑队友更新) |
| **适用场景** | 同质智能体 (如星际争霸小狗海) | **异构智能体** (如牧羊犬与羊、不同类型的机器人) |
| **计算量** | 较小 | 稍大 (因为要串行计算) |

---

##  总结

HAPPO 系列算法在 MARL 领域具有重要的理论地位：
1.  **解决了 Heterogeneous (异构) 问题**：以前的算法（如 QMIX, MAPPO）通常假设智能体共享参数以加速收敛。HAPPO 证明了即使每个智能体结构完全不同（异构），只要按顺序更新，也能高效协作。
2.  **解决了 Trust Region 问题**：它将单智能体的“信任区域”概念成功推广到了多智能体，解决了多智能体同时更新导致的**环境非平稳**和**信度分配**难题。

如果你面对的任务中，智能体各不相同（例如一个无人机配合一个地面车），且协作极其复杂，HAPPO 往往比 MAPPO 表现更稳健。