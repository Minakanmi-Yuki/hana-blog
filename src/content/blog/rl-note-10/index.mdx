---
title: "RLç¬”è®°ï¼ˆ10ï¼‰ï¼šActor-Critic"
publishDate: 2025-12-19
updatedDate: 2025-12-19
description: "ç­–ç•¥æ¢¯åº¦ä¸ä»·å€¼å‡½æ•°çš„å®Œç¾ç»“åˆï¼šè¯¦è§£ Actor-Critic æ¶æ„ã€‚ä» Baseline å‡å°æ–¹å·®çš„æ•°å­¦è¯æ˜ï¼Œåˆ°ä¼˜åŠ¿å‡½æ•° (Advantage) çš„æ¨å¯¼åŠ A2C ç®—æ³•æµç¨‹ã€‚"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## å¼•è¨€ï¼ˆIntroductionï¼‰

åœ¨ä¹‹å‰çš„ç¬”è®°ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¸¤ç§æˆªç„¶ä¸åŒçš„å¼ºåŒ–å­¦ä¹ è·¯å¾„ï¼š
1.  **Value-Based (å¦‚ Q-Learning)**ï¼šé€šè¿‡å­¦ä¹ ä»·å€¼å‡½æ•° $Q(s,a)$ æ¥é—´æ¥å¯¼å‡ºç­–ç•¥ã€‚ä¼˜ç‚¹æ˜¯æ–¹å·®å°ï¼ˆåˆ©ç”¨äº† TD çš„ä¸€æ­¥æ›´æ–°ï¼‰ï¼Œç¼ºç‚¹æ˜¯æ— æ³•å¤„ç†è¿ç»­åŠ¨ä½œã€‚
2.  **Policy-Based (å¦‚ REINFORCE)**ï¼šç›´æ¥å­¦ä¹ ç­–ç•¥ $\pi_\theta$ã€‚ä¼˜ç‚¹æ˜¯å¯ä»¥å¤„ç†è¿ç»­åŠ¨ä½œï¼Œç¼ºç‚¹æ˜¯æ–¹å·®æå¤§ï¼ˆå› ä¸ºä½¿ç”¨äº†è’™ç‰¹å¡æ´›å›æŠ¥ $G_t$ï¼‰ï¼Œä¸”åªèƒ½åœ¨å›åˆç»“æŸåæ›´æ–°ã€‚

**Actor-Critic (AC)** æ¶æ„æ—¨åœ¨ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ï¼š
*   **Actor (æ¼”å‘˜)**ï¼šå³ç­–ç•¥ç½‘ç»œ $\pi_\theta(a|s)$ï¼Œè´Ÿè´£ç”±çŠ¶æ€è¾“å‡ºåŠ¨ä½œã€‚
*   **Critic (è¯„è®ºå®¶)**ï¼šå³ä»·å€¼ç½‘ç»œ $V_w(s)$ æˆ– $Q_w(s,a)$ï¼Œè´Ÿè´£è¯„ä¼° Actor çš„åŠ¨ä½œå¥½ä¸å¥½ï¼Œå¸®åŠ© Actor å‡å°æ–¹å·®åŠ é€Ÿå­¦ä¹ ã€‚

---

## é™ä½æ–¹å·®çš„æŠ€å·§ï¼šåŸºçº¿ (Baseline)

å›é¡¾ REINFORCE çš„æ¢¯åº¦å…¬å¼ï¼š
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) G_t \right]
$$
è¿™é‡Œçš„ $G_t$ æ˜¯ä» $t$ æ—¶åˆ»å¼€å§‹çš„ç´¯ç§¯å›æŠ¥ã€‚ç”±äºç¯å¢ƒéšæœºæ€§å’Œç­–ç•¥éšæœºæ€§ï¼Œ$G_t$ çš„æ³¢åŠ¨éå¸¸å¤§ï¼Œå¯¼è‡´æ¢¯åº¦ä¼°è®¡ä¸ç¨³å®šã€‚

ä¸ºäº†å‡å°æ–¹å·®ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ª **åŸºçº¿å‡½æ•° (Baseline) $b(s)$**ã€‚åŸºçº¿å‡½æ•°åªä¸çŠ¶æ€æœ‰å…³ï¼Œä¸åŠ¨ä½œæ— å…³ã€‚
æˆ‘ä»¬æŠŠæ¢¯åº¦ä¿®æ”¹ä¸ºï¼š
$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) (G_t - b(s_t)) \right]
$$

### æ•°å­¦è¯æ˜ï¼šå¼•å…¥åŸºçº¿ä¸æ”¹å˜æ¢¯åº¦æœŸæœ›
æˆ‘ä»¬éœ€è¦è¯æ˜å‡å»ä¸€é¡¹ $b(s_t)$ åï¼Œæ¢¯åº¦çš„æœŸæœ›å€¼ä¸å˜ã€‚å³è¯æ˜ï¼š
$$
\mathbb{E}_{a_t \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a_t|s_t) \cdot b(s_t)] = 0
$$

è¯æ˜å¦‚ä¸‹ï¼š
$$
\begin{align}
\mathbb{E}_{a \sim \pi} [\nabla_\theta \log \pi(a|s) \cdot b(s)] 
&= \sum_{a} \pi(a|s) \frac{\nabla_\theta \pi(a|s)}{\pi(a|s)} b(s) \notag \\
&= b(s) \sum_{a} \nabla_\theta \pi(a|s) \notag \\
&= b(s) \nabla_\theta \sum_{a} \pi(a|s) \notag \\
&= b(s) \nabla_\theta (1) \notag \\
&= 0 \notag
\end{align}
$$
ç»“è®ºï¼šåªè¦ $b(s)$ ä¸ä¾èµ–äºåŠ¨ä½œ $a$ï¼Œæˆ‘ä»¬å¯ä»¥éšæ„å‡å»å®ƒè€Œä¸æ”¹å˜æ¢¯åº¦çš„æ–¹å‘ï¼Œä½†èƒ½æ˜¾è‘—æ”¹å˜æ¢¯åº¦çš„æ–¹å·®ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬é€‰æ‹©çŠ¶æ€ä»·å€¼å‡½æ•° $V(s)$ ä½œä¸ºåŸºçº¿ã€‚

---

## Actor-Critic æ¶æ„æ¼”å˜

###  Q Actor-Critic
å¦‚æœæˆ‘ä»¬ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œ $Q_w(s,a)$ æ¥è¿‘ä¼¼ REINFORCE ä¸­çš„ $G_t$ï¼Œæ¢¯åº¦å˜ä¸ºï¼š
$$
\nabla_\theta J(\theta) \approx \mathbb{E} [\nabla_\theta \log \pi_\theta(a|s) Q_w(s,a)]
$$
ä½†è¿™å¹¶æ²¡æœ‰è§£å†³é—®é¢˜ï¼Œå› ä¸ºå•çº¯æ‹Ÿåˆ $Q$ å€¼è¿˜æ˜¯å¾ˆéš¾ã€‚

###  Advantage Actor-Critic (A2C)
ä¸ºäº†åˆ©ç”¨ Baseline å‡å°æ–¹å·®ï¼Œæˆ‘ä»¬å¸Œæœ›æ¢¯åº¦å½¢å¼ä¸ºï¼š
$$
\nabla_\theta J(\theta) \approx \mathbb{E} [\nabla_\theta \log \pi_\theta(a|s) (Q_w(s,a) - V_v(s))]
$$
å…¶ä¸­ $Q(s,a) - V(s)$ è¢«ç§°ä¸º **ä¼˜åŠ¿å‡½æ•° (Advantage Function)**ï¼Œè®°ä¸º $A(s,a)$ã€‚å®ƒè¡¨ç¤ºâ€œåœ¨çŠ¶æ€ $s$ ä¸‹ï¼ŒåŠ¨ä½œ $a$ æ¯”å¹³å‡æƒ…å†µå¥½äº†å¤šå°‘â€ã€‚

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦åŒæ—¶ç»´æŠ¤ä¸¤ä¸ªç½‘ç»œï¼ˆä¸€ä¸ªç®— $Q$ï¼Œä¸€ä¸ªç®— $V$ï¼‰ï¼Œè®­ç»ƒä¼šå¾ˆéº»çƒ¦ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ **TD Error** æ¥è¿‘ä¼¼ä¼˜åŠ¿å‡½æ•°ã€‚

æ ¹æ®è´å°”æ›¼æ–¹ç¨‹ï¼š
$$ Q(s_t, a_t) \approx r_t + \gamma V(s_{t+1}) $$
å› æ­¤ï¼Œä¼˜åŠ¿å‡½æ•°å¯ä»¥å†™ä¸ºï¼š
$$
\begin{align}
A(s_t, a_t) &= Q(s_t, a_t) - V(s_t) \notag \\
&\approx r_t + \gamma V(s_{t+1}) - V(s_t) \notag
\end{align}
$$
è¿™æ°å¥½å°±æ˜¯ **TD Error ($\delta_t$)**ï¼

å› æ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦ç»´æŠ¤ä¸€ä¸ª **Actor ç½‘ç»œ $\pi_\theta(a|s)$** å’Œä¸€ä¸ª **Critic ç½‘ç»œ $V_w(s)$** å³å¯ã€‚

---

## A2C ç®—æ³•æµç¨‹

åœ¨ Advantage Actor-Critic ä¸­ï¼ŒCritic çš„ä»»åŠ¡æ˜¯æŠŠ $V_w(s)$ ä¼°å¾—è¶Šå‡†è¶Šå¥½ï¼ŒActor çš„ä»»åŠ¡æ˜¯åˆ©ç”¨ Critic æä¾›çš„ TD Error æ¥æ›´æ–°ç­–ç•¥ã€‚

### 1. Critic çš„æ›´æ–° (Value Update)
Critic çš„ç›®æ ‡æ˜¯æœ€å°åŒ– TD Error çš„å¹³æ–¹ï¼ˆå³å›å½’é—®é¢˜ï¼‰ï¼š
$$
L_{critic}(w) = \frac{1}{2} \left( r_t + \gamma V_w(s_{t+1}) - V_w(s_t) \right)^2
$$
æ¢¯åº¦æ›´æ–°ï¼š
$$ w \leftarrow w + \alpha_c \delta_t \nabla_w V_w(s_t) $$

### 2. Actor çš„æ›´æ–° (Policy Update)
Actor ä½¿ç”¨ TD Error ä½œä¸ºä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡å€¼è¿›è¡Œæ¢¯åº¦ä¸Šå‡ï¼š
$$
\nabla_\theta J(\theta) \approx \nabla_\theta \log \pi_\theta(a_t|s_t) \delta_t
$$
æ¢¯åº¦æ›´æ–°ï¼š
$$ \theta \leftarrow \theta + \alpha_a \delta_t \nabla_\theta \log \pi_\theta(a_t|s_t) $$

> **ğŸ’¡ ç›´è§‰ç†è§£**ï¼š
> *   å¦‚æœ $\delta_t > 0$ï¼ˆæƒŠå–œï¼Œç»“æœæ¯”é¢„æœŸå¥½ï¼‰ï¼šå¢åŠ åŠ¨ä½œ $a_t$ çš„æ¦‚ç‡ã€‚
> *   å¦‚æœ $\delta_t < 0$ï¼ˆå¤±æœ›ï¼Œç»“æœæ¯”é¢„æœŸå·®ï¼‰ï¼šå‡å°åŠ¨ä½œ $a_t$ çš„æ¦‚ç‡ã€‚
> *   è¿™é‡Œçš„â€œé¢„æœŸâ€å°±æ˜¯ Critic æä¾›çš„ $V(s_t)$ã€‚

###  A2C ä¼ªä»£ç 
$$
\begin{aligned}
& \bullet \; \text{Initialize Actor } \pi_\theta \text{ and Critic } V_w \\
& \bullet \; \textbf{For } \text{episode } = 1 \to E \textbf{ do}: \\
& \bullet \qquad \text{Initialize state } s \\
& \bullet \qquad \textbf{For } \text{step } t = 1 \to T \textbf{ do}: \\
& \bullet \qquad \qquad \text{Sample action } a \sim \pi_\theta(\cdot|s) \\
& \bullet \qquad \qquad \text{Execute } a, \text{ observe } r, s' \\
& \bullet \qquad \qquad \textbf{// Calculate TD Error (Advantage)} \\
& \bullet \qquad \qquad \delta \leftarrow r + \gamma V_w(s') - V_w(s) \\
& \bullet \qquad \qquad \textbf{// Update Critic} \\
& \bullet \qquad \qquad w \leftarrow w + \alpha_c \delta \nabla_w V_w(s) \\
& \bullet \qquad \qquad \textbf{// Update Actor} \\
& \bullet \qquad \qquad \theta \leftarrow \theta + \alpha_a \delta \nabla_\theta \log \pi_\theta(a|s) \\
& \bullet \qquad \qquad s \leftarrow s' \\
& \bullet \qquad \textbf{End For} \\
& \bullet \; \textbf{End For}
\end{aligned}
$$

---

## åå·®ä¸æ–¹å·®çš„æƒè¡¡ (Bias-Variance Tradeoff)

æˆ‘ä»¬å¯ä»¥æ€»ç»“ä¸€ä¸‹ä¸åŒè®¡ç®— Advantage çš„æ–¹æ³•ï¼Œå®ƒä»¬ä½“ç°äº† RL ä¸­æ ¸å¿ƒçš„æƒè¡¡ï¼š

1.  **è’™ç‰¹å¡æ´› (REINFORCE)**ï¼š
    *   $A_t \approx G_t - V(s_t)$
    *   **æ— åå·®**ï¼š$G_t$ æ˜¯çœŸå®å›æŠ¥ã€‚
    *   **é«˜æ–¹å·®**ï¼šå—æ•´ä¸ªåºåˆ—éšæœºæ€§å½±å“ã€‚
2.  **Actor-Critic (TD)**ï¼š
    *   $A_t \approx r_t + \gamma V(s_{t+1}) - V(s_t)$
    *   **ä½æ–¹å·®**ï¼šåªå—ä¸€æ­¥éšæœºæ€§å½±å“ã€‚
    *   **æœ‰åå·®**ï¼šä¾èµ–äº Critic çš„ä¼°è®¡ $V(s_{t+1})$ï¼Œå¦‚æœ Critic è¿˜æ²¡ç»ƒå¥½ï¼ŒActor å°±ä¼šè¢«å¸¦åã€‚

ä¸ºäº†å¹³è¡¡ä¸¤è€…ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **å¤šæ­¥ TD** æˆ–è€… **GAE (Generalized Advantage Estimation)**ï¼Œè¿™æ˜¯ PPO ç­‰è¿›é˜¶ç®—æ³•çš„æ ¸å¿ƒæŠ€å·§ã€‚

---

## æ€»ç»“

Actor-Critic æ¶æ„æ˜¯ç°ä»£æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ä¸»æµæ¶æ„ã€‚
*   å®ƒè§£å†³äº† REINFORCE æ–¹å·®å¤§ã€æ›´æ–°æ…¢çš„é—®é¢˜ã€‚
*   å®ƒè§£å†³äº† DQN æ— æ³•å¤„ç†è¿ç»­åŠ¨ä½œçš„é—®é¢˜ã€‚
*   å®ƒæ˜¯åç»­ **A3C, DDPG, TRPO, PPO, SAC** ç­‰é«˜çº§ç®—æ³•çš„å…±åŒç¥–å…ˆã€‚