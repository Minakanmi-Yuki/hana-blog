---
title: 具身十日谈：关于仿真以及 InternData A1
publishDate: 2025-11-23
updatedDate: 2025-11-23
description: 关于仿真以及 InternData A1
heroImage: {src : "https://picr2.axi404.top/126327443_p0.webp", color: '#E8CFBC'}
pixivLink: '126327443'
category: 'research'
draft: false
tags:
    - 'Embodied AI'
    - 'Deep Dive'
---

import { ManualTOC } from '@/components/advanced'

<ManualTOC title='' categories={[
    {
        title: '具身十日谈',
        items: [
            {
                title: '数据与仿真器',
                href: '/blog/embodied-talk-1',
                order: '1'
            },
            {
                title: 'VLA 为什么需要 VLM',
                href: '/blog/embodied-talk-2',
                order: '2'
            },
            {
                title: 'GEN-0 以及后续的 VLA 发展的看法',
                href: '/blog/embodied-talk-3',
                order: '3'
            },
            {
                title: '关于仿真以及 InternData A1',
                href: '/blog/embodied-talk-4',
                order: '4'
            }
        ]
    }
]} />

## 前言

最近同实验室的田洋师兄（也是去年做了 seer 的一作，相当 solid）放出了最新的大作，InternData A1，个人也有幸在其中混了一下。趁机聊聊这篇可以说近期以来最 highlight 的仿真工作，以及在与洋哥一直以来交流切磋，得到的不少仿真的 insight。

## 关于 InternData A1

在当下的时间点来看，InternData A1 可以说是目前具身智能仿真领域最大的数据集，我们在 Isaac Sim 中合成了大量的仿真数据，从技能数量上以及各种维度上都远超于之前的工作，并且实现了个人认为的两个重大 milestone。

其一，InternData A1 在数据质量上达到了 Pi[^pi0][^pi05] Dataset 水准。

[^pi0]: Pi-0: https://arxiv.org/abs/2410.24164
[^pi05]: Pi-0.5: https://arxiv.org/abs/2504.16054

当下其实有不少的模型工作都在比较的时候超越了 Pi0 的水平。当然，其中比较 tricky 的点其实不少，更好的 base model，在 Pi 的基础上进行进一步的 finetune，或者一些特殊的模型设计。事实上 Pi 系列工作一直以来都是比较直接的工作，从直觉出发，并且在大量数据上进行训练，因此在比较的时候，我们也完全从直觉出发，给出了本质的比较。

InternData A1 作为仿真的数据合成管线以及数据集，我们使用我们的数据 From scratch 训练了 PaliGemma[^paligemma]，并且和从 Pi Dataset 的训练的 Pi0 在仿真/真机进行了大量的实验，包括了不同的任务，可以说是从各个维度进行评测，来体现预训练的效果，结果上来说是可以超过了 Pi Dataset，而并不 overclaim 来说，「comparable」也已经是很客观的评价了。与其他大规模数据集相比则是明显超过。我们都采用很直接的训练，不包括任何的 trick，结果相当 solid。

[^paligemma]: PaliGemma: https://arxiv.org/abs/2407.07726

这意味着我们为任何意义上的模型训练提供了一个大规模的高质量数据集，也意味着，在此之前学界所质疑的，仿真存在的 sim2real gap 带来的不可用性在预训练层面也得到了证伪。这毫无疑问是前所未有的。

其二，InternData A1 也实现了完全的 sim2real 单任务后训练。

InternData A1 直接进行 Sim 的 pre-training 和 post-training，然后 deploy in real，这其中也依然相当直接，一些任务同样 impressive。这意味着 photorealistic 的仿真管线足以跨越 sim 与 real 的 gap，兼顾仿真的效率与现实的质量。伴随着渲染技术的发展，我们发现，排除玄之又玄的物理一致性之外，主要制约 sim 与 real 的主要确实在于 texture level（btw 这似乎也是为什么一些 locomotion 工作在需要视觉的时候采用 depth 作为输入），而这一问题在发展中正在被不断解决。

## 预训练科学以及之后

在 GEN-0 发布之后，我和洋哥曾经聊了很多，当时确实是可以说冲击很大，数据工厂发展的速度实属太快了，至少比我的预期快了半年到一年。当时和洋哥曾经笑着开玩笑，说，「假如 GEN-0 还没发布，我认为你的工作大概是 9 分，不过现在可能只有 6 分或者 7 分」。当然，这是调侃，实际上 InternData A1 依然值得 8 分，在仿真上值得 9 或者 10，不过这其实反映了一个点，我们认为最为宝贵的，从数据中迭代到了认知，被 GEN-0[^gen0] 先行一步讲出了大多数。

[^gen0]: GEN-0: https://generalistai.com/blog/nov-04-2025-GEN-0

在此再次旧事重提，也就是预训练科学中的得到的见解：数据多样性的提高带来模型的预训练效果提高。

在论文的实验中，我们在完整的数据中减去包含大量物品 instance 的大规模 pick and place 数据或者相对物体较少的灵巧操作数据，事实证明尽管前者包含更多的物体以及更大的数量，对于预训练效果的帮助其实更小。

从某种程度上，不只是多样性的重要性。仅从我个人的观点出发，这甚至在一定程度上可以说明，朴素的预训练几乎不从 VL 中大量学习，或者几乎不学习 VL to A 的 mapping，这显然留下了值得探索的空白。

另一方面，关于仿真。

毫无疑问，InternData A1 给目前还几乎没有新消息的仿真打了一剂强而有力的强心剂。在此之前仿真中较为知名的可能是 Simpler Env[^simplerenv]，生成数据来说则是 RoboCasa，最近比较火的应该是 Feifei Li 的 Behavior 挑战赛。

[^simplerenv]: Simpler Env: https://arxiv.org/abs/2405.05941
[^roboCasa]: RoboCasa: https://arxiv.org/abs/2406.02523
[^behaviorchallenge]: Behavior Challenge: https://behavior.stanford.edu/challenge/index.html

长久以来数据生成一直在沉寂中，在 RoboCasa 之后的工作绝大多数都是 toy 的规模，Casa 也只是浅尝辄止，这本质上源自于两头均有难度的 trade off。

数据生成的方案，在搭建了基座之后，主要的问题在于动作轨迹的生成。其中主要是三种方案。

其一，即 RL 方式采集数据。好处在于，理论上只需要 goal condition 就可以生成数据，这毫无疑问可以搭建一条引入 VLM 不断设计任务的自动化管线。然而事实是，只使用 Goal Condition 会带来显著的 Dense reward 问题，导致数据几乎不可用。RoboGen 是这方面的先驱，质量可以说相当高，但是其主页中关于 mobile manipulation 的 demo 其实也都比较奇怪，感兴趣的读者可以去看看。这是一个后续值得探索的点，不过至少目前此路不通。

其二，即使用 hardcode 来写 skill。这方面的工作其实不少，尤其是在大量的 Benchmark 工作中。他们之所以使用一堆的 task 的定义，除了在 hardcode 定义任务之外，也在 hardcode 写采集数据的程序。代价显而易见，就是并不 scalable。Gemini 给出了有趣的调侃，「任务的数量并不能说明它的规模，而只能说明它招揽的实习生数量」。

其三，也就是所谓的 general skill。也就是依然使用 code，但是通过一些解算来计算 key way point，从而尽量避免 hardcode，在一定程度上增加泛化，节约 scaling 的时候的代码量。而这毫无疑问需要水平，以及有的任务也很难 general。

当然，除此之外，对于仿真，流体与软体也是难点，因此 InternData A1 做出了这些内容，才令人惊喜。本身我们的方案使用了 general 与 hardcode 在一定程度上获得了恰当的 balance，效果上还算不错。

在和洋哥讨论的时候，我曾经聊过，是否要说一些风凉话，洋哥觉得这种事情说出来也是有价值的。尽管仿真目前看上去，似乎我们也能做 pre-training 了，有算力的话效率也有了，岂不是一片大好。然而在长期的探索中，我们似乎看到了一些 upper bound。

其一，也是最严重的问题，即 long horizon 数据采集的指数级开销。对于 short horizon 数据来说，我们可以接受一定的开销，甚至即使成功率是 1%，其实在百卡集群的生成中，依然可以效率不错，但是将这些 task 组合为长程任务后，伴随着成功率的乘积，生成效率呈指数级下降。这件事情在短期内来看几乎是不可避免的死局，除非我们可以将 subtask 的成功率提升到 100%，或者用一些 hack 的手段解决这个问题。对于真机，这种问题往往出现概率更低，尤其是在 UMI[^umi] 上，成熟的数采人员失败概率可以接近于零，Long horizon 的采集更加简单。在「多样性」是第一性的前提下，真机的方案更加可能。

[^umi]: UMI: https://arxiv.org/abs/2402.10329

第二，InternData A1 没有探索数据的 upper bound。A1 其实基本上在 skill 的数量上做出了充分探索，但是在训练成本等方面取舍之后，并没有更无限度地增加数据数量，即对于数据的广度比较充足，而深度有限。问题是，假如说当前的广度，或者当前广度的非几何数量级提升之后，只在深度上提升数量，能否作为充分的数据。一个观察是，目前来看的任何领域，数据都不存在所谓充足；另一个观察是，数据对模型的边际效益明显（毕竟是指数），而仿真的开发成本对数据边际效益同样明显，对算力确实是线性，而显然平方级别的边际效益也说明了一些事。

除此之外，其实本身仿真还是迎来了更多的发展，我们的 Codebase 以及相关资产之后也会开源，相信对于社区还是 pre-training research 都有相当可观的裨益。

## 项目地址

论文: [https://arxiv.org/abs/2511.16651](https://arxiv.org/abs/2511.16651)

Huggingface: [https://huggingface.co/datasets/InternRobotics/InternData-A1](https://huggingface.co/datasets/InternRobotics/InternData-A1)