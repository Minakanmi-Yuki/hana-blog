---
title: "RLç¬”è®°ï¼ˆ12ï¼‰ï¼šPPO"
publishDate: 2025-12-21
updatedDate: 2025-12-21
description: "OpenAI çš„é»˜è®¤ç®—æ³•ï¼šè¯¦è§£ PPO å¦‚ä½•é€šè¿‡ Clip æŠ€å·§ç®€åŒ– TRPOã€‚æ¶µç›– PPO-Clip ä¸ PPO-Penalty ä¸¤ç§å˜ä½“ã€GAE ä¼˜åŠ¿ä¼°è®¡åŠå®Œæ•´çš„æŸå¤±å‡½æ•°è®¾è®¡ã€‚åœ£PPOä¼Ÿå¤§æ— éœ€å¤šè¨€ï¼"
heroImage: {src : "https://pic.hana0721.top/rl-note-2.8vndvqvn7c.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## å¼•è¨€ï¼ˆIntroductionï¼‰

ä¸Šä¸€ç« æåˆ°çš„ **TRPO** è™½ç„¶ç†è®ºä¼˜ç¾ï¼ˆä¿è¯å•è°ƒä¸å‡ï¼‰ï¼Œä½†è®¡ç®—å¤ªå¤æ‚äº†ï¼ˆéœ€è¦å…±è½­æ¢¯åº¦æ³•æ±‚è§£ Hessian-Vector Productï¼‰ã€‚
OpenAI æå‡ºçš„ **PPO (Proximal Policy Optimization)** æ˜¯ TRPO çš„ä¸€é˜¶è¿‘ä¼¼ç‰ˆæœ¬ã€‚
*   **æ ¸å¿ƒæ€æƒ³**ï¼šTRPO ä½¿ç”¨ KL æ•£åº¦ä½œä¸º**ç¡¬çº¦æŸ (Constraint)**ï¼Œè€Œ PPO å°†çº¦æŸè½¬åŒ–ä¸º**æƒ©ç½šé¡¹ (Penalty)** æˆ–è€…ç›´æ¥é€šè¿‡**æˆªæ–­ (Clipping)** æ¥é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ã€‚
*   **åœ°ä½**ï¼šPPO æ˜¯ç›®å‰ Deep RL çš„åŸºå‡†ç®—æ³•ï¼Œå¹³è¡¡äº†å®ç°å¤æ‚åº¦ã€æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚

---

## å˜ä½“ 1ï¼šPPO-Clip (ä¸»æµ)

è¿™æ˜¯ç›®å‰æœ€å¸¸ç”¨çš„ PPO ç‰ˆæœ¬ï¼Œå®ƒä¸éœ€è¦è®¡ç®— KL æ•£åº¦ï¼Œç›´æ¥åœ¨ç›®æ ‡å‡½æ•°é‡ŒåŠ¨æ‰‹è„šã€‚

###  é‡è¦æ€§é‡‡æ ·æ¯”ç‡
å®šä¹‰æ–°æ—§ç­–ç•¥çš„æ¯”ç‡ä¸º $r_t(\theta)$ï¼š
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$
*   å½“ $\theta = \theta_{\text{old}}$ æ—¶ï¼Œ$r_t = 1$ã€‚
*   æˆ‘ä»¬å¸Œæœ› $r_t$ ä¸è¦åç¦» 1 å¤ªå¤šï¼Œè¿™æ„å‘³ç€æ–°ç­–ç•¥æ²¡æœ‰å‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚

###  æˆªæ–­ç›®æ ‡å‡½æ•°
PPO-Clip çš„æ ¸å¿ƒç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š
$$
L^{CLIP}(\theta)=\hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \right) \right]
$$

*   $\epsilon$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼ˆé€šå¸¸ä¸º 0.2ï¼‰ï¼Œè¡¨ç¤ºå…è®¸ç­–ç•¥å˜åŒ–çš„å¹…åº¦ï¼ˆTrust Regionï¼‰ã€‚
*   $\text{clip}(r, 1-\epsilon, 1+\epsilon)$ï¼šæŠŠæ¯”ç‡ $r$ å¼ºåˆ¶é™åˆ¶åœ¨ $[0.8, 1.2]$ ä¹‹é—´ã€‚

###  ç›´è§‚ç†è§£ï¼šä¸ºä»€ä¹ˆè¦ Minï¼Ÿ
æˆ‘ä»¬éœ€è¦åˆ†ä¸¤ç§æƒ…å†µæ¥çœ‹ä¼˜åŠ¿å‡½æ•° $\hat{A}_t$ çš„æ­£è´Ÿï¼š

*   **æƒ…å†µ Aï¼šåŠ¨ä½œæ˜¯å¥½çš„ ($\hat{A}_t > 0$)**
    *   æˆ‘ä»¬å¸Œæœ›å¢åŠ è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œå³ $r_t(\theta)$ å¢å¤§ã€‚
    *   ä½†ä¸èƒ½æ— é™å¢å¤§ã€‚å¦‚æœ $r_t > 1+\epsilon$ï¼Œ`clip` å‡½æ•°ä¼šå°†å…¶é”æ­»åœ¨ $1+\epsilon$ã€‚
    *   æ­¤æ—¶ $\min$ æ“ä½œç”Ÿæ•ˆï¼Œç›®æ ‡å‡½æ•°ä¸å†éš $r_t$ å¢åŠ è€Œå¢åŠ ã€‚è¿™é˜²æ­¢äº†ç­–ç•¥æ›´æ–°æ­¥å­å¤ªå¤§ã€‚
*   **æƒ…å†µ Bï¼šåŠ¨ä½œæ˜¯åçš„ ($\hat{A}_t < 0$)**
    *   æˆ‘ä»¬å¸Œæœ›å‡å°è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œå³ $r_t(\theta)$ å‡å°ã€‚
    *   ä½†ä¸èƒ½æ— é™å‡å°ã€‚å¦‚æœ $r_t < 1-\epsilon$ï¼Œ`clip` å‡½æ•°ä¼šå°†å…¶é”æ­»åœ¨ $1-\epsilon$ã€‚
    *   æ­¤æ—¶ $\min$ æ“ä½œç”Ÿæ•ˆï¼Œé˜²æ­¢ç­–ç•¥è¿‡åº¦ä¿®æ­£ï¼ˆä»¥æ­¤é¿å…ç­–ç•¥åå¡Œï¼‰ã€‚

> **ğŸ’¡ æ€»ç»“**ï¼š
> åªæœ‰å½“ç­–ç•¥å˜åŒ–åœ¨â€œå®‰å…¨åŒºåŸŸâ€å†…æ—¶ï¼Œæˆ‘ä»¬æ‰è¿›è¡Œå¥–åŠ±ä¼˜åŒ–ï¼›ä¸€æ—¦è¶…å‡ºå®‰å…¨åŒºåŸŸï¼Œå°±ä¸å†ç»™äºˆé¢å¤–çš„æ¢¯åº¦å¥–åŠ±ã€‚

---

## å˜ä½“ 2ï¼šPPO-Penalty (è‡ªé€‚åº” KL)

è¿™æ˜¯å¦ä¸€ç§æ¥è¿‘ TRPO åŸä¹‰çš„æ–¹æ³•ï¼Œå°† KL æ•£åº¦ä½œä¸ºæ­£åˆ™é¡¹åŠ å…¥ Lossï¼Œå¹¶åŠ¨æ€è°ƒæ•´ç³»æ•° $\beta$ã€‚

### ç›®æ ‡å‡½æ•°
$$
L^{KLPEN}(\theta)=\hat{\mathbb{E}}_t \left[ r_t(\theta)\hat{A}_t - \beta D_{KL}(\pi_{\theta_{\text{old}}}(\cdot|s_t) || \pi_\theta(\cdot|s_t)) \right]
$$

### è‡ªé€‚åº” $\beta$ æ›´æ–°è§„åˆ™
æˆ‘ä»¬åœ¨æ¯æ¬¡æ›´æ–°åè®¡ç®—å¹³å‡ KL æ•£åº¦ $d = \hat{\mathbb{E}}_t [D_{KL}]$ï¼Œå¹¶ä¸ç›®æ ‡å€¼ $d_{\text{targ}}$ æ¯”è¾ƒï¼š

*   **å¦‚æœ $d < d_{\text{targ}} / 1.5$**ï¼šè¯´æ˜ç­–ç•¥å˜åŠ¨å¤ªå°ï¼Œæ­¥å­å¤ªä¿å®ˆã€‚**å‡å°æƒ©ç½š** $\beta \leftarrow \beta / 2$ã€‚
*   **å¦‚æœ $d > d_{\text{targ}} \times 1.5$**ï¼šè¯´æ˜ç­–ç•¥å˜åŠ¨å¤ªå¤§ï¼Œæ­¥å­å¤ªå±é™©ã€‚**å¢å¤§æƒ©ç½š** $\beta \leftarrow \beta \times 2$ã€‚

---

## å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE)

åœ¨è®¡ç®—ä¼˜åŠ¿å‡½æ•° $\hat{A}_t$ æ—¶ï¼Œç®€å•çš„å¤šæ­¥ TD æˆ–è’™ç‰¹å¡æ´›éƒ½æœ‰å±€é™ã€‚PPO é€šå¸¸ä½¿ç”¨ **GAE (Generalized Advantage Estimation)** æ¥å¹³è¡¡åå·®å’Œæ–¹å·®ã€‚

æˆ‘ä»¬å®šä¹‰ TD Error $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ã€‚
GAE æ˜¯ $\delta$ çš„åŠ æƒå‡ ä½•å¹³å‡ï¼š

$$
\hat{A}_t^{GAE} = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}
$$

*   $\lambda = 0$ï¼šå³å•æ­¥ TDï¼ˆåå·®å¤§ï¼Œæ–¹å·®å°ï¼‰ã€‚
*   $\lambda = 1$ï¼šå³è’™ç‰¹å¡æ´›ï¼ˆæ— åå·®ï¼Œæ–¹å·®å¤§ï¼‰ã€‚
*   $\lambda \in (0, 1)$ï¼šé€šå¸¸å– 0.95ï¼Œåœ¨ä¸¤è€…ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ã€‚

---

## å®Œæ•´çš„ PPO æŸå¤±å‡½æ•°

åœ¨å®é™…ä»£ç å®ç°ï¼ˆå¦‚ Actor-Critic æ¶æ„ï¼‰ä¸­ï¼ŒPPO çš„æ€» Loss åŒ…å«ä¸‰éƒ¨åˆ†ï¼š
1.  **ç­–ç•¥æŸå¤± (Policy Loss)**ï¼šå³ $L^{CLIP}$ï¼Œè®©ç­–ç•¥å˜å¥½ã€‚
2.  **ä»·å€¼æŸå¤± (Value Loss)**ï¼š$L^{VF} = (V_\theta(s_t) - V_{target})^2$ï¼Œè®© Critic ä¼°å€¼æ›´å‡†ã€‚
3.  **ç†µå¥–åŠ± (Entropy Bonus)**ï¼š$S[\pi_\theta]$ï¼Œé¼“åŠ±ç­–ç•¥ä¿æŒéšæœºæ€§ï¼Œé˜²æ­¢è¿‡æ—©æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚

$$
L^{Total}_t(\theta) = - L^{CLIP}_t(\theta) + c_1 L^{VF}_t(\theta) - c_2 S[\pi_\theta](s_t)
$$
*(æ³¨ï¼šé€šå¸¸æ·±åº¦å­¦ä¹ æ¡†æ¶æ˜¯æœ€å°åŒ– Lossï¼Œæ‰€ä»¥æœ€å¤§åŒ–ç›®æ ‡å‰åŠ è´Ÿå·)*

---

## PPO ç®—æ³•æµç¨‹

$$
\begin{aligned}
& \bullet \; \text{Initialize policy parameters } \theta \text{ and value parameters } \phi \\
& \bullet \; \textbf{For } \text{iteration } k = 1, 2, \dots \textbf{ do}: \\
& \bullet \qquad \textbf{1. Data Collection:} \\
& \bullet \qquad \text{Run policy } \pi_{\theta_{old}} \text{ in environment for } T \text{ steps} \\
& \bullet \qquad \text{Compute advantage estimates } \hat{A}_1, \dots, \hat{A}_T \text{ using GAE} \\
& \bullet \qquad \textbf{2. Optimization:} \\
& \bullet \qquad \textbf{For } \text{epoch } = 1 \to K \textbf{ do}: \\
& \bullet \qquad \qquad \text{Shuffle data and divide into mini-batches} \\
& \bullet \qquad \qquad \textbf{For } \text{each mini-batch } B \textbf{ do}: \\
& \bullet \qquad \qquad \qquad L = L^{CLIP}(\theta) - c_1 L^{VF}(\phi) + c_2 S[\pi_\theta] \\
& \bullet \qquad \qquad \qquad \text{Update } \theta, \phi \text{ using Adam optimizer} \\
& \bullet \qquad \qquad \textbf{End For} \\
& \bullet \qquad \textbf{End For} \\
& \bullet \qquad \theta_{old} \leftarrow \theta \\
& \bullet \; \textbf{End For}
\end{aligned}
$$

---

## æ€»ç»“

PPO ä¹‹æ‰€ä»¥èƒ½æˆä¸º OpenAI çš„é»˜è®¤ç®—æ³•ï¼ˆDefault Algorithmï¼‰ï¼Œæ˜¯å› ä¸ºå®ƒï¼š
1.  **ç®€å•**ï¼šåªéœ€è¦å¯¹æ¢¯åº¦è¿›è¡Œç®€å•çš„æˆªæ–­ï¼ˆClipï¼‰ï¼Œä¸éœ€è¦å¤æ‚çš„äºŒé˜¶ä¼˜åŒ–ã€‚
2.  **ç¨³å®š**ï¼šæˆªæ–­æœºåˆ¶ä¿è¯äº†ç­–ç•¥ä¸ä¼šå› ä¸ºä¸€æ¬¡ç³Ÿç³•çš„æ›´æ–°è€Œå´©æºƒã€‚
3.  **é€šç”¨**ï¼šæ—¢é€‚ç”¨äºç¦»æ•£åŠ¨ä½œï¼ˆAtariï¼‰ï¼Œä¹Ÿé€‚ç”¨äºè¿ç»­åŠ¨ä½œï¼ˆæœºå™¨äººæ§åˆ¶ï¼‰ã€‚

è‡³æ­¤ï¼Œç»å…¸çš„ Policy Gradient å®¶æ—ï¼ˆREINFORCE $\to$ Actor-Critic $\to$ TRPO $\to$ PPOï¼‰å·²ç»æ¢³ç†å®Œæ¯•ã€‚