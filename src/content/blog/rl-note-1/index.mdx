---
title: "RL笔记（1）：初入强化学习"
publishDate: 2025-12-10
updatedDate: 2025-12-10
description: "什么是强化学习？它与监督学习有何不同？核心要素与基本世界观的构建。"
heroImage: {src : "https://pic.hana0721.top//rl-note.3yex1wenfg.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

**强化学习 (Reinforcement Learning, RL)** 是机器学习中一个独特的领域，它关注的是**智能体 (Agent)** 如何在**环境 (Environment)** 中通过**试错 (Trial-and-Error)** 来学习最佳策略。

如果说监督学习是“老师手把手教你做题”（有标签），那么强化学习就是“把你扔进游戏里自己玩，赢了得分，输了扣分”。智能体必须通过与环境的交互，自己总结出怎么做才能获得最高的**累积奖励**。

> **核心直觉**：
> 就像训练小狗：小狗做动作（Action），你给它骨头或训斥（Reward）。小狗为了多吃骨头，逐渐学会了“听到口令坐下”这个策略（Policy）。

---

## 强化学习 vs. 其他机器学习

为了搞懂 RL，我们先看它和我们熟悉的监督学习有什么本质区别：

1.  **没有“正确答案” (No Ground Truth)**：
    *   **监督学习**：输入一张猫的图片，标签明确告诉你“这是猫”。
    *   **RL**：你走了一步棋，没人告诉你这一步是“对”还是“错”，你只知道这局棋最后是赢了还是输了。
2.  **反馈是延迟的 (Delayed Reward)**：
    *   你现在做的一个决定（比如买股票），可能要很久之后（卖出时）才知道好坏。这被称为**信用分配问题 (Credit Assignment Problem)**。
3.  **数据是动态分布的 (Non-i.i.d Data)**：
    *   你的策略变了，你看到的画面（状态）也就变了。智能体的动作会直接改变它未来接收到的数据。

---

## 核心组件：RL 的世界观

强化学习的一切都发生在一个循环交互中。我们需要掌握以下几个核心术语，它们是后续所有章节的基石。

### 1. 交互循环 (The Loop)
在每一个时刻 $t$：
1.  **观察**：智能体看到环境的状态 $S_t$。
2.  **动作**：智能体根据策略，选择一个动作 $A_t$。
3.  **反馈**：环境受到动作影响，变成新状态 $S_{t+1}$，并给出一个即时奖励 $R_t$。

### 2. 状态 (State, $S$)
状态是对环境现状的描述。
*   **例子**：在玩《王者荣耀》时，屏幕上的所有画面（英雄位置、血量、小地图）就是状态。
*   **全观测 vs. 部分观测**：如果你能看到整个棋盘，这叫全观测；如果你在打扑克，你看不到对手的牌，这叫部分观测（POMDP）。

### 3. 动作 (Action, $A$)
智能体能做的事情。
*   **离散动作**：上下左右、跳跃、开火（如超级马里奥）。
*   **连续动作**：方向盘转动 30.5 度、油门踩 70%（如自动驾驶）。

### 4. 奖励 (Reward, $R$)
奖励是一个标量数值，是环境给智能体的唯一反馈信号。
*   **奖励假设 (The Reward Hypothesis)**：所有目标都可以被描述为“最大化累积奖励的期望”。
*   **注意**：奖励定义了“好坏”，但没告诉智能体“怎么做”。

### 5. 策略 (Policy, $\pi$)
策略是智能体的大脑，它定义了**在某个状态下该采取什么动作**。
*   **确定性策略**：看到 $s$，就做 $a$。即 $a = \pi(s)$。
*   **随机性策略**：看到 $s$，有 70% 概率做 $a_1$，30% 概率做 $a_2$。即 $\pi(a|s) = P(A_t=a|S_t=s)$。

### 6. 价值 (Value, $V$)
这是 RL 中最关键的概念之一（后续笔记会详细讲）。
**奖励**是眼前的利益，**价值**是长远的目光。
*   **例子**：为了赢下棋局（高价值），你可能需要牺牲一个车（负的即时奖励）。
*   价值函数预测了：从当前状态出发，未来**总共**能拿多少分。

---

## 两个核心难题

在深入学习具体算法前，我们需要理解阻碍智能体变强的两大难题：

### 1. 序列决策 (Sequential Decision Making)
智能体的动作有**长远影响**。现在的选择决定了未来的状态，进而限制了未来的选择。
*   *解决思路*：我们需要引入 **马尔可夫决策过程 (MDP)** 来数学化描述这个过程（详见笔记 3），并通过 **动态规划**（笔记 5）或 **时序差分**（笔记 6）来求解未来的价值。

### 2. 探索与利用 (Exploration vs. Exploitation)
这是 RL 独有的矛盾。
*   **利用 (Exploitation)**：根据现有经验，做那个我认为最好的动作（拿确定的分数）。
*   **探索 (Exploration)**：尝试一个没做过的动作，虽然可能导致扣分，但也可能发现一条通往更高分的新捷径。
*   *解决思路*：就像去餐厅吃饭，是去吃那家确定的好吃的店（利用），还是去试一家新开的店（探索）？
*   这个问题在 **多臂老虎机 (Multi-armed Bandit)** 问题中最为纯粹（详见笔记 2）。

---

## 学习路线图 (Roadmap)

本系列笔记将按照以下逻辑逐步深入：

1.  **无状态的基础**：从最简单的**多臂老虎机**（笔记 2）开始，只考虑动作选择，不考虑状态转移。
2.  **建立数学模型**：引入**马尔可夫决策过程 (MDP)**（笔记 3），正式描述状态转移和序列决策。
3.  **求解 MDP**：
    *   如果你知道环境的一切规则（上帝视角），使用**动态规划**（笔记 5）。
    *   如果你只能通过玩游戏来学习（蒙眼摸象），使用**蒙特卡洛**（笔记 4）和**时序差分**（笔记 6）。
4.  **进阶与深度强化学习**：
    *   当状态太多存不下表格时，我们引入神经网络，进入 Deep RL 时代（DQN, Policy Gradient, Actor-Critic 等）。
    *   一直到最前沿的 SAC, PPO 等算法。

让我们开始这段旅程吧！