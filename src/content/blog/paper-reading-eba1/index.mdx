---
title: 'Paper Reading: Embodied AI 1'
publishDate: 2025-12-26
updatedDate: 2025-12-26
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

import { ManualTOC } from '@/components/advanced'

<ManualTOC
  title=''
  categories={[
    {
      title: 'Embodied AI Paper Reading',
      items: [
        {
          title: 'Batch 1',
          href: '/blog/paper-reading-eba1',
          order: '1'
        },
        {
          title: 'Batch 2',
          href: '/blog/paper-reading-eba2',
          order: '2'
        },
        {
          title: 'Batch 3',
          href: '/blog/paper-reading-eba2',
          order: '3'
        }
      ]
    }
  ]}
/>

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## ACT

<ArxivRating id='2304.13705' tldr='ACT采用了CVAE架构，使用了BERT风格的编码器以及Transformer风格的解码器，在高频机器人Manipulation任务上表现出优异的性能。' rank={3}>
![](https://pic.hana0721.top/act-easy.92qlspndp7.webp)
ACT是一种经典的机器人Manipulation方法，采用了CVAE架构，上图展示了ACT的模型架构。
ACT的输入是机器人Joint position以及多视角下的观测图像，输出是未来一系列的Action。
CVAE-Encoder采用了BERT-like的结构，以机器人Joint position和专家Action chunking（加上了position embedding）为输入，并将\[CLS\]经过投影后得出隐向量$z$。
CVAE-Decoder采用Transformer-like的结构，输入是经过特征提取后的多视角Image token、机器人Joint position以及经过CVAE-Encoder编码后的隐向量 $z$（在测试阶段， $z$是0），输出是Action chunking。
在训练过程中，使用了L1损失而非L2损失，这里文章里有详细的解释。
![](https://pic.hana0721.top/temperal-ensembling.51emebjacb.webp)
ACT使用了Temperal ensembling来提高输出动作哦的平滑性，其实就是针对多个Action chunking同个时间步下的Action进行加权，具体见图。
ACT在文章的最后给出了详细的流程图，非常清晰，这点很值得点赞！
![](https://pic.hana0721.top/act-all.58hu9r5frx.webp)

</ArxivRating>

## DP

<ArxivRating id='2303.04137' tldr='DP使用历史观测作为Condition的Diffusion模型来生成Action Chunk。具有更快的推理速度以及更多的动作模态' rank={3}>
![](https://pic.hana0721.top/diffusion-policy.3d59h5tcqw.webp)
Diffusion Policy是非常经典的工作。
简单来说，就是使用Diffusion模型来进行Action的预测，输入是Noise，输出是Action Chunk。文中给出了CNN-based和Transformer-based的两种变体。
不同于DDPM，除了去噪step $k$，DP还将历史观测 $o_t,o_{t-1},o_{t-2}$ 作为Condition。
![](https://pic.hana0721.top/multimodal-action.51emecjmx5.webp)
DP直接输出Action Chunk，加快推理速度的同时也提高了Action的一致性和多样性。
在实际执行过程中，采用了保守策略，只执行Action Chunk的前几步Action，这样会更加稳定。

</ArxivRating>

## RT-1

<ArxivRating id='2212.06817' tldr='RT-1采用FiLM结构处理Image观测以及Language指令得出Token，并使用Transformer-Decoder来解码Action' rank={3}>
![](https://pic.hana0721.top/RT-1-easy.7axmxvm4yg.webp)
RT-1是Google于22年提出的工作，是一种王源（传统）VLA模型。
VLA模型其实就以视觉图像、语言指令等为输入，输出智能体的Action。
为什么这么说它是王源（传统）呢？因为它没有使用经过大规模预训练的LLM或是VLM作为Fusion模型。
RT-1旨在实现泛化性能，使用了大规模的Multi-task Dataset进行训练，能够完成少量的Unseen Task。
![](https://pic.hana0721.top/RT-1-all.3uvb5scf72.webp)
文中给出了详细的架构图，下面我来分块讲解。
从整体上来看RT-1，它以历史图像（文中是6个图像）和语言指令为输入，以Action为输出。
首先，它使用了FiLM架构来融合Image和Language的信息。
视觉主干网络是EfficientNet-3B（经过ImageNet预训练），语言特征提取使用了Universal Sentence Encoder，这些网络的参数能从网上下载。
然后，得出了Image Token，并进一步使用TokenLearner进行降维处理。
最后，将Token拼接起来，并使用位置编码，经过一个Transformer-Decoder风格的解码器，最终得出Action。
值得注意的是，RT-1将连续动作进行了离散化，文中具体离散为256个区间。
在结构上可以看到，RT-1采用了许多推理加速的方法，比如减少输入的Token或是采用高效的模型（EfficientNet-3B）。
RT-1虽然能够做到一定程度的泛化，但是也有一定的局限性。
为什么说具有局限性？首先，RT-1只使用了具身Dataset直接拟合Action，优质的具身Dataset本来就少，而且难以获取。
再来就是，RT-1也很容易摆烂，也是因为Dataset不够全面，遇到一些没有见过的物体，可能就无法work了。
RT-1早期的贡献还是值得肯定的，并且在后续的工作RT-2中解决了这些问题。

</ArxivRating>

## RT-2

<ArxivRating id='2307.15818' tldr='RT-2加入了VLM进行Fusion，是现有比较规范的VLA架构。' rank={3}>
![](https://pic.hana0721.top/RT-2.8l0k484kpi.webp)
RT-2是Google继RT-1后于2023年提出的一种VLA模型。
RT-2抛弃了RT-1的FiLM结构，使用了预训练的VLM来处理Image观测与Language指令（本质是Image经过ViT映射为Language Token并与指令Token一同输入到LLM中）。
RT-2采用了PaLI-X  和 PaLM-E 作为主干，这两个模型都进行了大规模VQA预训练。
和RT-1一样，RT-2也对连续动作进行了离散化处理（256个区间），但RT-2将离散动作映射到了LLM Token中，因此需要保留256个词元作为动作词元。
对PaLI-X，将动作词元对应到整数词元。对PaLM-E，则采用频率最低的256词元作为动作词元。
同时，RT-2对训练数据进行了处理，转为了适合VLM训练的VQA格式。
RT-2使用了协同微调，也就是将网络数据与机器人数据混合起来微调，这样可以提高泛化能力。
因为使用LLM进行Action的输出，所以需要限制其输出动作词元，RT-2使用了Prompt进行限制。
RT-2发布了不同参数规模的模型，如5B和55B。5B可以部署在本地，55B需要使用云服务进行部署。
RT-2还可以结合CoT的架构进行推理，如SayCan这类方法，以处理更加复杂的指令。
![](https://pic.hana0721.top/RT-2-performance.3uvb5ucnb1.webp)
RT-2在Unseen限制下表现出一定的泛化能力，但还是具有局限性。
比如，它不能处理一些没有经验的新动作，且无法完成一些Unseen Object的动力学任务（如移动马克笔到指定位置）。
它使用离散化的动作，可能难以处理一些高精度的任务。
而且使用低频词元去学习高频输出的动作词元，会出现双峰学习的问题（学习VLM降低这些词元的输出频率，而学习VLA却要提高，属于是左右脑互博）。

</ArxivRating>

## OpenVLA

<ArxivRating id='2406.09246' tldr='OpenVLA-like的奠基工作' rank={4}>

![](https://pic.hana0721.top/openvla.iclclg6tx.webp)
OpenVLA基本继承了RT-2的思想，形成了OpenVLA-like的VLA范式。
简单来说，就是使用LLM对Image观测以及Language指令进行Fusion，输出Action Token，最后再Detokenizer成机器人可执行的Action。
OpenVLA使用Prismatic-7B VLM作为主干，文中详细解释了为什么采用Prismatic-7B VLM。
和RT-2一样，使用256个频率最低的词元作为Action Token。
训练数据主要使用了OXE Dataset，并对数据进行了处理，以便于训练。
最终，OpenVLA展示了在不同消费级和服务级的GPU上的运行效果。
然而，OpenVLA也有着诸多Limitation。
首先，它只接受单图像预测，无法处理异构的机器人设置。
其次，它的吞吐量不够，无法实现高频率的控制。
最后，同时也是最大的硬伤，和RT-2一样，在面对Co-Training时，选择频率最低的256个Token作为Action Token，会出现双峰学习问题。
但不可否认的是，OpenVLA还是一份伟大的工作，开创了OpenVLA-like的VLA学习范式。

</ArxivRating>

## OXE | RT-X

<ArxivRating id='2310.08864' tldr='Embodied AI领域的大规模Dataset' rank={5}>

![](https://pic.hana0721.top/OXE.5moa1wm2lz.webp)
Open X-Embodiment数据集包含超过100万个真实机器人轨迹，涵盖22种机器人实体，从单机械臂机器人到双臂机器人和四足机器人。
该数据集是通过汇集来自全球 34 个机器人研究实验室的 60 个现有机器人数据集，并将它们转换为一致的数据格式以便于下载和使用而构建的。
![](https://pic.hana0721.top/RT-X.3yex4q5zbj.webp)
该团队还使用OXE训练了RT-1和RT-2，展现出泛化性能。
工作量如此之大，伟大无需多言。

</ArxivRating>

## Unified Video Action Model

<ArxivRating id='2503.00200' tldr='UVA使用掩码学习了通用的多任务模型' rank={3}>

![](https://pic.hana0721.top/UVA-easy.465lu585n.webp)
从数据流上看，UVA首先学习了Image与Action Chunk的Joint Latent，然后使用Diffusion模型解码出未来的Action Chunk以及观测Image。
为了兼容多种类型的输入，并解耦出对应的Action Chunk以及Image，UVA采用了掩码学习。
![](https://pic.hana0721.top/UVA-all.2h8s31j1cb.webp)
下面讲解学习Joint Latent的流程。
对于每张历史Image使用预训练的VAE编码器进行编码，得出$N$个$d$维的Image Token。
对于每个历史Action Chunk，为了对齐Image Token的维度，采用重复$M$次拼接的方式对齐到$N$维度，并经过一个MLP投影到$N\times d$。
对于要预测的Image，处理方式与历史Image一样，但加上了定义好的掩码。
最终，将三类Token拼接在一起，输入到Transformer中进行编码，得出Joint Latent。
接下来讲解预测流程。
对于预测Image，将每个Token作为Image Diffusion的Condition，经过去噪还原出局部的VAE的Latent，最后拼接经过VAE的解码器还原出Image。
对于预测Action Chunk，则将所有的Token作为Action Diffusion的Condition，经过去噪得出Future Action Chunk。
UVA使用了灵活目标的掩码训练方法，有助于防止模型对特定任务产生过拟合，从而提升模型的整体通用性和鲁棒性。

</ArxivRating>

## Genie Envisioner

<ArxivRating id='2508.05635' tldr='' rank={0}>

</ArxivRating>

## TinyVLA

<ArxivRating id='2409.12514' tldr='TinyVLA是一种早期的PI-like VLA' rank={3}>

![](https://pic.hana0721.top/TinyVLA.64ebqsl3yh.webp)
TinyVLA首次提出了VLM+DP的VLA范式，这个范式后续被应用到了 $\pi$ 系列。
在这个范式下，Action不再由LLM的Token Detokenize出来，而是直接由一个Action Expert（DP或是ACT）输出。
这样做，可以避免RT-2以及OpenVLA中的双峰学习问题，能够避免灾难性遗忘，使得VLM和Action Expert各司其职。
在TinyVLA中，选用了Pythia作为VLM主干，选用DP作为Action Expert。
Action Expert以VLM主干输出的信息为Condition，输出Action Chunk。
在训练过程中，使用LoRA微调VLM，防止了VLM的灾难性遗忘，且可以进行高效的微调。
在实验部分，TinyVLA超越了OpenVLA，且可以在OOD的场景下完成一些任务。
TinyVLA应该是最早提出PI-like架构的工作，算是祖师爷。

</ArxivRating>

## CogACT

<ArxivRating id='2411.19650' tldr='CogACT使用BERT-like的VLM进行Fusion，使用DP输出Action Chunk' rank={3}>

![](https://pic.hana0721.top/CogACT.67xxokoxle.webp)
CogACT也是一种PI-like的VLA，分为VLM Fusion和Action Expert模块。
在VLM Fusion模块中，CogACT采用了一种BERT-like的方式进行Fusion，输入Image观测以及Language指令，输出类似于\[CLS\]的Cognition Feature。
在Action Expert模块中，以Cognition Feature为Condition，经过一个DiT得出Action Chunk。
![](https://pic.hana0721.top/CogACT-ensembling.8s3s17oxad.webp)
CogACT和ACT一样，使用了类似的Action Ensembling，但CogACT中的权重是自适应的，避免了不同模式中不合理地聚合Action。

</ArxivRating>

## PI-0

<ArxivRating id='2410.24164' tldr='PI-like系列奠基之作' rank={4}>
![](https://pic.hana0721.top/pi0.eszi6dq8l.webp)
简单说一下 $\pi_0$ 的两个结构，对Image观测与Language指令进行Fusion的VLM以及基于FLow Matching的Action Expert。在Fusion模块，考虑到实时控制的需求，采用了规模小便利性高的 PaliGemma。Image观测被ViT映射到语义空间，与Language指令一同进入PaliGemma，得出上下文Token。Action Expert采用Flow Matching结构建模Action连续分布。其输入为随机化的Action Chunk，以上下文Token的cross attention以及机器人的Joint Position作为Condition，使用Flow Matching Loss进行训练，输出真实的Action Chunk。
在训练数据上，$\pi_0$ 使用了长达10000小时的真机数据，以及开源数据OXE等。在多种任务上表现出强大的性能，非常牛逼。

</ArxivRating>

## PI-0.5

<ArxivRating id='2504.16054' tldr='' rank={0}>

</ArxivRating>

## GR-3

<ArxivRating id='2507.15493' tldr='' rank={0}>

</ArxivRating>

## InstrucVLA

<ArxivRating id='2507.17520' tldr='' rank={0}>

</ArxivRating>

## DinoV3

<ArxivRating id='2508.10104' tldr='' rank={0}>

</ArxivRating>

## FiS-VLA

<ArxivRating id='2506.01953' tldr='' rank={0}>

</ArxivRating>

## VIMA

<ArxivRating id='2210.03094' tldr='' rank={0}>

</ArxivRating>

## SayCan

<ArxivRating id='2204.01691' tldr='' rank={0}>

</ArxivRating>

## PaLM-E

<ArxivRating id='2303.03378' tldr='' rank={0}>

</ArxivRating>

## ViLA

<ArxivRating id='2311.17842' tldr='' rank={0}>

</ArxivRating>

## CoPa

<ArxivRating id='2403.08248' tldr='' rank={0}>

</ArxivRating>

## PointLLM

<ArxivRating id='2308.16911' tldr='' rank={0}>

</ArxivRating>

## EmbodiedGPT

<ArxivRating id='2305.15021' tldr='' rank={0}>

</ArxivRating>

## RT-Trajectory

<ArxivRating id='2311.01977' tldr='' rank={0}>

</ArxivRating>

## Im2Flow2Act

<ArxivRating id='2407.15208' tldr='' rank={0}>

</ArxivRating>
