---
title: 'Paper Reading: Embodied AI 1'
publishDate: 2025-12-26
updatedDate: 2025-12-26
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## ACT

<ArxivRating id='2304.13705' tldr='ACT采用了CVAE架构，使用了BERT风格的编码器以及Transformer风格的解码器，在高频机器人Manipulation任务上表现出优异的性能。' rank={3}>
![](https://pic.hana0721.top/act-easy.92qlspndp7.webp)
ACT是一种经典的机器人Manipulation方法，采用了CVAE架构，上图展示了ACT的模型架构。
ACT的输入是机器人Joint position以及多视角下的观测图像，输出是未来一系列的Action。
CVAE-Encoder采用了BERT-like的结构，以机器人Joint position和专家Action chunking（加上了position embedding）为输入，并将\[CLS\]经过投影后得出隐向量$z$。
CVAE-Decoder采用Transformer-like的结构，输入是经过特征提取后的多视角Image token、机器人Joint position以及经过CVAE-Encoder编码后的隐向量 $z$（在测试阶段， $z$是0），输出是Action chunking。
在训练过程中，使用了L1损失而非L2损失，这里文章里有详细的解释。
![](https://pic.hana0721.top/temperal-ensembling.51emebjacb.webp)
ACT使用了Temperal ensembling来提高输出动作哦的平滑性，其实就是针对多个Action chunking同个时间步下的Action进行加权，具体见图。
ACT在文章的最后给出了详细的流程图，非常清晰，这点很值得点赞！
![](https://pic.hana0721.top/act-all.58hu9r5frx.webp)

</ArxivRating>

## DP

<ArxivRating id='2303.04137' tldr='DP使用历史观测作为Condition的Diffusion模型来生成Action Chunk。具有更快的推理速度以及更多的动作模态' rank={3}>
![](https://pic.hana0721.top/diffusion-policy.3d59h5tcqw.webp)
Diffusion Policy是非常经典的工作。
简单来说，就是使用Diffusion模型来进行Action的预测，输入是Noise，输出是Action Chunk。文中给出了CNN-based和Transformer-based的两种变体。
不同于DDPM，除了去噪step $k$，DP还将历史观测 $o_t,o_{t-1},o_{t-2}$ 作为Condition。
![](https://pic.hana0721.top/multimodal-action.51emecjmx5.webp)
DP直接输出Action Chunk，加快推理速度的同时也提高了Action的一致性和多样性。
在实际执行过程中，采用了保守策略，只执行Action Chunk的前几步Action，这样会更加稳定。

</ArxivRating>

## RT-1

<ArxivRating id='2212.06817' tldr='RT-1采用FiLM结构处理Image观测以及Language指令得出Token，并使用Transformer-Decoder来解码Action' rank={3}>
![](https://pic.hana0721.top/RT-1-easy.7axmxvm4yg.webp)
RT-1是Google于22年提出的工作，是一种王源（传统）VLA模型。
VLA模型其实就以视觉图像、语言指令等为输入，输出智能体的Action。
为什么这么说它是王源（传统）呢？因为它没有使用经过大规模预训练的LLM或是VLM作为Fusion模型。
RT-1旨在实现泛化性能，使用了大规模的Multi-task Dataset进行训练，能够完成少量的Unseen Task。
![](https://pic.hana0721.top/RT-1-all.3uvb5scf72.webp)
文中给出了详细的架构图，下面我来分块讲解。
从整体上来看RT-1，它以历史图像（文中是6个图像）和语言指令为输入，以Action为输出。
首先，它使用了FiLM架构来融合Image和Language的信息。
视觉主干网络是EfficientNet-3B（经过ImageNet预训练），语言特征提取使用了Universal Sentence Encoder，这些网络的参数能从网上下载。
然后，得出了Image Token，并进一步使用TokenLearner进行降维处理。
最后，将Token拼接起来，并使用位置编码，经过一个Transformer-Decoder风格的解码器，最终得出Action。
值得注意的是，RT-1将连续动作进行了离散化，文中具体离散为256个区间。
在结构上可以看到，RT-1采用了许多推理加速的方法，比如减少输入的Token或是采用高效的模型（EfficientNet-3B）。
RT-1虽然能够做到一定程度的泛化，但是也有一定的局限性。
为什么说具有局限性？首先，RT-1只使用了具身Dataset直接拟合Action，优质的具身Dataset本来就少，而且难以获取。
再来就是，RT-1也很容易摆烂，也是因为Dataset不够全面，遇到一些没有见过的物体，可能就无法work了。
RT-1早期的贡献还是值得肯定的，并且在后续的工作RT-2中解决了这些问题。

</ArxivRating>

## RT-2

<ArxivRating id='2307.15818' tldr='RT-2加入了VLM进行Fusion，是现有比较规范的VLA架构。' rank={3}>
![](https://pic.hana0721.top/RT-2.8l0k484kpi.webp)
RT-2是Google继RT-1后于2023年提出的一种VLA模型。
RT-2抛弃了RT-1的FiLM结构，使用了预训练的VLM来处理Image观测与Language指令（本质是Image经过ViT映射为Language Token并与指令Token一同输入到LLM中）。
RT-2采用了PaLI-X  和 PaLM-E 作为主干，这两个模型都进行了大规模VQA预训练。
和RT-1一样，RT-2也对连续动作进行了离散化处理（256个区间），但RT-2将离散动作映射到了LLM Token中，因此需要保留256个词元作为动作词元。
对PaLI-X，将动作词元对应到整数词元。对PaLM-E，则采用频率最低的256词元作为动作词元。
同时，RT-2对训练数据进行了处理，转为了适合VLM训练的VQA格式。
RT-2使用了协同微调，也就是将网络数据与机器人数据混合起来微调，这样可以提高泛化能力。
因为使用LLM进行Action的输出，所以需要限制其输出动作词元，RT-2使用了Prompt进行限制。
RT-2发布了不同参数规模的模型，如5B和55B。5B可以部署在本地，55B需要使用云服务进行部署。
RT-2还可以结合CoT的架构进行推理，如SayCan这类方法，以处理更加复杂的指令。
![](https://pic.hana0721.top/RT-2-performance.3uvb5ucnb1.webp)
RT-2在Unseen限制下表现出一定的泛化能力，但还是具有局限性。
比如，它不能处理一些没有经验的新动作，且无法完成一些Unseen Object的动力学任务（如移动马克笔到指定位置）。
它使用离散化的动作，可能难以处理一些高精度的任务。
而且使用低频词元去学习高频输出的动作词元，会出现双峰学习的问题（学习VLM降低这些词元的输出频率，而学习VLA却要提高，属于是左右脑互博）。

</ArxivRating>

## OpenVLA

<ArxivRating id='2406.09246' tldr='' rank={4}>

</ArxivRating>

## OXE

<ArxivRating id='2310.08864' tldr='' rank={0}>

</ArxivRating>

## Unified Video Action Model

<ArxivRating id='2503.00200' tldr='' rank={0}>

</ArxivRating>

## Genie Envisioner

<ArxivRating id='2508.05635' tldr='' rank={0}>

</ArxivRating>

## TinyVLA

<ArxivRating id='2409.12514' tldr='' rank={0}>

</ArxivRating>

## CogACT

<ArxivRating id='2411.19650' tldr='' rank={0}>

</ArxivRating>

## PI-0

<ArxivRating id='2410.24164' tldr='' rank={0}>

</ArxivRating>

## PI-0.5

<ArxivRating id='2504.16054' tldr='' rank={0}>

</ArxivRating>

## GR-3

<ArxivRating id='2507.15493' tldr='' rank={0}>

</ArxivRating>

## InstrucVLA

<ArxivRating id='2507.17520' tldr='' rank={0}>

</ArxivRating>

## DinoV3

<ArxivRating id='2508.10104' tldr='' rank={0}>

</ArxivRating>

## FiS-VLA

<ArxivRating id='2506.01953' tldr='' rank={0}>

</ArxivRating>
