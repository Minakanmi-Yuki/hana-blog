---
title: 'Paper Reading: Embodied AI 1'
publishDate: 2025-12-26
updatedDate: 2025-12-26
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

import { ManualTOC } from '@/components/advanced'

<ManualTOC
  title=''
  categories={[
    {
      title: 'Embodied AI Paper Reading',
      items: [
        {
          title: 'Batch 1',
          href: '/blog/paper-reading-eba1',
          order: '1'
        },
        {
          title: 'Batch 2',
          href: '/blog/paper-reading-eba2',
          order: '2'
        },
        {
          title: 'Batch 3',
          href: '/blog/paper-reading-eba3',
          order: '3'
        },
        {
          title: 'Batch 4',
          href: '/blog/paper-reading-eba4',
          order: '4'
        }
      ]
    }
  ]}
/>

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## ACT

<ArxivRating id='2304.13705' tldr='ACT采用了CVAE架构，使用了BERT风格的编码器以及Transformer风格的解码器，在高频机器人Manipulation任务上表现出优异的性能。' rank={3}>
![](https://pic.hana0721.top/act-easy.92qlspndp7.webp)
ACT是一种经典的机器人Manipulation方法，采用了CVAE架构，上图展示了ACT的模型架构。
ACT的输入是机器人Joint position以及多视角下的观测图像，输出是未来一系列的Action。
CVAE-Encoder采用了BERT-like的结构，以机器人Joint position和专家Action chunking（加上了position embedding）为输入，并将\[CLS\]经过投影后得出隐向量$z$。
CVAE-Decoder采用Transformer-like的结构，输入是经过特征提取后的多视角Image token、机器人Joint position以及经过CVAE-Encoder编码后的隐向量 $z$（在测试阶段， $z$是0），输出是Action chunking。
在训练过程中，使用了L1损失而非L2损失，这里文章里有详细的解释。
![](https://pic.hana0721.top/temperal-ensembling.51emebjacb.webp)
ACT使用了Temperal ensembling来提高输出动作哦的平滑性，其实就是针对多个Action chunking同个时间步下的Action进行加权，具体见图。
ACT在文章的最后给出了详细的流程图，非常清晰，这点很值得点赞！
![](https://pic.hana0721.top/act-all.58hu9r5frx.webp)

</ArxivRating>

## DP

<ArxivRating id='2303.04137' tldr='DP使用历史观测作为Condition的Diffusion模型来生成Action Chunk。具有更快的推理速度以及更多的动作模态' rank={3}>
![](https://pic.hana0721.top/diffusion-policy.3d59h5tcqw.webp)
Diffusion Policy是非常经典的工作。
简单来说，就是使用Diffusion模型来进行Action的预测，输入是Noise，输出是Action Chunk。文中给出了CNN-based和Transformer-based的两种变体。
不同于DDPM，除了去噪step $k$，DP还将历史观测 $o_t,o_{t-1},o_{t-2}$ 作为Condition。
![](https://pic.hana0721.top/multimodal-action.51emecjmx5.webp)
DP直接输出Action Chunk，加快推理速度的同时也提高了Action的一致性和多样性。
在实际执行过程中，采用了保守策略，只执行Action Chunk的前几步Action，这样会更加稳定。

</ArxivRating>

## RT-1

<ArxivRating id='2212.06817' tldr='RT-1采用FiLM结构处理Image观测以及Language指令得出Token，并使用Transformer-Decoder来解码Action' rank={3}>
![](https://pic.hana0721.top/RT-1-easy.7axmxvm4yg.webp)
RT-1是Google于22年提出的工作，是一种王源（传统）VLA模型。
VLA模型其实就以视觉图像、语言指令等为输入，输出智能体的Action。
为什么这么说它是王源（传统）呢？因为它没有使用经过大规模预训练的LLM或是VLM作为Fusion模型。
RT-1旨在实现泛化性能，使用了大规模的Multi-task Dataset进行训练，能够完成少量的Unseen Task。
![](https://pic.hana0721.top/RT-1-all.3uvb5scf72.webp)
文中给出了详细的架构图，下面我来分块讲解。
从整体上来看RT-1，它以历史图像（文中是6个图像）和语言指令为输入，以Action为输出。
首先，它使用了FiLM架构来融合Image和Language的信息。
视觉主干网络是EfficientNet-3B（经过ImageNet预训练），语言特征提取使用了Universal Sentence Encoder，这些网络的参数能从网上下载。
然后，得出了Image Token，并进一步使用TokenLearner进行降维处理。
最后，将Token拼接起来，并使用位置编码，经过一个Transformer-Decoder风格的解码器，最终得出Action。
值得注意的是，RT-1将连续动作进行了离散化，文中具体离散为256个区间。
在结构上可以看到，RT-1采用了许多推理加速的方法，比如减少输入的Token或是采用高效的模型（EfficientNet-3B）。
RT-1虽然能够做到一定程度的泛化，但是也有一定的局限性。
为什么说具有局限性？首先，RT-1只使用了具身Dataset直接拟合Action，优质的具身Dataset本来就少，而且难以获取。
再来就是，RT-1也很容易摆烂，也是因为Dataset不够全面，遇到一些没有见过的物体，可能就无法work了。
RT-1早期的贡献还是值得肯定的，并且在后续的工作RT-2中解决了这些问题。

</ArxivRating>

## RT-2

<ArxivRating id='2307.15818' tldr='RT-2加入了VLM进行Fusion，是现有比较规范的VLA架构。' rank={3}>
![](https://pic.hana0721.top/RT-2.8l0k484kpi.webp)
RT-2是Google继RT-1后于2023年提出的一种VLA模型。
RT-2抛弃了RT-1的FiLM结构，使用了预训练的VLM来处理Image观测与Language指令（本质是Image经过ViT映射为Language Token并与指令Token一同输入到LLM中）。
RT-2采用了PaLI-X  和 PaLM-E 作为主干，这两个模型都进行了大规模VQA预训练。
和RT-1一样，RT-2也对连续动作进行了离散化处理（256个区间），但RT-2将离散动作映射到了LLM Token中，因此需要保留256个词元作为动作词元。
对PaLI-X，将动作词元对应到整数词元。对PaLM-E，则采用频率最低的256词元作为动作词元。
同时，RT-2对训练数据进行了处理，转为了适合VLM训练的VQA格式。
RT-2使用了协同微调，也就是将网络数据与机器人数据混合起来微调，这样可以提高泛化能力。
因为使用LLM进行Action的输出，所以需要限制其输出动作词元，RT-2使用了Prompt进行限制。
RT-2发布了不同参数规模的模型，如5B和55B。5B可以部署在本地，55B需要使用云服务进行部署。
RT-2还可以结合CoT的架构进行推理，如SayCan这类方法，以处理更加复杂的指令。
![](https://pic.hana0721.top/RT-2-performance.3uvb5ucnb1.webp)
RT-2在Unseen限制下表现出一定的泛化能力，但还是具有局限性。
比如，它不能处理一些没有经验的新动作，且无法完成一些Unseen Object的动力学任务（如移动马克笔到指定位置）。
它使用离散化的动作，可能难以处理一些高精度的任务。
而且使用低频词元去学习高频输出的动作词元，会出现双峰学习的问题（学习VLM降低这些词元的输出频率，而学习VLA却要提高，属于是左右脑互博）。

</ArxivRating>

## OpenVLA

<ArxivRating id='2406.09246' tldr='OpenVLA-like的奠基工作' rank={4}>

![](https://pic.hana0721.top/openvla.iclclg6tx.webp)
OpenVLA基本继承了RT-2的思想，形成了OpenVLA-like的VLA范式。
简单来说，就是使用LLM对Image观测以及Language指令进行Fusion，输出Action Token，最后再Detokenizer成机器人可执行的Action。
OpenVLA使用Prismatic-7B VLM作为主干，文中详细解释了为什么采用Prismatic-7B VLM。
和RT-2一样，使用256个频率最低的词元作为Action Token。
训练数据主要使用了OXE Dataset，并对数据进行了处理，以便于训练。
最终，OpenVLA展示了在不同消费级和服务级的GPU上的运行效果。
然而，OpenVLA也有着诸多Limitation。
首先，它只接受单图像预测，无法处理异构的机器人设置。
其次，它的吞吐量不够，无法实现高频率的控制。
最后，同时也是最大的硬伤，和RT-2一样，在面对Co-Training时，选择频率最低的256个Token作为Action Token，会出现双峰学习问题。
但不可否认的是，OpenVLA还是一份伟大的工作，开创了OpenVLA-like的VLA学习范式。

</ArxivRating>

## OXE | RT-X

<ArxivRating id='2310.08864' tldr='Embodied AI领域的大规模Dataset' rank={5}>

![](https://pic.hana0721.top/OXE.5moa1wm2lz.webp)
Open X-Embodiment数据集包含超过100万个真实机器人轨迹，涵盖22种机器人实体，从单机械臂机器人到双臂机器人和四足机器人。
该数据集是通过汇集来自全球 34 个机器人研究实验室的 60 个现有机器人数据集，并将它们转换为一致的数据格式以便于下载和使用而构建的。
![](https://pic.hana0721.top/RT-X.3yex4q5zbj.webp)
该团队还使用OXE训练了RT-1和RT-2，展现出泛化性能。
工作量如此之大，伟大无需多言。

</ArxivRating>

## Unified Video Action Model

<ArxivRating id='2503.00200' tldr='UVA使用掩码学习了通用的多任务模型' rank={3}>

![](https://pic.hana0721.top/UVA-easy.465lu585n.webp)
从数据流上看，UVA首先学习了Image与Action Chunk的Joint Latent，然后使用Diffusion模型解码出未来的Action Chunk以及观测Image。
为了兼容多种类型的输入，并解耦出对应的Action Chunk以及Image，UVA采用了掩码学习。
![](https://pic.hana0721.top/UVA-all.2h8s31j1cb.webp)
下面讲解学习Joint Latent的流程。
对于每张历史Image使用预训练的VAE编码器进行编码，得出$N$个$d$维的Image Token。
对于每个历史Action Chunk，为了对齐Image Token的维度，采用重复$M$次拼接的方式对齐到$N$维度，并经过一个MLP投影到$N\times d$。
对于要预测的Image，处理方式与历史Image一样，但加上了定义好的掩码。
最终，将三类Token拼接在一起，输入到Transformer中进行编码，得出Joint Latent。
接下来讲解预测流程。
对于预测Image，将每个Token作为Image Diffusion的Condition，经过去噪还原出局部的VAE的Latent，最后拼接经过VAE的解码器还原出Image。
对于预测Action Chunk，则将所有的Token作为Action Diffusion的Condition，经过去噪得出Future Action Chunk。
UVA使用了灵活目标的掩码训练方法，有助于防止模型对特定任务产生过拟合，从而提升模型的整体通用性和鲁棒性。

</ArxivRating>

## Genie Envisioner

<ArxivRating id='2508.05635' tldr='' rank={0}>

</ArxivRating>

## TinyVLA

<ArxivRating id='2409.12514' tldr='TinyVLA是一种早期的PI-like VLA' rank={3}>

![](https://pic.hana0721.top/TinyVLA.64ebqsl3yh.webp)
TinyVLA首次提出了VLM+DP的VLA范式，这个范式后续被应用到了 $\pi$ 系列。
在这个范式下，Action不再由LLM的Token Detokenize出来，而是直接由一个Action Expert（DP或是ACT）输出。
这样做，可以避免RT-2以及OpenVLA中的双峰学习问题，能够避免灾难性遗忘，使得VLM和Action Expert各司其职。
在TinyVLA中，选用了Pythia作为VLM主干，选用DP作为Action Expert。
Action Expert以VLM主干输出的信息为Condition，输出Action Chunk。
在训练过程中，使用LoRA微调VLM，防止了VLM的灾难性遗忘，且可以进行高效的微调。
在实验部分，TinyVLA超越了OpenVLA，且可以在OOD的场景下完成一些任务。
TinyVLA应该是最早提出PI-like架构的工作，算是祖师爷。

</ArxivRating>

## CogACT

<ArxivRating id='2411.19650' tldr='CogACT使用BERT-like的VLM进行Fusion，使用DP输出Action Chunk' rank={3}>

![](https://pic.hana0721.top/CogACT.67xxokoxle.webp)
CogACT也是一种PI-like的VLA，分为VLM Fusion和Action Expert模块。
在VLM Fusion模块中，CogACT采用了一种BERT-like的方式进行Fusion，输入Image观测以及Language指令，输出类似于\[CLS\]的Cognition Feature。
在Action Expert模块中，以Cognition Feature为Condition，经过一个DiT得出Action Chunk。
![](https://pic.hana0721.top/CogACT-ensembling.8s3s17oxad.webp)
CogACT和ACT一样，使用了类似的Action Ensembling，但CogACT中的权重是自适应的，避免了不同模式中不合理地聚合Action。

</ArxivRating>

## PI-0

<ArxivRating id='2410.24164' tldr='PI-like系列奠基之作' rank={4}>
![](https://pic.hana0721.top/pi0.eszi6dq8l.webp)
简单说一下 $\pi_0$ 的两个结构，对Image观测与Language指令进行Fusion的VLM以及基于FLow Matching的Action Expert。在Fusion模块，考虑到实时控制的需求，采用了规模小便利性高的 PaliGemma。Image观测被ViT映射到语义空间，与Language指令一同进入PaliGemma，得出上下文Token。Action Expert采用Flow Matching结构建模Action连续分布。其输入为随机化的Action Chunk，以上下文Token的cross attention以及机器人的Joint Position作为Condition，使用Flow Matching Loss进行训练，输出真实的Action Chunk。
在训练数据上，$\pi_0$ 使用了长达10000小时的真机数据，以及开源数据OXE等。在多种任务上表现出强大的性能，非常牛逼。

</ArxivRating>

## PI-0.5

<ArxivRating id='2504.16054' tldr='' rank={0}>

</ArxivRating>

## GR-3

<ArxivRating id='2507.15493' tldr='' rank={0}>

</ArxivRating>

## InstrucVLA

<ArxivRating id='2507.17520' tldr='' rank={0}>

</ArxivRating>

## DinoV3

<ArxivRating id='2508.10104' tldr='' rank={0}>

</ArxivRating>

## FiS-VLA

<ArxivRating id='2506.01953' tldr='' rank={0}>

</ArxivRating>

## VIMA

<ArxivRating id='2210.03094' tldr='VIMA 采用 Multi-Modal Prompt 来完成一系列的 Robotic Manipulation 任务' rank={2}>

![](https://pic.hana0721.top/VIMA.70atg93q8d.webp)
VIMA 的具体做法就是将 Robotic Manipulation 任务规范为 Multi-Modal Prompt。
Multi-Modal Prompt 具体实现为 Vision 与 Text 的排列组合，文中考虑了6种任务类别。
由于时代的局限，当时没有 Multi-Modal 的 Robotic Task 的 Benchmark。
于是，该团队提出了VIMA-Bench，可以通过脚本化的 Oracle 智能体生成大量的模仿学习数据。

![](https://pic.hana0721.top/VIMA-Contral.60uq3314ou.webp)
VIMA 这个工作比较早期，所以只使用了 T5 进行 Vision 与 Text 的 Fusion。
对于 Text，采用预训练的 T5 词嵌入获取 Text Token。
对于 Scene Vision，首先使用领域微调的 Mask R-CNN 来提取物体，然后经过 ViT 编码成 Scene Token。
对于 Object Vision，和 Scene Vision一样的方式，编码为 Object Token。
经过 T5 模型，可以得出 Prompt Tokens。

VIMA 控制器采用了 Decoder-Only 的架构，于 Decision Transformer 类似，采用序列建模的方式。
VIMA 控制器以 History 为输入，输出 Action。
这部分与 Fusion 相似，被处理成了若干的 Vision Token，然后与机器人的 Joint Position 以及 Action 拼接，形成 History Token。
在推理过程中，经过了多层的 Self-Attention 与 Cross-Attention 结构，自回归产生下一步的 Action。
在 Cross-Attention 模块中，Prompt Tokens 被投影至 $K_{\mathcal{P}}$ 和 $V_{\mathcal{P}}$，并使用 History 投影的 $Q_{\mathcal{H}}$查询。

</ArxivRating>

## SayCan

<ArxivRating id='2204.01691' tldr='SayCan 引入 RL 中的 Value Function 来纠正 LLM 的幻觉问题' rank={2}>

![](https://pic.hana0721.top/SayCan.86u4ow4enj.webp)
SayCan 是任务规划的早期工作。任务规划就是给一个高级任务指令，分解为一系列机器人可执行的下游任务。
很显然，具有强泛化能力的 LLM 非常适合做这个任务，但也有一些问题。
最主要的问题就是 LLM 无法识别到物理世界，从而产生一些机器人无法执行的任务，也就是幻觉问题。

![](https://pic.hana0721.top/SayCan-method.4ubeuinub1.webp)
为了解决这个问题，SayCan 引入了 RL 中的 Value Function，表示为在当前状态下执行成功某个 skill 的成功率。
在 SayCan设置的场景中，机器人有固定的 skill 集合，每次从里面选择一个 skill 执行。
直接让 LLM 输出需要执行的 skill 是不保险的，因为可能输出 skill 集合中没有的 skill。
所以，SayCan 不再让 LLM 去输出下一步执行的 skill，而是让 LLM 给所有的 skill 进行评分。
同时，SayCan 会给 LLM 一些 Example Prompt，让 LLM 能够更加有效地提取知识。
最后将两者结合，去选择机器人应该执行的 skill。

</ArxivRating>

## PaLM-E

<ArxivRating id='2303.03378' tldr='基于 PaLM 训练的通用机器人场景 VLM' rank={3}>

![](https://pic.hana0721.top/PaLM-E.45i5blou92.webp)
PaLM-E 做的事情很简单，就是在 PaLM 的基础上添加若干的视觉编码组件，如 ViT、OSRT等。
然后，使用 VQA 与机器人场景数据进行 Co-Training，这样就得到了一个通用的 VLM，可以处理多种任务。
在机器人场景下，PaLM-E 可以发挥 Planning 的作用。

PaLM-E 其实没有多少的 insight，主要是在输入上做处理，具体可以去看原文。
但 PaLM-E 的效果很好，在 Planning 任务中薄纱了当时的 SayCan，且有跨任务的能力。

</ArxivRating>

## ViLA

<ArxivRating id='2311.17842' tldr='对 VLM 使用 Prompt 进行 Planning' rank={3}>

![](https://pic.hana0721.top/ViLA.2oc09ver89.webp)
ViLA 的思路非常简单，就是告诉 VLM 当前要执行的 Task 以及 Obs，然后使用 Prompt 输出 CoT 形成 Task Planning，
然后维护 TODO List，每次执行 TODO List 中的第一条 Low-level Task，不断循环直到任务结束。

ViLA 的思路虽然简单，但是有用且本质，非常具有启发性。

</ArxivRating>

## CoPa

<ArxivRating id='2403.08248' tldr='结合 VLM、SoM、GraspNet、SE(3)等方法形成完整的操作流程' rank={2}>

![](https://pic.hana0721.top/CoPa-overview.6m4dqmphlv.webp)
CoPa 结合了许多种方法，让机器人直接输出 Action 去完成 high-level 任务。
CoPa 将大多数操作任务分为任务导向抓取和运动规划两个阶段。

![](https://pic.hana0721.top/CoPa-grounding.7axnand0m8.webp)
在"任务导向抓取"中，分为粗粒度 Object 锚定和细粒度 Grasp 锚定。
在粗粒度 Object 锚定中，先使用 SoM 去分割出 Obs 中的可抓取 Object，然后再使用 GPT-4V 去筛选最终抓取 Object。
在细粒度 Grasp 锚定中，使用同样的方式去筛选最终 Grasp 的位置。

![](https://pic.hana0721.top/CoPa-grasp.8dxclj8uhq.webp)
进一步，CoPa 使用 GraspNet 去生成机器人的候选抓取姿态，并使用 Grasp 锚定进行筛选。

![](https://pic.hana0721.top/CoPa-planning.3govrov2ot.webp)
在"任务运动规划"中，首先采用同样的锚定方式识别多个任务相关的部件。
然后，使用 GPT-4V 生成操作约束限制。
最后，可以得出一系列的 SE(3) 的矩阵，求可以得出抓取姿态。

</ArxivRating>

## PointLLM

<ArxivRating id='2308.16911' tldr='' rank={0}>

</ArxivRating>

## EmbodiedGPT

<ArxivRating id='2305.15021' tldr='' rank={0}>

</ArxivRating>

## RT-Trajectory

<ArxivRating id='2311.01977' tldr='' rank={0}>

</ArxivRating>

## Im2Flow2Act

<ArxivRating id='2407.15208' tldr='' rank={0}>

</ArxivRating>
