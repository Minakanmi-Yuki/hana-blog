---
title: 'Paper Reading: Embodied AI 1'
publishDate: 2025-12-26
updatedDate: 2025-12-26
description: 从零开始的Embodied AI研究生活。
heroImage: { src: 'https://pic.hana0721.top/rl-note-3.3yex1xdpri.webp', color: '#8C8275' }
category: 'research'
pixivLink: '127192375'
tags:
  - 'Paper Reading'
  - 'embodied ai'
---

import { ArxivRating, RatingCriteria } from '@/components/advanced'

<RatingCriteria />

## 前言

RL菜鸡开始进军Embodied AI，慢慢积累，提升自己。

## ACT

<ArxivRating id='2304.13705' tldr='ACT采用了CVAE架构，使用了BERT风格的编码器以及Transformer风格的解码器，在高频机器人Manipulation任务上表现出优异的性能。' rank={3}>
![](https://pic.hana0721.top/act-easy.92qlspndp7.webp)
ACT是一种经典的机器人Manipulation方法，采用了CVAE架构，上图展示了ACT的模型架构。
ACT的输入是机器人Joint position以及多视角下的观测图像，输出是未来一系列的Action。
CVAE-Encoder采用了BERT-like的结构，以机器人Joint position和专家Action chunking（加上了position embedding）为输入，并将\[CLS\]经过投影后得出隐向量$z$。
CVAE-Decoder采用Transformer-like的结构，输入是经过特征提取后的多视角Image token、机器人Joint position以及经过CVAE-Encoder编码后的隐向量 $z$（在测试阶段， $z$是0），输出是Action chunking。
在训练过程中，使用了L1损失而非L2损失，这里文章里有详细的解释。
![](https://pic.hana0721.top/temperal-ensembling.51emebjacb.webp)
ACT使用了Temperal ensembling来提高输出动作哦的平滑性，其实就是针对多个Action chunking同个时间步下的Action进行加权，具体见图。
ACT在文章的最后给出了详细的流程图，非常清晰，这点很值得点赞！
![](https://pic.hana0721.top/act-all.58hu9r5frx.webp)

</ArxivRating>

## DP

<ArxivRating id='2303.04137' tldr='DP使用历史观测作为Condition的Diffusion模型来生成Action Chunk。具有更快的推理速度以及更多的动作模态' rank={3}>
![](https://pic.hana0721.top/diffusion-policy.3d59h5tcqw.webp)
Diffusion Policy是非常经典的工作。
简单来说，就是使用Diffusion模型来进行Action的预测，输入是Noise，输出是Action Chunk。文中给出了CNN-based和Transformer-based的两种变体。
不同于DDPM，除了去噪step $k$，DP还将历史观测 $o_t,o_{t-1},o_{t-2}$ 作为Condition。
![](https://pic.hana0721.top/multimodal-action.51emecjmx5.webp)
DP直接输出Action Chunk，加快推理速度的同时也提高了Action的一致性和多样性。
在实际执行过程中，采用了保守策略，只执行Action Chunk的前几步Action，这样会更加稳定。

</ArxivRating>

## RT-1

<ArxivRating id='2212.06817' tldr='' rank={3}>
![](https://pic.hana0721.top/RT-1-easy.7axmxvm4yg.webp)
RT-1是Google于22年提出的工作，是一种王源（传统）VLA模型。
VLA模型其实就以视觉图像、语言指令等为输入，输出智能体的Action。
为什么这么说它是王源（传统）呢？因为它没有使用经过大规模预训练的LLM或是VLM作为Fusion模型。
RT-1旨在实现泛化性能，使用了大规模的Multi-task Dataset进行训练，能够完成少量的Unseen Task。
![](https://pic.hana0721.top/RT-1-all.3uvb5scf72.webp)
文中给出了详细的架构图，下面我来分块讲解。
从整体上来看RT-1，它以历史图像（文中是6个图像）和语言指令为输入，以Action为输出。
首先，它使用了FiLM架构来融合Image和Language的信息。
视觉主干网络是EfficientNet-3B（经过ImageNet预训练），语言特征提取使用了Universal Sentence Encoder，这些网络的参数能从网上下载。
然后，得出了Image Token，并进一步使用TokenLearner进行降维处理。
最后，将Token拼接起来，并使用位置编码，经过一个Transformer-Decoder风格的解码器，最终得出Action。
值得注意的是，RT-1将连续动作进行了离散化，文中具体离散为256个区间。
在结构上可以看到，RT-1采用了许多推理加速的方法，比如减少输入的Token或是采用高效的模型（EfficientNet-3B）。
RT-1虽然能够做到一定程度的泛化，但是也有一定的局限性。
为什么说具有局限性？首先，RT-1只使用了具身Dataset直接拟合Action，优质的具身Dataset本来就少，而且难以获取。
再来就是，RT-1也很容易摆烂，也是因为Dataset不够全面，遇到一些没有见过的物体，可能就无法work了。
RT-1早期的贡献还是值得肯定的，并且在后续的工作RT-2中解决了这些问题。

</ArxivRating>

## RT-2

<ArxivRating id='2307.15818' tldr='' rank={0}>

</ArxivRating>

## OpenVLA

<ArxivRating id='2406.09246' tldr='' rank={0}>

</ArxivRating>

## OXE

<ArxivRating id='2310.08864' tldr='' rank={0}>

</ArxivRating>

## Unified Video Action Model

<ArxivRating id='2503.00200' tldr='' rank={0}>

</ArxivRating>

## Genie Envisioner

<ArxivRating id='2508.05635' tldr='' rank={0}>

</ArxivRating>

## TinyVLA

<ArxivRating id='2409.12514' tldr='' rank={0}>

</ArxivRating>

## CogACT

<ArxivRating id='2411.19650' tldr='' rank={0}>

</ArxivRating>

## PI-0

<ArxivRating id='2410.24164' tldr='' rank={0}>

</ArxivRating>

## PI-0.5

<ArxivRating id='2504.16054' tldr='' rank={0}>

</ArxivRating>

## GR-3

<ArxivRating id='2507.15493' tldr='' rank={0}>

</ArxivRating>

## InstrucVLA

<ArxivRating id='2507.17520' tldr='' rank={0}>

</ArxivRating>

## DinoV3

<ArxivRating id='2508.10104' tldr='' rank={0}>

</ArxivRating>

## FiS-VLA

<ArxivRating id='2506.01953' tldr='' rank={0}>

</ArxivRating>
