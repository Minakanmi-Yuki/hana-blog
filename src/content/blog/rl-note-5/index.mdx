---
title: "RL笔记（5）：动态规划"
publishDate: 2025-12-23
updatedDate: 2025-12-23
description: "动态规划"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 策略迭代

### 策略评估
贝尔曼期望方程：
$$
V^\pi(s)=\sum_{a\in \mathcal{A}}\pi(a|s)(\mathcal{R}(s,a)+\gamma\sum_{s^\prime\in \mathcal{S}}\mathcal{P}(s^\prime|s,a)V^\pi(s^\prime))
$$

根据动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。
在得知子问题的解后，就可以求解当前问题。

动态规划算法的基本思想是将当前问题分解为若干个子问题，先求解子问题，再求解当前问题。
-   当前问题：求解当前状态 $s$ 的价值函数 $V^\pi(s)$；
-   子问题：求解下一个状态 $s^\prime$ 的价值函数 $V^\pi(s^\prime)$；

考虑所有的状态，可以使用上一轮的状态价值函数来计算当前这轮的状态价值函数：
$$
V^{k+1}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}(s,a)+\gamma\sum_{s^\prime\in\mathcal{S}}\mathcal{P}(s^\prime|s,a)V^{k}(s^\prime))
$$

随机选定初始值 $V^0$ ，当 $k\rightarrow\infin$ 时，序列 $\{ V^k \}$ 会收敛到 $V^\pi$ ，证明如下：

