---
title: "RL笔记（5）：动态规划"
publishDate: 2025-12-23
updatedDate: 2025-12-23
description: "动态规划"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 简介
动态规划（DP）在强化学习中指的是在环境模型已知（即状态转移概率 $\mathcal{P}$ 和奖励函数 $\mathcal{R}$ 已知）的情况下，计算最优策略的一类算法。
DP 的核心思想是将复杂问题分解为子问题，通过求解子问题来求解原问题。在 MDP 中，这体现为利用贝尔曼方程进行迭代更新。
-   预测问题（Prediction）：给定策略 $\pi$ ，求解该策略的价值函数 $V^\pi$。
-   控制问题（Control）：求解最优价值函数 $V^*$ 和最优策略 $\pi^*$。

## 策略迭代
核心思想： “先以此为据，算个清楚；再择优而行。”
策略迭代将求解过程严格拆分为两个交替的步骤，直到策略不再改变。
-   步骤一：策略评估 (Policy Evaluation)：在当前策略 $\pi$ 固定不变的情况下，计算出该策略下精确的状态价值函数 $V^\pi$。这意味着在这个步骤里，我们通常需要多次迭代（或者解线性方程组）直到 $V^\pi$ 完全收敛。
-   步骤二：策略提升 (Policy Improvement)：基于刚刚算出来的精确 $V^\pi$，贪心地更新策略。即对于每个状态，选择那个能带来最大期望回报的动作 $\pi^\prime(s)=\arg\max_{a} Q^\pi(s,a)$。

特点： 收敛所需的迭代步数少，但每一步内部的计算量大（因为策略评估需要很久）。

### 策略评估
策略评估的目标是求解给定策略 $\pi$ 的状态价值函数 $V^\pi$。
对于策略 $\pi$，状态价值函数满足贝尔曼期望方程：
$$
V^\pi(s)=\sum_{a\in \mathcal{A}}\pi(a|s)(\mathcal{R}(s,a)+\gamma\sum_{s^\prime\in \mathcal{S}}\mathcal{P}(s^\prime|s,a)V^\pi(s^\prime))
$$
这构成了一个线性方程组，但在状态空间较大时直接求解困难，因此采用迭代法。

#### 贝尔曼方程迭代更新
将当前时刻估计的价值函数 $V_k$ 代入贝尔曼方程右侧，计算下一轮的估计 $V_{k+1}$ ：
$$
V_{k+1}(s)\leftarrow\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}(s,a)+\gamma\sum_{s^\prime\in\mathcal{S}}\mathcal{P}(s^\prime|s,a)V_{k}(s^\prime))
$$
随机选定初始值 $V_0$ ，当 $k\rightarrow\infin$ 时，序列 $\{ V_k \}$ 会收敛到 $V^\pi$ ，下节给出证明。

#### 收敛性证明（Banach不动点定理）
为了证明序列 $\{ V_k \}$ 收敛于 $V^\pi$，我们将迭代过程看作算子 $\mathcal{T}^\pi$ 的应用：
$$
V_{k+1}=\mathcal{T}^\pi V_k
$$
证明步骤如下：
1.  定义范数：采用无穷范数 $\parallel V \parallel_\infty=\max_{s\in \mathcal{S}} | V(s)|$；
2.  压缩映射性质：对于任意两个价值函数 $U, V$，有：
    $$
    \begin{align}
    \parallel \mathcal{T}^\pi U - \mathcal{T}^\pi V \parallel_\infty 
    &= \max_{s} | \gamma \sum_{a}\pi(a|s)\sum_{s^\prime}\mathcal{P}(s^\prime|s,a)(U(s^\prime)-V(s^\prime)) | \notag \\
    &\leqslant \gamma \max_{s} \sum_{a}\pi(a|s)\sum_{s^\prime}\mathcal{P}(s^\prime|s,a) |U(s^\prime)-V(s^\prime)| \notag \\
    &\leqslant \gamma \max_{s} \sum_{a}\pi(a|s)\sum_{s^\prime}\mathcal{P}(s^\prime|s,a) \parallel U - V \parallel_\infty \notag \\
    &= \gamma \parallel U - V \parallel_\infty \notag
    \end{align}
    $$
3.  结论：只要折扣因子 satisfying $0 \le \gamma < 1$，算子 $\mathcal{T}^\pi$ 就是一个$\gamma$-压缩映射（Contraction Mapping）。根据 Banach 不动点定理，序列 $\{V_k\}$ 必收敛于唯一的固定点 $V^\pi$。


