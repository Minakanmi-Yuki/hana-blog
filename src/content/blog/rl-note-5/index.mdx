---
title: "RL笔记（5）：动态规划"
publishDate: 2025-12-23
updatedDate: 2025-12-23
description: "动态规划"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 简介
动态规划（DP）在强化学习中指的是在环境模型已知（即状态转移概率 $\mathcal{P}$ 和奖励函数 $\mathcal{R}$ 已知）的情况下，计算最优策略的一类算法。
DP 的核心思想是将复杂问题分解为子问题，通过求解子问题来求解原问题。在 MDP 中，这体现为利用贝尔曼方程进行迭代更新。
-   预测问题（Prediction）：给定策略 $\pi$ ，求解该策略的价值函数 $V^\pi$。
-   控制问题（Control）：求解最优价值函数 $V^*$ 和最优策略 $\pi^*$。

## 策略迭代

### 策略评估
贝尔曼期望方程：
$$
V^\pi(s)=\sum_{a\in \mathcal{A}}\pi(a|s)(\mathcal{R}(s,a)+\gamma\sum_{s^\prime\in \mathcal{S}}\mathcal{P}(s^\prime|s,a)V^\pi(s^\prime))
$$

根据动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。
在得知子问题的解后，就可以求解当前问题。

动态规划算法的基本思想是将当前问题分解为若干个子问题，先求解子问题，再求解当前问题。
-   当前问题：求解当前状态 $s$ 的价值函数 $V^\pi(s)$；
-   子问题：求解下一个状态 $s^\prime$ 的价值函数 $V^\pi(s^\prime)$；

考虑所有的状态，可以使用上一轮的状态价值函数来计算当前这轮的状态价值函数：
$$
V^{k+1}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)(\mathcal{R}(s,a)+\gamma\sum_{s^\prime\in\mathcal{S}}\mathcal{P}(s^\prime|s,a)V^{k}(s^\prime))
$$

随机选定初始值 $V^0$ ，当 $k\rightarrow\infin$ 时，序列 $\{ V^k \}$ 会收敛到 $V^\pi$ ，证明如下：

