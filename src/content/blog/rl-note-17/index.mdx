---
title: "RL笔记（17）：模型预测控制 (MPC)"
publishDate: 2025-12-26
updatedDate: 2025-12-26
description: "在已知环境模型的情况下，如何高效规划？详解模型预测控制 (MPC) 的原理：预测未来、滚动优化与MPC-SAC。"
heroImage: {src : "https://picr2.axi404.top/Paper-reading-Uni-zh.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

在之前的笔记中，我们探讨了**无模型 (Model-Free)** 算法（如 Q-Learning, PPO, SAC）如何通过试错学习。我们也初步接触了**基于模型 (Model-Based)** 的 Dyna-Q，它通过学习环境模型来生成模拟数据辅助学习。

今天我们深入 Model-Based RL 的另一个重要分支：**模型预测控制 (Model Predictive Control, MPC)**。
MPC 的核心优势在于，它**直接利用环境模型进行前向规划**，像一个经验丰富的棋手一样“预判”未来，然后根据预测结果做出当前最优的决策。

MPC 不仅仅是生成模拟数据（像 Dyna-Q），它是在每一个状态下，都**重新规划**一次未来的动作序列。

---

## MPC 的核心思想

MPC 是一种控制策略，它假设我们拥有一个精确的环境模型 $M(s,a) \to (r, s')$。

###  预测与规划 (Prediction & Planning)
在当前状态 $s_t$，MPC 并不直接选择一个动作。而是：
1.  **预测未来**：假设从当前状态 $s_t$ 开始，执行一系列动作 $\pi = \{a_t, a_{t+1}, \dots, a_{t+H}\}$（称为**预测时域 (Prediction Horizon)** $H$）。
2.  **模拟轨迹**：利用模型 $M$，模拟出这一系列动作可能产生的轨迹及其累积奖励。
3.  **优化规划**：找到能最大化累积奖励的动作序列 $\pi^*$。

###  滚动优化 (Rolling Optimization)
MPC 的决策过程是**滚动**的：
1.  在状态 $s_t$，MPC 找到最优动作序列 $\pi^* = \{a_t^*, a_{t+1}^*, \dots, a_{t+H}^*\}$。
2.  **只执行第一个动作 $a_t^*$**。
3.  智能体进入新状态 $s_{t+1}$。
4.  **重新规划**：基于新的状态 $s_{t+1}$，重复步骤 1-3。

> **💡 直觉**：
> MPC 就像一个有预见性的决策者。它不会一次性把所有步骤都定死，而是每走一步，都根据当前情况重新规划下一步的最佳路径。这使得它能很好地应对环境变化。

---

## MPC 与强化学习的结合

MPC 本身是一种控制方法，如何与 RL 结合呢？

###  学习模型
在 Model-Based RL 中，我们首先需要学习一个环境模型 $M_\theta$。
*   **监督学习**：用收集到的真实数据 $(s, a, r, s')$ 来训练模型。
    *   预测奖励：$r \approx R_\phi(s,a)$
    *   预测状态：$s' \approx M_\theta(s,a)$
*   **模型类型**：可以是**概率模型**（如 PETS 使用的概率 GP 回归）来估计模型的不确定性，也可以是确定性模型（如简单的神经网络）。

###  MPC 规划
一旦有了模型 $M_\theta$，就可以进行规划：
*   在当前状态 $s_t$，MPC 在所有可能的 $H$ 步动作序列中搜索，找到能最大化模拟累积奖励的序列 $\pi^* = \{a_t^*, a_{t+1}^*, \dots, a_{t+H}^*\}$。
*   执行 $a_t^*$。

###  优化策略
MPC 产生的动作序列 $\pi^*$ 本身就可以看作是当前状态下的一个“策略”。我们可以用标准的 RL 算法（如 PPO, SAC）来优化**生成这个 $\pi^*$ 的参数**。

例如，**MPC-SAC** 的做法：
1.  **训练一个模型** $M_\theta$。
2.  **训练 Actor-Critic**：
    *   Critic 学习 $Q$ 值。
    *   Actor 学习策略 $\pi_\phi$。
    *   在 Actor 的更新时，**使用 MPC 规划 $H$ 步**，获得更准确的奖励信号来更新 Actor。

---

## PETS 算法示例 (Probabilistic Embeded Trajectory Sampling)

PETS 是一个经典的 Model-Based RL 算法，它使用了**概率模型**来处理模型不确定性。

###  概率模型
PETS 使用**高斯过程 (Gaussian Process, GP)** 或其他概率模型来学习奖励函数 $R_\phi$ 和状态转移函数 $M_\theta$。
*   **优点**：GP 不仅能给出预测值，还能给出预测的**不确定性**（方差）。

###  采样与规划
在 MPC 规划时，PETS 会从学到的概率模型中**采样**多条可能的未来轨迹。
*   如果模型预测某一步不确定性很高（方差大），意味着未来可能有很多种情况，MPC 会倾向于选择风险更小的路径。

###  策略学习
PETS 也会用学习到的模型和规划结果来训练一个 Actor-Critic 策略。

---

## MPC 的优势与劣势

### 优势
*   **高样本效率**：通过模型可以“预演”大量场景，大大减少与真实环境的交互次数。
*   **可解释性**：模型可以被检查，知道智能体为什么这么规划。
*   **安全性**：在规划时可以预见潜在的危险状态，并加以避免。

### 劣势
*   **模型偏差 (Model Bias)**：这是 Model-Based 方法的软肋。如果模型不准确，规划的结果可能完全错误。
*   **计算成本**：MPC 规划本身需要大量的计算，尤其是在动作空间连续、预测时域长的情况下。

---

## 总结

MPC 提供了一种强大的在**已知模型**下进行序列决策的方法。它通过向前看（预测）和滚动优化，能够做出比直接 RL 更明智的决策。
将 MPC 与 RL 结合，是 Model-Based RL 的重要研究方向，其目标是学好模型，并用好模型，从而以最高的样本效率解决复杂任务。

我们之前的笔记主要聚焦于 Model-Free 方法（Q-Learning, REINFORCE, PPO, SAC）。Model-Based 方法（Dyna-Q, MPC）提供了另一种思路，它们往往在样本效率上具有优势，但在模型精度要求上较高。