---
title: "RL笔记（16）：模仿学习 (Imitation Learning)"
publishDate: 2025-12-25
updatedDate: 2025-12-25
description: "没有奖励函数怎么办？详解模仿学习的三大流派：行为克隆 (BC) 的简单粗暴与局限、逆强化学习 (IRL) 的理论推导，以及生成式对抗模仿学习 (GAIL) 的对抗博弈思想。"
heroImage: {src : "https://pic.hana0721.top/rl-note-3.1vz4dv2u7f.webp", color: '#8C8275'}
category: 'daily'
pixivLink: '127192375'
---

## 引言（Introduction）

在传统的强化学习中，智能体通过最大化累积奖励来学习。然而，在很多复杂任务中，“定义奖励”比“解决任务”本身还要难。
*   **例子**：自动驾驶。你很难用数学公式精确定义什么叫“开得稳且安全”。但是，我们可以很容易地收集到人类老司机的驾驶数据。

**模仿学习 (Imitation Learning)** 的目标是：给定一组专家演示数据 $\tau_E = \{(s_1, a_1), (s_2, a_2), \dots \}$，训练一个策略 $\pi_\theta$，使其行为尽可能接近专家策略 $\pi_E$。

我们不需要知道背后的奖励函数是什么，只需要“照猫画虎”。

---

## 行为克隆 (Behavioral Cloning, BC)

这是最简单、最直观的模仿学习方法。

### 核心思想
把模仿学习看作一个**监督学习 (Supervised Learning)** 问题。
*   **输入**：状态 $s$（特征）。
*   **标签**：动作 $a$（专家在 $s$ 下做的动作）。

我们的目标是训练一个分类器（离散动作）或回归器（连续动作）$\pi_\theta(s)$，使其输出拟合专家的动作。

### 目标函数
使用最大似然估计 (MLE)：
$$
\max_\theta \mathbb{E}_{(s,a) \sim \pi_E} [\log \pi_\theta(a|s)]
$$
即最小化负对数似然损失：
$$
L(\theta) = - \sum_{i=1}^N \log \pi_\theta(a_i|s_i)
$$

### 局限性：协变量偏移 (Covariate Shift)
虽然 BC 简单有效，但它有一个致命弱点：**误差累积**。

1.  **训练分布**：策略是在专家访问过的状态 $s \sim P(\pi_E)$ 上训练的。
2.  **测试分布**：在实际运行时，策略 $\pi_\theta$ 可能会犯一点小错。
3.  **恶性循环**：这一点小错会导致智能体进入一个**专家从未去过**的状态（Out-of-Distribution）。在这种陌生状态下，BC 策略可能会胡乱行动，导致更大的错误，最终迅速偏离轨道（比如自动驾驶车偏离一点点后，不知道怎么修回去，直接冲出跑道）。

> **💡 总结**：BC 只是单纯地拟合动作，而不理解“为什么要这么做”。

---

## 逆强化学习 (Inverse RL, IRL)

为了解决 BC 的问题，IRL 提出了一个更深层的思路：**专家之所以这么做，是因为他在最大化某个未知的奖励函数 $R$。**

如果我们能把这个 $R$ 反推（Inverse）出来，再用标准的 RL 算法（如 PPO/SAC）去最大化这个 $R$，不就能学会专家的策略了吗？

### 核心流程
1.  **假设**：存在一个参数化的奖励函数 $R_\phi(s,a)$。
2.  **目标**：找到参数 $\phi$，使得在该奖励函数下，专家的轨迹获得的累积回报高于其他任何策略。
    $$ \mathbb{E}_{\pi_E} [R_\phi] \ge \mathbb{E}_{\pi} [R_\phi] $$
3.  **内层循环**：给定当前的 $R_\phi$，通过 RL 训练一个最优策略 $\pi^*$。
4.  **外层循环**：更新 $R_\phi$，使得专家轨迹和 $\pi^*$ 轨迹的差距变大（让专家得分更高）。

### 难点
IRL 是一个**不适定 (Ill-posed)** 问题：
*   很多奖励函数都能解释同一种行为（例如：如果 $R=0$，任何策略都是最优的）。
*   因此，IRL 通常需要引入**最大熵 (Maximum Entropy)** 假设：在所有能解释专家行为的奖励函数中，选择那个对策略分布约束最少（熵最大）的。

> **💡 总结**：IRL 试图理解专家的“动机”。但它计算极其昂贵，因为每更新一次奖励函数，就要重新跑一遍完整的 RL 训练。

---

## 生成式对抗模仿学习 (GAIL)

Ho & Ermon (2016) 证明了：**我们其实不需要显式地把奖励函数 $R$ 恢复出来。**
如果我们直接训练一个策略，让它的状态-动作分布 $\rho_\pi(s,a)$ 和专家的分布 $\rho_E(s,a)$ 难以区分，不就达到目的了吗？

这正是 **GAN (生成对抗网络)** 的思想。

### 核心架构
GAIL 包含两个网络进行博弈：
1.  **判别器 (Discriminator) $D_w(s,a)$**：
    *   任务是**找茬**。它接收一个 $(s,a)$ 对，判断这是**专家**产生的（输出 1），还是**智能体**产生的（输出 0）。
2.  **生成器 (Generator) $\pi_\theta(a|s)$**：
    *   即我们的策略网络。任务是**欺骗**判别器，让自己的行为看起来像专家。

### 目标函数
GAIL 的优化目标与 GAN 几乎一致：
$$
\min_\pi \max_D V(\pi, D) = \mathbb{E}_{\pi_E}[\log D(s,a)] + \mathbb{E}_{\pi}[\log(1 - D(s,a))]
$$

### 训练流程
1.  **训练判别器 $D$**：
    *   采样专家数据 $(s_E, a_E)$ 和 智能体数据 $(s_\pi, a_\pi)$。
    *   让 $D(s_E, a_E) \to 1$，让 $D(s_\pi, a_\pi) \to 0$。
2.  **训练策略 $\pi$**：
    *   我们使用 $D$ 的输出作为一种**替代奖励 (Surrogate Reward)**。
    *   定义奖励函数：$r(s,a) = - \log(1 - D(s,a))$ （或者其他变体，如 $\log D(s,a)$）。
    *   如果 $D$ 认为当前动作像专家（$D \approx 1$），奖励就高；不像专家，奖励就低。
    *   使用 PPO 或 TRPO 来最大化这个奖励。

### 优势
*   **解决了 Covariate Shift**：因为 $\pi$ 是在环境中交互训练的（RL），遇到偏离的状态时，判别器会给低分，RL 算法会学会如何修正回到正轨。
*   **效率高**：不需要像 IRL 那样反复求解最优策略，策略和判别器是同步更新的。

---

## 总结与对比

| 维度 | 行为克隆 (BC) | 逆强化学习 (IRL) | 生成对抗模仿 (GAIL) |
| :--- | :--- | :--- | :--- |
| **核心思想** | **监督学习**：拟合 $s \to a$ 映射 | **逆向推理**：反推 $R(s,a)$ | **对抗博弈**：让分布 $\rho_\pi \approx \rho_E$ |
| **交互需求** | 不需要与环境交互 (Offline) | 需要 (Online) | 需要 (Online) |
| **奖励函数** | 无 | 显式学出 $R$ | 隐式奖励 ($D$ 的输出) |
| **优点** | 简单、训练快 | 可解释性强、鲁棒 | 效果好、无需解内层 RL 循环 |
| **缺点** | 误差累积 (Covariate Shift) | 计算极其昂贵 | 训练不稳定 (GAN 通病) |

> **💡 思考**：
> 如果专家的数据也不是完美的（比如偶尔手滑），BC 会傻傻地把错误也学进去，而 IRL/GAIL 由于有奖励函数的平滑作用，通常能学出比专家更好的策略。